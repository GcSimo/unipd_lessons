\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc} % standard unicode
\usepackage[italian]{babel} % corretta sillabazione in italiano
\usepackage{geometry} % per impostare margini e layout pagina
\usepackage{amssymb} % per l'ambiente matematico
\usepackage{amsmath} % per l'ambiente matematico
\usepackage{enumitem} % per elenchi puntati
\usepackage{multirow} % per celle che si espandono su più righe
\usepackage{tabularx} % per tabelle con larghezza flessibile
\usepackage{booktabs} % per linee orizzontali tabelle
\usepackage{hyperref} % per collegamenti
\usepackage{graphicx} % per immagini
\usepackage{listings} % per codice
\usepackage{xcolor} % per colori nel codice
\usepackage{dirtytalk} % per le ""

% per margini
\geometry{a4paper,left=25mm, right=25mm, bottom=25mm, top=30mm}

% per centrare testo nelle tabelleX
\renewcommand\tabularxcolumn[1]{m{#1}}

% percorso delle immagini da inserire
\graphicspath{ {./ } }

% funzioni
\newcommand\f[4]{\begin{smallmatrix} {#1} &\to &{#2} \\ {#3} &\mapsto &{#4} \end{smallmatrix}}
\newcommand\m[2]{\text{M}_{\underline{#1}}^{\underline{#2}}}

\newcommand\tc{\;\text{t.c.}\;} % tale che
\newcommand\img{\text{Im}}		% immagine
\newcommand\rg{\text{rg}} 		% rango
\newcommand\nul{\text{null}}	% nullità
\newcommand\sol{\text{Sol}}		% soluzioni
\newcommand\sgn{\text{sgn}}		% segno
\newcommand\dist{\text{dist}}	% distanza
\newcommand\tran{^{\mkern-1.5mu\mathsf{T}}} % trasposta

% smallmatrix 3 x 1
\newcommand\psmatrix[3]{\left( \begin{smallmatrix} {#1} \\ {#2} \\ {#3} \end{smallmatrix} \right)}

\title{Appunti di algebra lineare e geometria}
\author{Giacomo Simonetto}
\date{Secondo semestre 2023-24}

\begin{document}

% -------------------------------------- Copertina e indice ---------------------------------------
\maketitle
\begin{abstract}
	Appunti del corso di algebra lineare e geometria della facoltà di Ingegneria Informatica dell'Università di Padova.
\end{abstract}

\newpage

\tableofcontents

\newpage

% ----------------------------------------- introduzione ------------------------------------------
\section{Definizioni  di anello commutativo e campo e spazio vettoriale}
\subsection{Anello commutativo}
Un anello commutativo con unità è un insieme in cui sono definite due operazioni \(+\) e \(\times\) e che soddisfa le seguenti
proprietà:
\begin{itemize}
	\item[-] per la somma \(+\):
	\begin{enumerate}
		\item proprietà associativa
		\item proprietà commutativa
		\item esistenza dell'elemento neutro
		\item esistenza dell'elemento opposto
	\end{enumerate}
	\item[-] per il prodotto \(\times\):
	\begin{enumerate}[resume]
		\item proprietà associativa
		\item proprietà commutativa
		\item proprietà distributiva
		\item esistenza dell'elemento neutro
	\end{enumerate}
\end{itemize}

\subsection{Campo}
Un campo \(k\) è un insieme in cui sono definite le due operazioni \(+\) e \(\times\), che soddisfa sia le 8 proprietà dell'anello
commutativo, sia la seguente:
\begin{enumerate}[topsep=3pt, itemsep=0pt] \setcounter{enumi}{8}
	\item esistenza dell'elemento inverso: \(\forall a \in k \; \text{con} \; a \neq 0 \;\; \exists b \in k \; \text{tale che} \; a \cdot b =  b \cdot a = 1\)
\end{enumerate}

In un campo \(k\) valgono le seguenti proprietà:
\begin{enumerate}[topsep=3pt, itemsep=0pt]
	\item si possono risolvere equazioni di primo grado: \(ax + b = 0 \quad \Rightarrow \quad x = -b/a\)
	\item per ogni elemento di \(k\) vale \(a \cdot 0 = 0\)
\end{enumerate}

\subsection{Spazio vettoriale}
Uno spazio vettoriale \(V\) su un campo \(k\) è un insieme di vettori non vuoto dotato delle operazioni:
\begin{enumerate}
	\item somma \(+\): \(\qquad \qquad \qquad \qquad \qquad f : \f{V \times V}{V}{(v, w)}{v + w} \)
	\item prodotto per uno scalare \(\times\): \(\qquad \;\; f : \f{k \times V}{V}{(\alpha, v)}{\alpha \cdot w}\)
\end{enumerate}
Inoltre devono valere le 8 proprietà di anello commutativo.
Gli elementi di uno spazio vettoriale si chiamano vettori.

\subsubsection*{Proprietà dell'elemento neutro}
Sia \(V\) uno spazio vettoriale su campo \(k\):
\begin{enumerate}[topsep=3pt, itemsep=0pt]
	\item \(0 \in k, \; v \in V \quad \Rightarrow \quad 0 \cdot v = \vec{0}\)
	\item \(\alpha \in k, \; \vec{0} \in V \quad \Rightarrow \quad \alpha \cdot \vec{0} = \vec{0}\)
\end{enumerate}

\subsubsection*{Proprietà dell'opposto}
Sia \(V\) uno spazio vettoriale su campo \(k\) e \(v \in V\), allora vale \(-1 \cdot v = -v\) e \(-v\) è l'opposto di \(v\). \\
Sia \(A\) un anello commutativo di uno spazio vettoriale (spazio vettoriale), allora l'opposto è unico.

\newpage


\section{Spazi vettoriali, sottospazi e vettori}
\subsection{Combinazione lineare di vettori}
Sia \(V\) uno spazio vettoriale su campo \(k\), dati \(v_1, \dots v_n \in V\) vettori e \(\alpha_1, \dots \alpha_n \in k\) scalari
allora possiamo costruire il vettore \(v = \alpha_1 v_1 + \alpha_2 v_2 + \dots + \alpha_n v_n\), chiamato combinazione lineare dei
vettori \(v_1, \dots v_n\).

\subsection{Vettori linearmente indipendenti}
Siano \(v_1, \dots v_n\) vettori di uno spazio vettoriale \(V\), i vettori sono linearmente indipendenti se l'unica soluzione di
\(\alpha_1 v_1 + \dots + \alpha_n v_n = \vec{0}\) è per \(\alpha_1 = \dots = \alpha_n = 0\).

Quando dei vettori non sono linearmente indipendenti, si dicono linearmente dipendenti ed è possibile scriverne uno come combinazione
lineare degli altri.

Se tra i vettori è presente il vettore nullo \(\vec{0}\), allora i vettori saranno linearmente dipendenti.

Se \(v_1, \dots v_n\) sono linearmente dipendenti e aggiungo altri vettori \(v_{n+1}, \dots v_m\), allora saranno ancora linearmente
dipendenti.

\subsection{Sottospazi vettoriali}
Sia \(V\) uno spazio vettoriale su campo \(k\), un sottospazio vettoriale di \(V\) è un sottoinsieme \(W\) di \(V\) che è sempre
sottospazio vettoriale secondo le stesse operazioni di \(V\).

Sia \(W\) un sottoinsieme di uno spazio vettoriale \(V\) (\(W \subseteq V\)), \(W\) è sottospazio vettoriale di \(V\) (\(W \leq V\))
se e solo se valgono le seguenti condizioni:
\begin{enumerate}[topsep=3pt, itemsep=0pt]
	\item \(W\) non è vuoto: \(W \neq \varnothing\)
	\item \(W\) è chiuso per la somma: \(w_1, w_2 \in W \quad \Rightarrow \quad w_1 + w_2 = w_3 \in W\)
	\item \(W\) è chiuso per il prodotto: \(\alpha \in k, \; w_1 \in W \quad \Rightarrow \quad \alpha w_1 = w_2 \in W\)
\end{enumerate}

Sia \(W \leq V\), allora \(\vec{0} \in W\), ovvero l'elemento nullo di \(V\) appartiene anche a \(W\).

Ogni spazio vettoriale \(V\) ha almeno due sottospazi vettoriali, ovvero \(W_1 = \{ \vec{0} \}\) e \(W_2 = V\), per cui
dato un generico sottospazio \(W\) di \(V\), vale \(\{ \vec{0} \} \leq W \leq V\).

\subsubsection*{Unione, somma e intersezione tra sottospazi vettoriali}
Siano \(U, W \leq V\) sottospazi di \(V\), spazio vettoriale su \(k\), vengono definite:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(U \cap W = \left\{ v \in U \land v \in W \right\}\) è l'intersezione, si dimostra che \(U \cap W \leq V\)
	\item[-] \(U \cup W = \left\{ v \in U \lor v \in W \right\}\) è l'unione, che si dimostra non essere sottospazi
	\item[-] \(U + W = \left\{ u + w \tc u \in U, \; w \in W\right\}\) è il più piccolo sottospazio vettoriale che
	contiene \(U\) e \(W\)
\end{itemize}

\subsubsection*{Sottospazio generato da vettori generatori}
Sia \(V\) uno spazio vettoriale su campo \(k\) e \(S \subseteq V\) un insieme di vettori di \(V\), allora il sottospazio vettoriale
generato da \(S\) è definito come il più piccolo sottospazio di \(V\) che contiene \(S\) e si indica \(L(S)\) o \(<S>\).

Sia \(S = \left\{ v_1, \dots v_n \right\}\), si dimostra che il sottospazio \(L(S) = \lambda_1 v_1 + \dots + \lambda_n v_n\) con
\(\lambda_1, \dots \lambda_n \in k\) è dato dalla combinazione lineare dei vettori di \(S\) chiamati vettori generatori.

Sia \(V\) spazio vettoriale su campo \(k\), un sottoinsieme \(S = \left\{ v_1, \dots v_n \right\}\) è detto insieme di generatori
di \(V\) se \(L(S) = V\), per cui ogni vettore di \(V\) si può scrivere come combinazione lineare dei vettori di \(S\).

\newpage

\subsection{Base di uno spazio vettoriale}
\begin{itemize}
	\item[-] Sia \(V\) uno spazio vettoriale su \(k\), una base di \(V\) è un sottoinsieme \(B\) di vettori che sono contemporaneamente
	generatori di \(V\) e linearmente indipendenti.
	
	\item[-] Uno stesso spazio vettoriale può avere più basi differenti.
	
	\item[-] Sia \(V\) spazio vettoriale con base \(\left\{ v_1, \dots v_n \right\}\), ogni vettore di \(V\) si scrive in modo
	unico come combinazione lineare dei vettori \(v_1, \dots v_n\).
	
	\item[-] Sia \(V\) spazio vettoriale e \(S = \left\{ v_1, \dots v_n \right\}\) sottoinsieme di \(V\), \(S\) è base di \(V\)
	se e solo se ogni vettore di \(V\) si scrive in modo unico come combinazione lineare dei vettori di \(S\).
	
	\item[-] Sia \(V\) spazio vettoriale con \(B = \left\{ v_1, \dots v_n \right\}\) base di \(V\) e sia \(v \in V\) vettore
	esprimibile come combinazione lineare dei vettori della base: \(v = \alpha_1 v_1 + \dots + \alpha_n v_n = \psmatrix{\alpha_1}{\dots}{\alpha_n}^B\),
	con \(\alpha_1, \dots \alpha_n\) coordinate di \(v\) rispetto alla base \(B\).
	
	\item[-] Uno spazio vettoriale si dice finitamente generato se è generato da un numero finito di vettori.
	
	\item[-] Sia \(V\) spazio vettoriale con \(v_1, \dots v_n\) insieme di generatori di \(V\), sia \(w = \alpha_1 v_1 + \dots + \alpha_n v_n\)
	vettore di \(V\), allora \(w, v_1, \dots v_{n-1}\) è insieme di generatori di \(V\).
	
	\item[-] Sia \(V\) spazio vettoriale \(\left\{ v_1, \dots v_n \right\}\) insieme di generatori di \(V\) e \(\left\{ w_1, \dots w_r \right\}\)
	vettori linearmente indipendenti di \(V\), allora \(r \leq n\).
	
	\item[-] Tutte le basi di uno spazio vettoriale hanno lo stesso numero di elementi, per cui si definisce la dimensione di uno
	spazio vettoriale come il numero di elementi di una sua base e si indica con \(\dim V\). Per convenzione \(V = \left\{ \vec{0} \right\}\)
	ha base \(B = \varnothing\) e dimensione \(\dim V = 0\).
	
	\item[-] Sia \(V\) spazio vettoriale e \(S = \left\{ v_1 \dots v_n \right\}\) insieme di generatori, da \(S\) si può estrarre
	una base di \(V\) (tenendo solo vettori linearmente indipendenti), e vale \(\dim V \leq S\).
	
	\item[-] Sia \(V\) spazio vettoriale con \(dim V = n\) e \(S = \left\{ v_1, \dots v_n \right\}\) vettori linearmente indipendenti,
	allora \(S\) può essere completato ad una base di \(V\) (aggiungendo altri vettori linearmente indipendenti).
	
	\item[-] Sia \(V\) spazio vettoriale e siano \(R,S \subseteq V\), allora \(R \subseteq S \; \Rightarrow \; L(R) \leq L(S)\)
	
	\item[-] Sia \(V\) spazio vettoriale con \(\dim V = n\), allora le seguenti proprietà si implicano a vicenda:
	\begin{enumerate}
		\item \(v_1, \dots v_n\) sono linearmente indipendenti
		\item \(v_1, \dots v_n\) generano \(V\)
		\item \(\left\{ v_1, \dots v_n \right\}\) è base di \(V\)
	\end{enumerate}
	
	\item[-] Sia \(V\) uno spazio vettoriale e \(U, W\) sottospazi di \(V\) con \(U \cap W\) e \(U + W\) sottospazi di \(V\), allora
	\(\dim (U + W) = \dim U + \dim W - \dim (U \cap W)\) detta Formula di Grassmann. Nel caso particolare in cui \(U \cap W = \left\{ \vec{0} \right\}\)
	si ha \(\dim (U \cap W) = 0\) e \(\dim (U + W) = \dim U + \dim W\) e i due sottospazi \(U\) e \(W\) si dicono in somma diretta
	e si scrive \(U + W = U \oplus W\).

	\item[-] Un vettore di \(U \oplus W\) si scrive in modo unico nella forma \(u + w\) con \(u \in U\) e \(w \in W\).
	
	\item[()] Sia \(V = k^n\) spazio vettoriale su campo \(k\) e sia \(U = \left\{ (x_1, \dots x_n) \tc \alpha_1 x_1 + \dots + \alpha_n x_n = 0\right\}\)
	per determinati \(\alpha_1, \dots \alpha_n \in k\) fissati, allora \(U \leq V\).
\end{itemize}

\newpage

\section{Funzioni e applicazioni lineari}
Siano \(V, W\) spazi vettoriali su campo \(k\), \(f: W \to W\) si dice lineare se valgono:
\begin{enumerate}
	\item \(f(v_1 + v_2) = f(v_1) + f(v_2) \qquad \qquad \;\; \forall v_1, v_2 \in V\)
	\item \(f(\lambda v) = \lambda f(v) \qquad \qquad \qquad \qquad \qquad \forall v \in V, \; \forall \lambda \in k\)
\end{enumerate}

\begin{itemize}
	\item[-] Sia \(V\) spazio vettoriale su \(k\) e \(f_1, \dots f_n : V \to k\) lineari, allora posso costruire la funzione
	\(f\) lineare definita come \(f: \f{V}{k^n}{v}{\psmatrix{f_1(v)}{\dots}{f_n(v)}}\)

	\item[-] Sia \(f: V \to W\) lineare, allora \(f(\vec{0}) = \vec{0}\)
	
	\item[-] Le funzioni lineari del tipo \(f: k^n \to k\) sono tutte e sole del tipo \(\f{k^n}{k}{\psmatrix{x_1}{\dots}{x_n}}{a_1 x_1 + \dots + a_n x_n}\)
	con \(a_1, \dots a_n \in k\)
\end{itemize}

\subsection{Omomorfismo e isomorfismo}
Una funzione o applicazione lineare è chiamata omomorfismo. \\
Una funzione lineare biiettiva è chiamata isomorfismo. \\
Una funzione lineare che ha lo stesso spazio vettoriale sia nel dominio che nel codominio si dice endomorfismo.

\begin{itemize}
	\item[-] Sia \(f: V \to W\) isomorfismo (\(f\) biiettiva), allora \(f\) è invertibile ed esiste \(f^-1: W \to V\) lineare
	inversa di \(f\).

	\item[-] Tutti gli spazi vettoriali su campo \(k\) di dimensione \(n\) sono isomorfi a \(k^n\) e l'isomorfismo (la funzione)
	dipende dalla base scelta negli spazi vettoriali.
\end{itemize}

\subsection{Nucleo e immagine di una funzione}
Siano \(V, W\) spazi vettoriali sul campo \(k\) e sia \(f: V \to W\) una funzione lineare, si definiscono:
\begin{enumerate}
	\item nucleo di f: \(\qquad \quad \ker f = \{ v \in V \tc f(\vec{v}) = \vec{0}\} \; \subseteq V\)
	\item immagine di f: \(\quad \;\;\;  \img f = \{ w \in W \tc \exists v \in V \; \text{per cui} \; f(v) = w\} \subseteq W\)
\end{enumerate}
Sia \(f: V \to W\) funzione lineare, allora:
\begin{enumerate}
	\item \(\ker f \leq V\) (è sottospazio)
	\item \(\img f \leq W\) (è sottospazio)
\end{enumerate}
Sia \(f: V \to W\) funzione lineare, \(f\) è iniettiva se e solo se \(\ker f = \{\vec{0}\}\) \\
Sia \(f: V \to W\) funzione lineare, \(f\) è suriettiva se e solo se \(\img f = W\) \\
Sia \(f: V \to W\) lineare, allora \(\dim \ker f + \dim \img f = \dim V\) \\
La nullità di \(f\) è \(\nul f = \dim \ker f\), il rango di \(f\) è \(\rg f = \dim \img f\), per cui vale che \(\nul f + \rg f = \dim V\)

\subsection{Funzioni e vettori}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] In generale una funzione non manda vettori linearmente indipendenti in vettori linearmente indipendenti, ma: (due successive)
	\item[-] Se \(f(v_1), \dots f(v_n)\) sono linearmente indipendenti, allora \(v_1, \dots v_n\) sono linearmente indipendenti.
	\item[-] Se \(f\) è iniettiva: \(v_1, \dots v_n\) linearmente indipendenti \(\Leftrightarrow\) \(f(v_1), \dots f(v_n)\) linearmente indipendenti.
	
	\item[-] Sia \(f: V \to W\) lineare, \(\{v_1, \dots v_n\}\) generatori di \(V\), allora \(\{w_1 = f(v_1), \dots w_n = f(v_n)\}\)
	sono generatori di \(W\).

	\item[-] Sia \(f: V \to W\) lineare, sia \(\{v_1, \dots v_n\}\) base \(V\), \(f\) è univocamente determinata dalla conoscenza di
	\(f(v_1), \dots f(v_n)\)

	\item[-] Siano \(V, W\) spazi vettoriali sul campo \(k\), sia \(\{v_1, \dots v_n\}\) base di \(V\) siano \(w_1, \dots w_n\)
	arbitrari vettori di \(W\), allora esiste ed è unica la funzione \(f: V \to W \tc f(v_1) = w_1, \dots f(v_n) = w_n\)
\end{itemize}

\newpage


\section{Matrici}
Sia \(M_{m,n}(k)\) uno spazio vettoriale i cui vettori sono definiti come matrici \(m \times n\) (\(m\) righe, \(n\) colonne):
\[M_{m,n}(k) = \left\{ \begin{pmatrix}
	a_{11} & a_{12} & \dots & a_{1n} \\
	a_{21} & a_{22} & \dots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \dots & a_{mn}
\end{pmatrix} = a(i,j) \qquad \text{con} \begin{matrix}
	a_{11}, \dots a_{mn} \in k \\
	i = 1,\dots m \\
	j = 1,\dots n	
\end{matrix} \right\}\]

\subsection{Operazioni}
\subsubsection*{Somma tra matrici}
\[\begin{pmatrix}
	a_{11} & a_{12} & \dots & a_{1n} \\
	a_{21} & a_{22} & \dots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \dots & a_{mn}
\end{pmatrix} + 
\begin{pmatrix}
	b_{11} & b_{12} & \dots & b_{1n} \\
	b_{21} & b_{22} & \dots & b_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	b_{m1} & b_{m2} & \dots & b_{mn}
\end{pmatrix} = 
\begin{pmatrix}
	a_{11} + b_{11} & a_{12} + b_{12} & \dots & a_{1n} + b_{1n} \\
	a_{21} + b_{21} & a_{22} + b_{22} & \dots & a_{2n} + b_{1n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} + b_{m1} & a_{m2} + b_{m2} & \dots & a_{mn} + b_{mn}
\end{pmatrix}\]

\subsubsection*{Prodotto per uno scalare}
\[\lambda \cdot \begin{pmatrix}
	a_{11} & a_{12} & \dots & a_{1n} \\
	a_{21} & a_{22} & \dots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \dots & a_{mn}
\end{pmatrix} = 
\begin{pmatrix}
	\lambda \cdot a_{11} & \lambda \cdot a_{12} & \dots & \lambda \cdot a_{1n} \\
	\lambda \cdot a_{21} & \lambda \cdot a_{22} & \dots & \lambda \cdot a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	\lambda \cdot a_{m1} & \lambda \cdot a_{m2} & \dots & \lambda \cdot a_{mn}
\end{pmatrix}\]

\subsubsection*{Prodotto righe per colonne tra matrici}
Si può eseguire solo se le colonne della prima sono dello stesso numero delle righe della seconda.
Siano \(A = (a_{ih}) \in M_{m,n}(k)\), \(B = (b_{hj}) \in M_{n,r}(k)\), \(C = (c_{ij}) \in M_{m,r}(k)\):
\[A \cdot B = C \quad \longrightarrow \quad C = (c_{ij}) = \sum_{h=1}^{n} a_{ih} \cdot b_{hj} = a_{i1} \cdot b_{1j} + a_{i2} \cdot b_{2j} + \dots + a_{in} \cdot b_{nj}\]

\subsubsection*{Proprietà del prodotto tra matrici}
\begin{enumerate}
	\item non vale la proprietà commutativa \(AB \neq BA\)
	\item proprietà associativa: \((AB)C = A(BC)\)
	\item proprietà distributiva: \((A+B)C = AC + BC\) e \(C(A+B) = CA + CB\)
	\item prodotto per scalare: \((\lambda A)B = \lambda (AB) = A (\lambda B)\)
	\item elemento neutro: \(I_n = \left( \begin{smallmatrix}
		1 & 0 & \cdots & \cdots \\
		0 & 1 & 0 & \cdots \\
		\cdots & 0 & 1 & 0 \\
		\cdots & \cdots & 0 & 1
	\end{smallmatrix} \right)\) matrice identica \(n \times n\)
\end{enumerate}

\subsubsection*{Matrici trasposte}
Sia \(A = (a_{ij}) \in M_{m,n}(k)\) la trasposta è \(A\tran = (a_{ji}) \in M_{n,m}(k)\), ovvero la matrice ottenuta scambiando le
righe e le colonne di \(A\).

\subsubsection*{Proprietà della trasposta}
\begin{enumerate}
	\item \((A\tran)\tran = A\)
	\item \((A+B)\tran = A\tran + B\tran\)
	\item \((AB)\tran = B\tran A\tran\)
\end{enumerate}

\subsubsection*{Matrici simili}
Due matrici quadrate \(A\), \(A'\) \(\in M_n(k)\) si dicono simili se esiste una matrice \(P \in M_n(k)\) invertibile per cui
\(A' = P \, A \, P^{-1}\).

Due matrici sono simili se e solo se rappresentano lo stesso endomorfismo rispetto ad opportune basi (vedere
\hyperlink{composizioneFunzioniCambiamentiDiBase}{composizione di funzioni lineari e funzioni di cambiamento di base}).

Due matrici simili hanno stesso rango, determinante e polinomio caratteristico.

\subsubsection*{Matrici simmetriche}
Una matrice si dice simmetrica se \(A \tran = A\)

\subsubsection*{Matrici ortogonali}
Una matrice si dice ortogonale se \(A^{-1} = A\tran\) \\
Le matrici ortogonali sono tutte della forma \(\left( \begin{matrix} \cos \alpha & - \sin \alpha \\ \sin \alpha & \cos \alpha \end{matrix} \right)\)
ovvero associate a rotazioni del piano e \(\det A\) di una matrice ortogonale è sempre e solo  \(+1\) o \(-1\).

\subsubsection*{Elementi (matrici) nilpotenti e divisori di 0}
Una matrice quadrata non nulla \(A \in M_n(k) \, \backslash \, \{ \vec{0} \}\) si dice nilpotente se \(A^k = 0\) per un certo valore di \(k\). \\
Due matrici non nulle \(A\), \(B\) sono divisori di \(0\) se \(A \cdot B = \vec{0}\).

\subsection{Operazioni elementari sulle righe}
Sia \(A\) una matrice in \(M_{m,n}(k)\), posso agire sulle righe senza modificare il sottospazio generato da esse e senza modificare
il rango di \(A\) con le seguenti operazioni lineari:
\begin{enumerate}[topsep=3pt, itemsep=0pt]
	\item scambiando tra loro due righe \(R_i\) e \(R_j\)
	\item moltiplicando la riga \(R_i\) per uno scalare \(\lambda \in k, \lambda \neq 0\)
	\item nel posto di \(R_i\) scrivo \(R_i + \beta R_j\) con \(i \neq j, \beta \in k\)
\end{enumerate}

\subsubsection*{Rango di una matrice}
Il rango di una matrice è la dimensione del sottospazio generato dalle colonne di essa o il massimo numero di colonne linearmente
indipendenti di essa.

\subsubsection*{Riduzione in forma a scala}
La matrice \(A\) è in forma a scala se in ogni riga di \(C\) il primo coefficiente non nullo (detto pivot) si trova a destra di
quello della riga precedente. (a meno che non sia la prima riga, o che la riga precedente sia tutta nulla).
\[A \; \text{in forma a scala} \;  = \begin{pmatrix}
	\cdots & 0 & * & \cdots & \cdots & \cdots \\
	\cdots & \cdots & 0 & * & \cdots & \cdots \\
	\cdots & \cdots & \cdots & 0 & * & \cdots \\
	\cdots & \cdots & \cdots & \cdots & 0 & 0
\end{pmatrix}\]
Il rango di una matrice in forma a scala è il numero delle righe non nulle ed è anche il numero di pivot.

\subsubsection*{Sottospazi generati dalle righe e colonne di una matrice ridotta a scala}
Sia \(A'\) la matrice ridotta a scala di \(A\), siano \(R_1, \dots R_m\) le righe di \(A\), siano \(C_1, \dots C_n\) le colonne di \(A\)
\begin{enumerate}
	\item una base del sottospazio generato dalle righe di \(A\) è dato dalle righe non nulle di \(A'\)
	\item una base del sottospazio generato dalle colonne di \(A\) è dato dalle colonne di \(A\) (NON \(A'\)) corrispondenti alle
	colonne dei pivot di \(A'\)
\end{enumerate}

\subsubsection*{Metodo di eliminazione di Gauss}
Il sistema \(S: AX = B\) ha le stesse soluzioni di un sistema \(S': CX = D\) dove \((C|D)\) è ottenuto mediante operazioni elementari
sulla matrice completa \((A|B)\). Questo si utilizza per ricondursi a matrici più semplici da studiare (es. in forma a scala).

\subsubsection*{Matrici elementari}
Le matrici elementari sono matrici quadrate e invertibili ottenute dalle funzioni di operazioni lineari sulle righe delle matrici.
\begin{enumerate}
	\item la matrice per scambiare tra loro due righe si ottiene dalla matrice identica in cui sono state scambiate due righe
	come nell'esempio seguente, scambiando le righe 1 e 2:
	\[M_{1,2} = \begin{pmatrix}
		0 & 1 & 0 \\
		1 & 0 & 0 \\
		0 & 0 & 1
	\end{pmatrix} \qquad (\det A' = -\det A)\]
	si osserva che \(\rg P(i,j) = \rg I_n = n\), \(P(i,j)^{-1} = P(i,j)\)
	
	\item la matrice per moltiplicare una riga per uno scalare \(\lambda\) si ottiene dalla matrice identica con \(\lambda\)
	al posto dell'1, nella riga da moltiplicare, come nell'esempio moltiplicando la riga 2 per \(\pi\):
	\[M_2\pi = \begin{pmatrix}
		1 & 0 & 0 \\
		0 & \pi & 0 \\
		0 & 0 & 1
	\end{pmatrix} \qquad (\det A' = \lambda \det A)\]
	
	\item la matrice per ottenere una composizione lineare di righe si ottiene inserendo il coefficiente \(\lambda\) al posto
	dello 0, nella riga da modificare, nella colonna con lo stesso indice della riga che si vuole sommare, come nell'esempio
	con \(R_2 = R_2 + \lambda R_3\):
	\[M_{2,3}(\lambda) = \begin{pmatrix}
		1 & 0 & 0 \\
		0 & 1 & \lambda \\
		0 & 0 & 1
	\end{pmatrix} \qquad (\det A' = \det A)\]
\end{enumerate}

Per ottenere una generica matrice \(B\) che raggruppa più operazioni elementari eseguite su una matrice \(A\), ottenendo \(A'\)
si può procedere come prodotto delle matrici delle diverse operazioni elementari eseguite (\hyperlink{composizioneFunzioniCambiamentiDiBase}{funzioni composte}),
oppure si osserva che: \(B \cdot (A|I_n) = (BA | BI_n) = (A'|B)\). Quindi basta considerare la matrice \((A|I_n)\) e applicare
le operazioni elementari per ottenere \(A'\) al posto di \(A\) e in questo modo si avrà \(B\) al posto di \(I_n\).

\newpage

\section{Funzioni e matrici}
\subsubsection*{Funzioni come spazio vettoriale}
Siano \(f, g: V \to W\) funzioni lineari, definiamo:
\begin{itemize}
	\item[-] somma di funzioni: \(\qquad \qquad \qquad f + g: \f{V}{W}{v}{f(v) + g(v)}\)
	\item[-] prodotto funzione per scalare: \(\qquad \; \lambda f: \f{V}{W}{v}{\lambda f(v)} \qquad \text{con} \; \lambda \in k\)
\end{itemize}
In questo modo è possibile definire uno spazio vettoriale dato dall'insieme \(\hom(V,W)\), di tutti gli omomorfismi \(f: V \to W\)

\subsubsection*{Parallelismo funzioni matrici}
Siano \(f, g: V \to W\) funzioni lineari, siano \(\underline{v} = \{ v_1, \dots v_n \}\) base di \(V\) con \(\dim V = n\) e
\(\underline{w} = \{ w_1, \dots w_m \}\) base di \(W\) con \(\dim W = m\), si possono definire le matrici:
\begin{itemize}
	\item[-] \(A = (a_{ij}) = \m{v}{w}(f) \in M_{m,n}(k)\)
	\item[-] \(B = (b_{ij}) = \m{v}{w}(g) \in M_{m,n}(k)\)
\end{itemize}
Inoltre le operazioni tra \(f\) e \(g\) si possono definire in termini di matrici:
\begin{itemize}
	\item[-] somma di funzioni: \(\qquad \qquad \qquad f + g = \m{v}{w}(f + g) = \m{v}{w}(f) + \m{v}{w}(g)\)
	\item[-] prodotto funzione per scalare: \(\qquad \;\lambda f = \m{v}{w}(\lambda f) = \lambda \m{v}{w}(f)\)
	\item[-] composizione di funzioni: \(\qquad \quad \; g \circ f = \m{v}{z}(g \circ f) = \m{w}{z}(g) \cdot \m{v}{w}(f)\)
	\item[] definita con \(f: V \to W\), \(g: W \to Z\) e con \(\underline{v}\), \(\underline{w}\), \(\underline{z}\) basi di \(V, W, Z\)
\end{itemize}

\subsubsection*{Funzioni come prodotto matrice vettore}
Sia \(f: V \to W\) funzione lineare, siano \(\underline{v} = \{ v_1, \dots v_n \}\) base di \(V\) con \(\dim V = n\) e
\(\underline{w} = \{ w_1, \dots w_m \}\) base di \(W\) con \(\dim W = m\):
\begin{itemize}
	\item[-] definisco gli isomorfismi \(\text{iso}_1: \f{V}{k^n}{v}{\psmatrix{\lambda_1}{\dots}{\lambda_n}^{\underline{v}}}\) e \(\text{iso}_2: \f{W}{k^m}{w}{\psmatrix{\mu_1}{\dots}{\mu_2}^{\underline{w}}}\)
	\item[-] definisco l'operazione \(L_A: \f{k^n}{k^m}{\psmatrix{\lambda_1}{\dots}{\lambda_n}^{\underline{v}}}{\psmatrix{\mu_1}{\dots}{\mu_2}^{\underline{w}}} \qquad \)
	con \(A = \m{v}{w}(f) \in M_{m,n}(k), \quad \psmatrix{\mu_1}{\dots}{\mu_2}^{\underline{w}} = A \psmatrix{\lambda_1}{\dots}{\lambda_n}^{\underline{v}}\)
\end{itemize}
Per cui dato un vettore \(v = \lambda_1 v_1 + \dots + \lambda_n v_n \in V\) e \(w = \mu_1 w_1 + \dots + \mu_m w_m \in W\),
vale che: \[w = f(v) \quad \Rightarrow \quad \psmatrix{\mu_1}{\dots}{\mu_2}^{\underline{w}} = \m{v}{w}(f) \cdot \psmatrix{\lambda_1}{\dots}{\lambda_n}^{\underline{v}}\]

Ovvero ogni funzione, scelte una base del dominio e una del codominio, è possibile scriverla come matrice \(\dim W \times \dim V\),
e per applicare la funzione ad un vettore, si moltiplica a sinistra il vettore dato dalle coordinate rispetto alla base del dominio
per la matrice, ottenendo le coordinate del vettore rispetto alla base del codominio.

Per costruire la matrice relativa alla funzione si uniscono le coordinate dei vettori (verticali), secondo la base del codominio,
dati dall'applicazione della funzione ai vettori della base del dominio come di seguito: \\
sia \(f:V \to W\), \(\underline{v} = \{v_1, \dots v_n\}\) base di \(V\), \(\underline{w} = \{w_1, \dots w_m\}\) base di \(W\):
\[f: \begin{matrix}
	f(v_1) = a_1 w_1 + a_2 w_2 + \dots + a_m w_m \\
	f(v_2) = b_1 w_1 + b_2 w_2 + \dots + b_m w_m \\
	\dots \\
	f(v_n) = z_1 w_1 + z_2 w_2 + \dots + z_m w_m
\end{matrix} \quad \longrightarrow \quad A = \m{v}{w}(f)
\begin{aligned}
	&= \left( \begin{matrix}
		a_1 & b_1 & \cdots & z_1 \\
		a_2 & b_2 & \cdots & z_2 \\
		\vdots & \vdots & \ddots & \vdots \\
		a_m & b_m & \cdots & z_m
	\end{matrix} \right) \\
	&= \; \left(f(v_1), f(v_2), \dots f(v_n)\right) \text{ rispetto a } \underline{w}
\end{aligned}\]

\newpage

\subsubsection*{Immagine di una funzione e rango di una matrice}
Sia \(f:V \to W\) lineare, \(\underline{v} = \{v_1, \dots v_n\}\) base di \(V\), \(\underline{w} = \{w_1, \dots w_m\}\) base di \(W\),
allora l'immagine di \(f\) è il sottospazio generato dalle colonne di \(A = \m{v}{w}(f)\), ovvero \(\img f = L(f(v_1), \dots f(v_n))\).

\subsubsection*{Matrice del cambiamento di base}
Sia \(V\) spazio vettoriale su \(k\) con basi \(\underline{v} = \{v_1, \dots v_n\}\) e \(\underline{v'} = \{v_1', \dots v_n'\}\).
Un vettore \(v\) si può esprimere in due modi diversi secondo i due sistemi di coordinate:
\[v \;\; = \;\; \lambda_1 v_1 + \dots + \lambda_n v_n = \begin{pmatrix} \lambda_1 \\ \cdots \\ \lambda_n \\ \end{pmatrix}^{\underline{v}}
	\;\; = \;\;	\lambda_1' v_1' + \dots + \lambda_n' v_n' = \begin{pmatrix} \lambda_1' \\ \cdots \\ \lambda_n' \\ \end{pmatrix}^{\underline{v'}}\]
Costruisco la funzione identità \(id: \f{V_{\underline{v}}}{V_{\underline{v'}}}{v}{v'}\) e la relativa matrice in cui le colonne
sono le coordinate dei vettori della base \(\underline{v}\) rispetto alla base \(\underline{v'}\), come segue:
\[f: \begin{matrix}
	v_1 = a_1 v_1' + a_2 v_2' + \dots + a_n v_n' \\
	v_2 = b_1 v_1' + b_2 v_2' + \dots + b_n v_n' \\
	\dots \\
	v_n = z_1 v_1' + z_2 v_2' + \dots + z_n v_n'
\end{matrix} \quad \longrightarrow \quad A = \m{v}{v'}(f) = \left( \begin{matrix}
	a_1 & b_1 & \cdots & z_1 \\
	a_2 & b_2 & \cdots & z_2 \\
	\vdots & \vdots & \ddots & \vdots \\
	a_n & b_n & \cdots & z_n
\end{matrix} \right)\]
Per cui si ha che \(\begin{pmatrix} \lambda_1' \\ \cdots \\ \lambda_n' \\ \end{pmatrix}^{\underline{v'}} = \m{v}{v'} \cdot \begin{pmatrix} \lambda_1 \\ \cdots \\ \lambda_n \\ \end{pmatrix}^{\underline{v}}\)

Le matrici del cambiamento di base (essendo isomorfismi), sono invertibili e la loro inversa si ottiene scambiando le basi utilizzate:
\(\left( \m{v}{v'}(id) \right)^{-1} = \m{v'}{v}(id)\) e si ha che \(\m{v}{v'} \cdot \m{v'}{v} = I_n\)

\hypertarget{composizioneFunzioniCambiamentiDiBase}{\subsubsection*{Composizione di funzioni lineari tra spazi vettoriali e di funzioni di cambiamento di base}}
Siano \(V, W\) spazi vettoriali con \(\underline{v}\), \(\underline{v'}\) basi di \(V\) e \(\underline{w}\), \(\underline{w'}\)
basi di \(W\) e \(f: V \to W\), si ha che:
\begin{itemize}
	\item[-] \(\text{id}_1: V_{\underline{v}} \to V_{\underline{v'}} \qquad P = \m{v}{v'} \quad P^{-1} = \m{v'}{v} \qquad \qquad \text{id}_2: W_{\underline{w}} \to W_{\underline{w'}} \qquad S = \m{w}{w'} \quad S^{-1} = \m{w'}{w}\)
	\item[-] \(f: V_{\underline{v}} \to W_{\underline{w}} \qquad \; A = \m{v}{w} \qquad \qquad \qquad \qquad \qquad \;\; f': V_{\underline{v'}} \to W_{\underline{w'}} \qquad \;\; A' = \m{v'}{w'}\)
	\item[-] \(f' = id_2 \circ f \circ {id_1}^{-1} \quad \Rightarrow \quad \m{v'}{w'} = \m{w}{w'} \cdot \m{v}{w} \cdot \m{v'}{v} \quad \Rightarrow \quad A' = S \, A \, P^{-1}\)
\end{itemize}
Se la funzione \(f\) è un endomorfismo (es. \(f: V \to V\)), si ottiene che \(S = P\), per cui \(A' = P \, A \, P^{-1}\), ovvero
le matrici \(A\) e \(A'\) sono simili.

\newpage

\section{Sistemi di equazioni lineari}
Un sistema lineare di equazioni lineari a coefficienti in campo \(k\) è un insieme \(S\) di equazioni del tipo:
\[S \; : \; \begin{cases}
	a_{11} x_1 + a_{12} x_2 + \dots + a_{1n} x_n = b_1 \\
	a_{21} x_1 + a_{22} x_2 + \dots + a_{2n} x_n = b_2 \\
	\dots \\
	a_{m1} x_1 + a_{m2} x_2 + \dots + a_{mn} x_n = b_m
\end{cases} \quad \Rightarrow \quad
\begin{pmatrix}
	a_{11} & a_{12} & \dots & a_{1n} \\
	a_{21} & a_{22} & \dots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \dots & a_{mn}
\end{pmatrix} \cdot
\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} =
\begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{pmatrix}\]

\subsubsection*{Matrici complete e incomplete}
La matrice \(A\) si chiama matrice incompleta di \(S\): \(A = \begin{pmatrix}
	a_{11} & a_{12} & \dots & a_{1n} \\
	a_{21} & a_{22} & \dots & a_{2n} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{m1} & a_{m2} & \dots & a_{mn}
\end{pmatrix}\) \\
La matrice \((A|B)\) si chiama matrice completa di \(S\): \((A|B) = \begin{pmatrix}
	a_{11} & a_{12} & \dots & a_{1n} & | & b_1 \\
	a_{21} & a_{22} & \dots & a_{2n} & | & b_2 \\
	\vdots & \vdots & \ddots & \vdots & | & \vdots \\
	a_{m1} & a_{m2} & \dots & a_{mn} & | & b_n
\end{pmatrix}\)

\subsection{Soluzioni di un sistema lineare}
\begin{itemize}
	\item[-] L'insieme delle soluzioni di un sistema lineare \(S : AX = B\) è l'insieme dei vettori \(X\) delle incognite tali che
	tutte le equazioni siano verificate contemporaneamente. Nel caso in cui \(A\) sia invertibile, il sistema ha un'unica soluzione
	\(X = A^{-1} B\)
	\item[-] Un sistema lineare ha soluzioni se e solo se \(B \in \img f\), con \(f: \f{k^n}{k^m}{X}{AX}\). Si osserva che \(\img f\)
	corrisponde allo spazio vettoriale generato dalle colonne di \(A\), per cui \(B \in \img f \leftrightarrow B \in <C_{A_1}, C_{A_2}, \dots C_{A_n}>\) 
\end{itemize}

\subsubsection*{Sistemi lineari omogenei associati}
\begin{itemize}
	\item[-] Il sistema lineare \(S: AX = B\) si dice omogeneo se \(B = \vec{0}\) (\(S:AX = \vec{0}\)) e l'insieme delle soluzioni
	è \(X = \ker f\), con \(f: \f{k^n}{k^m}{X}{AX}\).
	\item[-] Il sistema lineare \(S: AX = B\) ha come soluzione uno spazio vettoriale se e solo se \(S\) è omogeneo.
	\item[-] Sia \(S: AX = B\) un sistema lineare, \(S' : AX = \vec{0}\) si dice sistema omogeneo associato ad \(S\).
	\item[-] Sia \(S: AX = B\) un sistema lineare, le soluzioni \(X\) del sistema sono tutte e sole della forma
	\(\sol S = \left\{ \overline{x} + y \tc y \in \ker f \right\} = \overline{x} + \ker f\), dove \(\overline{x}\) è una soluzione
	particolare di \(S\) e \(\ker f\) è la soluzione generale del sistema omogeneo associato ad \(S\).
\end{itemize}

\subsubsection*{Teorema di Rouché-Capelli}
Sia \(S: AX = B\) un sistema lineare di \(n\) equazioni in \(n\) incognite, e \(A = M_{m,n}(k)\), allora:
\begin{enumerate}
	\item \(S\) ha soluzioni se e solo se \(\rg A = \rg (A|B)\)
	\item se \(S\) ha soluzioni, \(\rg A = \rg (A|B) = r\) (eq. lin. indipendenti), \(n\) numero incognite allora:
	\begin{itemize}
		\item[-] se \(r = n\), \(S\) ha una sola soluzione
		\item[-] se \(r < n\), si sono infinte (\(\infty^{r-n}\)) soluzioni che dipendono da \(n-r\) parametri
	\end{itemize}
\end{enumerate}

\newpage

\section{Matrici invertibili e inverse}
Una matrice si dice invertibile se esiste \(B\) tale che \(AB = BA = I_n\). \\
Se \(B\) esiste, è la matrice inversa di \(A\) e si indica \(B^{-1}\).

Per calcolare l'inversa di \(A\), eseguo operazioni elementari sulle righe sulla matrice \((A|I_n)\) in modo da ricondurmi ad una
matrice \((I_n|A^{-1})\), dove \(I_n\) è la matrice identica e \(A^{-1}\) è la matrice inversa. Questo avviene per come è definita
la combinazione di matrici di operazioni elementari sulle righe (come spiegato dopo).

Sia \(A \in M_n(k)\) matrice quadrata di ordine \(n\): \\
\(A\) è invertibile \(\Leftrightarrow\) \(f\) è invertibile \(\Leftrightarrow\) \(f\) è isomorfismo \(\Leftrightarrow\) \(\dim \img f = n\)
\(\Leftrightarrow\) \(\rg A = n\)

\subsection{Funzioni e matrici invertibili}
\subsubsection*{Inversa sinistra}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] data una funzione \(f: V \to W\), la funzione \(g: W \to V\) si dice inversa sinistra di \(f\) se \(g \circ f = id_V\)
	\item[-] \(f\) ha inversa sinistra se e solo se \(f\) è iniettiva (\(\ker f = \left\{\vec{0}\right\}\))
	\item[-] data una matrice \(A \in \m{m}{n}(k)\), tale matrice ha inversa sinistra \(S \in \m{n}{m}(k)\) se \(SA = I_n\) e di
	conseguenza \(\rg A = n\)
\end{itemize}

\subsubsection*{Inversa destra}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] data una funzione \(f: V \to W\), la funzione \(h: W \to V\) si dice inversa destra di \(f\) se \(f \circ h = id_W\)
	\item[-] \(f\) ha inversa sinistra se e solo se \(f\) è suriettiva (\(\img f = W\))
	\item[-] data una matrice \(A \in \m{m}{n}(k)\), tale matrice ha inversa destra \(D \in \m{n}{m}(k)\) se \(AD = I_m\) e di
	conseguenza \(\rg A = m\)
\end{itemize}

\subsubsection*{Funzioni e matrici invertibili}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se una funzione \(f\) ha sia inversa destra che inversa sinistra, allora è biiettiva, ovvero è un isomorfismo e ha
	inversa unica \(g = h = f^{-1} : W \to V\)
	\item[-] se una matrice ha inversa destra e inversa sinistra, allora è matrice associata ad un isomorfismo ed ha inversa unica
	\(D = S = A^{-1}\), con \(\rg A = n = m\), ovvero se è quadrata
	\item[-] una matrice quadrata è invertibile se e solo se \(\det A \neq 0\)
\end{itemize}

\newpage

\section{Determinanti}
\subsection{Definizione}
Il determinante si definisce solo per matrici quadrate, se \(\det A \neq 0\), allora la matrice è invertibile.
\[\det A = \sum_{\sigma \in S_n} \sgn(\sigma) \; a_{1\sigma(1)} \; a_{2\sigma(2)} \; \dots \; a_{n\sigma(n)}\]
con \begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\sigma\) generica permutazione
	\item[-] \(S_n\) insieme delle permutazioni in \(\left\{ 1 \dots n \right\}\)
	\item[-] \(\sgn(\sigma)\) positivo se la permutazione si ottiene con un numero pari di scambi, negativo altrimenti
\end{itemize}
Esempio per \(n = 2\)
\[det A = \sum_{\sigma \in S_2} \sgn(\sigma) \; a_{1 \sigma(1)} \; a_{2 \sigma(2)} = \sgn(\sigma_1) \; a_{1 \sigma_1(1)} \; a_{2 \sigma_1(2)} + \sgn(\sigma_2) \; a_{1 \sigma_2(1)} \; a_{2 \sigma_2(2)} = a_{11} \; a_{22} -\; a_{12} \; a_{21}\]
Esempio per \(n = 3\)
\begin{align*}
	\det A &= \sum_{\sigma \in S_3} \sgn(\sigma) \; a_{1 \sigma(1)} \; a_{2 \sigma(2)} \; a_{3 \sigma(3)} \\
	&= \sgn(\sigma_1) \, a_{1 \sigma_1(1)} \, a_{2 \sigma_1(2)} \, a_{3 \sigma_1(3)} + \sgn(\sigma_2) \, a_{1 \sigma_2(1)} \, a_{2 \sigma_2(2)} \, a_{3 \sigma_2(3)} + \dots + \sgn(\sigma_6) \, a_{1 \sigma_6(1)} \, a_{2 \sigma_6(2)} \, a_{3 \sigma_6(3)} \\
	&= a_{11} \; a_{22} \; a_{33} - a_{11} \; a_{23} \; a_{32} - a_{13} \; a_{22} \; a_{31} - a_{12} \; a_{21} \; a_{33} + a_{12} \; a_{23} \; a_{31} + a_{13} \; a_{21} \; a_{32}
\end{align*}

\subsection{Regola di Sarrus (det 3x3)}
\[\det A = \det \left( \begin{matrix}
	a_{11} & a_{12} & a_{13} \\
	a_{21} & a_{22} & a_{23} \\
	a_{31} & a_{32} & a_{33}
\end{matrix} \right) = a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21} a_{32} - a_{13} a_{22} a_{31} - a_{11} a_{23} a_{32} - a_{12} a_{21} a_{33}\]
diagonali alto-sx \(\to\) basso-dx con segno \(+\), diagonali alto-dx \(\to\) basso-sx con segno \(-\)

\subsection{Proprietà dei determinanti}
\begin{enumerate}
	\item[1.1] \(\det\) matrice diagonale \(=\) prodotto elementi sulla diagonale principale
	\item[1.2] \(\det\) matrice triangolare superiore \(=\) prodotto elementi sulla diagonale principale
	\item[1.3] \(\det\) matrice triangolare inferiore \(=\) prodotto elementi sulla diagonale principale
	\item[1.4] \(\det \left( \begin{matrix} A & B \\ 0 & C \end{matrix} \right) = \det A \cdot \det C \qquad\) con \(A, B, C\) sottomatrici quadrate di matrice quadrata
	\item[2] \(\det A\tran = \det A\), ovvero le proprietà che valgono per le righe valgono anche per le colonne
	\item[3.1] \(\det \left( \begin{matrix} \dots \\ \alpha v + \alpha' v' \\ \dots \end{matrix} \right) = \alpha \det \left( \begin{matrix} \dots \\ v \\ \dots \end{matrix} \right) + \alpha' \det \left( \begin{matrix} \dots \\ v' \\ \dots \end{matrix} \right)\)
	\item[3.2] \(\det A' = -\det A\), con \(A'\) ottenuta scambiando due righe (o colonne) tra loro (matrice elem. \(M_{i,j}\))
	\item[3.3] \(\det A' = \lambda \det A\), con \(A'\) ottenuta moltiplicando una riga (o colonna) per uno scalare \(\lambda\) (\(M_{i}\lambda\))
	\item[3.4] \(\det A = 0\) se ci sono due righe (o colonne) uguali.
	\item[3.5] se ad una riga (o colonna) ci sommo combinazioni lineari di altre righe (o colonne), il determinante non cambia (matrice elementare \(M_{i,j}(\lambda)\))
	\item[4.1] teorema di Binet: \(\det (AB) = \det A \cdot \det B\)
	\item[4.2] corollario: \(\det A^{-1} = \frac{1}{\det A}\)
\end{enumerate}

\subsection{Formule di Laplace}
\[\det A = \sum_{j=1}^{n} (-1)^{i+j} \; a_{ij} \; \det A_{ij} = a_{ij} \; {a_{ij}}^* \qquad \text{ fissata la riga i}\]
\[\det A = \sum_{i=1}^{n} (-1)^{i+j} \; a_{ij} \; \det A_{ij} = a_{ij} \; {a_{ij}}^* \qquad \text{ fissata la colonna j}\]
con \(A_{ij}\) matrice ottenuta cancellando la riga \(i\) e la colonna \(j\) \\
con \({a_{ij}}^* = (-1)^{i+j} \; \det A_{ij}\) complemento algebrico \\
con \(A^* = \left( {a_{ij}}^* \right)\) matrice dei complementi algebrici

\subsection{Inversa di una matrice con determinante}
\[A^{-1} = \frac{1}{\det A} A^* \qquad \text{ con } \det A \neq 0\]

\subsection{Formula di Cramer}
Sia \(S: AX = B\) sistema di \(n\) equazioni con \(n\) incognite e \(det A \neq 0\), allora il sistema ha un'unica soluzione \(X = (x_1, x_2, \dots x_n)\)
data da: \[x_i = \frac{\det A_i}{\det A}\] con \(A_i\) matrice \(A\) con la colonna \(B\) nel posto \(i\)-esimo

\newpage


\section{Autovalori, autovettori e diagonalizzabilità}
Gli autovettori sono vettori linearmente indipendenti che costituiscono una base per la quale la matrice associata ad un endomorfismo
è diagonale e la diagonale è costituita dagli autovalori associati agli autovettori.

\subsection{Autovalori e polinomio caratteristico}
Gli autovalori di una matrice \(A\) sono le soluzioni del polinomio caratteristico \(p(\lambda) = \det (A - \lambda I_n)\). Ogni
autovalore \(\lambda\) ha una propria molteplicità algebrica \(m_a(\lambda)\) data dal numero di volte che compare nelle soluzioni
del polinomio caratteristico. La somma delle molteplicità algebriche di tutti gli autovalori è inferiore a \(n\) (righe della matrice).

\subsection{Autovettori e autospazi}
Per trovare gli autovettori corrispondenti agli autovalori, bisogna risolvere il sistema omogeneo \((A-\lambda I_n)v = \vec{0}\)
dove \(A\) è la matrice, \(\lambda\) è l'autovalore e \(v\) è l'autovettore da trovare. L'insieme delle soluzioni del sistema è
un sottospazio associato all'autovalore, detto autospazio \(E_A(\lambda) = \ker (A-\lambda I_n)\). Ogni autovalore ha una molteplicità
geometrica \(m_g(\lambda) = \dim E_A(\lambda)\) data dalla dimensione dell'autospazio.

\subsection{Matrice diagonale e diagonalizzabilità}
Una matrice \(A\) è diagonalizzabile se simile ad una matrice diagonale \(D\), ovvero se esiste una base di \(k^n\) fatta di
autovettori \(A\), ovvero se \(A\) ammette \(n\) autovettori linearmente indipendenti. In questo modo la matrice diagonale è
costituita dagli autovalori di \(A\) lungo la diagonale (corrispondenti agli autovettori della base, l'ordine è importante).
\[D = S^{-1} A S \quad \Leftrightarrow \quad AS = SD \quad \Leftrightarrow \quad A = S D S^{-1}\]
con \(A\) matrice di partenza, \(D\) matrice diagonale simile ad \(A\), \(S\) matrice con autovettori come colonne

\subsubsection*{Criteri di diagonalizzabilità}
Una matrice è diagonalizzabile se e solo se ha \(n\) autovalori distinti o se \(m_g(\lambda) = m_a(\lambda)\) per ogni autovalore. \\
Una matrice è sempre diagonalizzabile in \(\mathbb{C}\), ma non sempre in \(\mathbb{R}\). \\
Una matrice simmetrica ha sempre \(n\) autovalori distinti. \\
Alcune osservazioni:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se una matrice ha \(n\) autovettori linearmente indipendenti, allora è diagonalizzabile
	\item[-] un teorema garantisce che autovettori associati a autovalori distinti, sono linearmente indipendenti
	\item[-] se una matrice \(n \times n\) ha \(n\) autovalori distinti, allora è diagonalizzabile
	\item[-] la molteplicità geometrica è sempre minore o uguale a quella algebrica \(1 \leq m_g(\lambda) \leq m_a(\lambda)\)
	\item[-] se una matrice ha \(m_g(\lambda) = m_a(\lambda)\) per ogni autovalore, allora è diagonalizzabile
	\item[-] \(A\) è diagonalizzabile \(\quad \Leftrightarrow \quad p_A(\lambda) = (\lambda_1-\lambda)^{m_1} \cdot (\lambda_2-\lambda)^{m_2} \dots (\lambda_r-\lambda)^{m_r}\)
	con \(\lambda_i\) distinti autovalori e \(m_i = m_a(\lambda_i) = m_g(\lambda_i) = \dim E_A(\lambda_i)\).
\end{itemize}

\newpage


\section{Ortogonalità e prodotto scalare}
\subsection{Prodotto scalare}
In \(\mathbb{R}^n\) il prodotto scalare tra due vettori \(v = (a_1 \; a_2 \; a_3)\), \(w = (b_1 \; b_2 \; b_3)\) è uno scalare:
\[v \cdot w = v \tran \; w =(a_1 \; a_2 \; \dots \; a_n) \left(\begin{matrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{matrix}\right) = a_1b_1 + a_2b_2 + \dots + a_nb_n\]

\subsubsection*{Lunghezze e angoli}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(v \cdot v = {a_1}^2 + {a_2}^2 + \dots + {a_n}^2 = ||v||^2\)
	\item[-] \(||v|| = \sqrt{v \tran \; v}\) è detta norma o modulo del vettore ed è la lunghezza del vettore
	\item[-] \(v \cdot w = ||v|| \; ||w|| \cos \alpha \Rightarrow \cos \alpha = \displaystyle \frac{v \cdot w}{||v|| \; ||w||}\) con \(\alpha\) angolo tra i vettori
\end{itemize}

\subsubsection*{Aree e volumi}
L'area del parallelogramma di lati \(v, w\) è \(A = v \cdot w = ||v|| \cdot ||w|| \cdot \sin \alpha = \sqrt{\det \left( \begin{matrix} v \cdot v & v \cdot w \\ w \cdot v & w \cdot w \end{matrix} \right)}\) \\
Il volume del parallelepipedo di lati \(v, w, u\) è \(V = v \cdot w \cdot u = \sqrt{ \det \left( \begin{matrix} u \cdot u & u \cdot v & u \cdot w \\ v \cdot u & v \cdot v & v \cdot w \\ w \cdot u & w \cdot v & w \cdot w \end{matrix} \right)}\)

\subsubsection*{Disuguaglianza di Cauchy-Schwarz}
\[|v \cdot w| \leq ||v|| \; ||w|| \qquad \qquad |v \cdot w| \leq ||v|| \; ||w|| \Leftrightarrow v, w \text{ linearmente indipendenti}\]
\[-1 \leq \frac{v \cdot w}{||v|| \; ||w||} \leq 1 \qquad \text{conseguenza del teorema e condizione di esistenza } \cos \alpha\]

\subsubsection*{Proprietà}
\begin{enumerate}
	\item[1.] \(u \cdot u = ||u||^2 \geq 0\)
	\item[2.] \(v \cdot u = u \cdot v\)
	\item[3.] \(\vec{0} \cdot v = v \cdot \vec{0}\)
	\item[4.] \((\lambda u) \cdot v = u \cdot (\lambda v) = \lambda (u \cdot v)\)
	\item[5.1] \((u + v) \cdot z = u \cdot z + v \cdot z\)
	\item[5.2] \(u \cdot (v + z) = u \cdot v + u \cdot z\)
\end{enumerate}

\subsection{Vettori ortogonali}
Due vettori \(v,w\) si dicono ortogonali se e solo se \(v \cdot w = 0\) (con \(v, w \neq \vec{0}\)) \\
Dato un vettore \(v\), per trovare la sua proiezione lungo un altro vettore \(u\), si impone la condizione di perpendicolarità
\(v = v_\parallel + v_\perp\) con \(v_\parallel = \lambda u\) e \(v_\perp \cdot u = 0\) per cui si ottiene:
\[(v - \lambda u) \cdot u = 0 \qquad \Leftrightarrow \qquad \lambda = \frac{w \cdot u}{u \cdot u} \qquad \Leftrightarrow \qquad v_\parallel = \frac{w \cdot u}{u \cdot u} u\]

\newpage

\subsection{Sottospazi ortogonali}
Dato un sottospazio \(U \leq \mathbb{R}^n\), il sottospazio ortogonale ad \(U\) è \(U^\perp = \left\{ v \in \mathbb{R}^n \; | \; v \cdot u = 0 \; \forall u \in U \right\}\)
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(U \leq \mathbb{R}^n\), allora anche \(U^\perp \leq \mathbb{R}^n\)
	\item[-] \(v \in U^\perp \;\; \Leftrightarrow \;\; u_1 \cdot v = 0, \; u_2 \cdot v = 0 \; \dots \; u_r \cdot v = 0\) con \((u_1, \dots u_r)\) base di \(U\)
	\item[-] se \(U \leq \mathbb{R}^n\) e \(\dim U = r\), allora \(\dim U^\perp = n-r\) e \(\dim U + \dim U^\perp = \dim \mathbb{R}^n\), per cui \(U \cap U^\perp = \{ \vec{0} \}\)
	e sono in somma diretta \(U \oplus U^\perp = \mathbb{R}^n\)
	\item[-] l'ortogonale dell'ortogonale è il sottopazio di partenza \(U^{\perp\perp} = U\)
	\item[-] \((U+W)^\perp = U^\perp \cap W^\perp\) e \((U \cap W)^\perp = U^\perp + W^\perp\)
\end{itemize}
Dato \(U\) sotto forma di equazioni cartesiane, allora:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(U = \ker A\), con \(A\) matrice associata al sistema di equazioni cartesiane
	\item[-] \(U^\perp\) è sottospazio generato dalle righe di \(A\)
\end{itemize}
Per trovare le proiezioni ortogonali di \(v\) su \(U\) e \(U^\perp\):
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{Metodo 1:}\\
	trovo due basi di \(U\) e \(U^\perp\), le unisco, trovo le coordinate di \(v\) rispetto alla nuova base e le coordinate relative
	ai vettori della base di \(U\) sono la proiezione di \(v\) su \(U\), mentre le rimanenti sono la proiezione di \(v\) su \(U^\perp\)
	\item[-] \textbf{Metodo 2:}\\
	impongo che \((v-v') = v'' \in U^\perp\) per cui impongo la condizione di perpendicolarità per ogni vettore di \(U^\perp\) e risolvo
	il sistema \(u_1 \cdot (v-v') = 0, u_2 \cdot (v-v') = 0, \dots u_r \cdot (v-v') = 0\), \(u_1, \dots u_r\) base di \(U\) 
\end{itemize}

\subsection{Matrice di proiezione ortogonale}
La matrice di proiezione ortogonale di \(v\) su \(U\) è \(P = A(A \tran \; A)^{-1} A \tran\) e vale \(v' = P v\), con \(A\) matrice
che ha come colonne, i vettori della base di \(U\).

\subsection{Basi ortogonali e ortonormali e procedimento di Gram-Schmidt}
Sia \(U \leq \mathbb{R}^n\), una base ortogonale di \(U\) è una base \((u_1, \dots u_r)\) tale che \(u_i \cdot u_j = 0 \; \forall i \neq j\),
ovvero una base i cui vettori sono tutti ortogonali tra di loro. \\
Una base ortonormale è una base ortogonale in cui tutti i vettori hanno lunghezza unitaria.

\subsubsection*{Procedimento di Gram-Schmidt}
Per trovare una base ortogonale da una qualsiasi base, si utilizza il seguente procedimento:
\begin{enumerate}[topsep=3pt, itemsep=0pt]
	\item prendo \(u_1' = u_1\) primo vettore della base ortogonale
	\item scelgo \(u_2' \in <u_1, u_2>\) tale per cui \(u_2' \cdot u_1' = 0\), ovvero cerco la proiezione di \(u_2\) sulla retta
	perpendicolare a \(u_1'\): \(\displaystyle u_2' = u_2 - \frac{u_2 \cdot u_1'}{u_1' \cdot u_1'} u_1'\)
	\item scelgo \(u_3' \in <u_1, u_2, u_3>\) tale che \(u_3' \cdot u_1' = 0\) e \(u_3 \cdot u_2' = 0\) e ottengo che
	\(\displaystyle u_3' = u_3 - \frac{u_3 \cdot u_1'}{u_1' \cdot u_1'} u_1' - \frac{u_3 \cdot u_2'}{u_2' \cdot u_2'} u_2'\)
	\item continuo con tutti i vettori: \(\displaystyle u_i' = u_i - \frac{u_i \cdot u_1'}{u_1' \cdot u_1'} u_1' - \frac{u_i \cdot u_2'}{u_2' \cdot u_2'} u_2' - \dots - \frac{u_i \cdot u_{i-1}'}{u_{i-1}' \cdot u_{i-1}'} u_{i-1}'\)
\end{enumerate}
Per ottenere una base ortonormale, divido ciascun vettore per la sua norma: \(\displaystyle u_i'' = \frac{u_i'}{||u_i'||} = \frac{ u_i'}{\sqrt{u_i' \cdot u_i'}}\)

\newpage

\subsection{Diagonalizzazione matrici simmetriche reali}
Data una matrice quadrata reale e simmetrica \(A \in M_n(k)\), questa ha \(n\) autovalori distinti e gli autovettori associati sono
tutti perpendicolari tra loro. Dal teorema spettrale si ottiene che la matrice è ortogonalmente diagonalizzabile, ovvero esiste una
base ortonormale fatta di autovettori.

Una matrice è ortogonalmente diagonalizzabile se e solo se è simmetrica, ovvero quando esiste una matrice ortogonale \(P\) del
cambiamento di base tale per cui vale: \(D = P^{-1} \; A \; P\) e \(D = P\tran \; A \; P\).

Data una matrice simmetrica, per trovare una base ortogonale fatta da autovettori:
\begin{enumerate}[topsep=3pt, itemsep=0pt]
	\item calcolo il polinomio caratteristico \(p_A(\lambda) = (\lambda_1 - \lambda)^m_1 \cdot (\lambda_2 - \lambda)^m_2 \dots (\lambda_r - \lambda)^m_r\)
	e ottengo \(r\) autovalori \(\lambda_1, \dots \lambda_r\) con \(m_a(\lambda_i) = m_g(\lambda_i) = m_i\) in quanto è diagonalizzabile
	dato che è simmetrica
	\item calcolo gli autospazi e trovo una base ortonormale per ciascuno (es. utilizzo Gram-Schmidt)
	\item unisco i vettori delle varie basi ottenendo una base ortonormale fatta di autovettori
	\item[] in questo modo ho la matrice \(D\) con gli autovalori lungo la diagonale, la matrice \(P\) ottenuta unendo gli autovettori
	corrispondenti agli autovalori come vettori colonne e la matrice \(P^{-1} = P \tran\) eseguendo la trasposta di \(P\)
\end{enumerate}

\newpage

\section{Geometria affine}
\subsection{Spazio affine}
Uno spazio affine \(\mathbb{A}\) su \(\mathbb{R}\) è costituito da:
\begin{enumerate}[topsep=3pt, itemsep=0pt]
	\item un insieme non vuoto \(S\), detto insieme dei punti di \(\mathbb{A}\)
	\item uno spazio vettoriale \(V\) su \(\mathbb{R}\)
	\item una operazione \(+\) tra punto e vettore
\end{enumerate}

\subsection{Rette in uno spazio affine}
Una generica retta \(r\) è univocamente identificata da due punti distinti \(P, Q \in r\) appartenenti ad essa, o da un punto
e da un vettore direttore \(v\) (ottenuto come differenza di punti distinti della retta \(v = Q - P\))

\subsubsection*{Equazioni parametriche}
Sia \(X\) generico punto della retta, \(P\) punto della retta, \(v\) vettore e base del sottospazio direttore (di dimensione 1),
l'equazione parametrica della retta ha la forma:
\[r : X = P + \lambda v\]

\subsubsection*{Equazioni cartesiane}
Partendo dall'equazione parametrica e sostituendo il parametro \(\lambda\) si ottiene un sistema di \(n-1\) equazioni cartesiane
con \(n\) dimensione dello spazio affine. Nel caso di \(\mathbb{R}^3\) si ha:
\[r: \begin{cases}
	ax + by + cz = d \\
	ex + fy + gz = h
\end{cases}\]
(si osserva che i vettori \((a, b, c, d)\) e \((e, f, g, h)\) definiscono il sottospazio direttore perpendicolare a \(r\))

\subsection{Piani in uno spazio affine}
Un generico piano \(\pi\) è univocamente identificato da tre punti distinti e non allineati \(P, Q, R \in \pi\), o da un punto
\(P\) e due vettori linearmente indipendenti \(v, w\) (con \(v = Q-P\) e \(w = R-P\)).

\subsubsection*{Equazioni parametriche}
Sia \(X\) un generico punto del piano, \(P\) punto del piano, \(v, w\) vettori base del sottospazio direttore (di dimensione 2),
l'equazione parametrica del piano ha la forma:
\[\pi: X = P + \lambda v + \mu w\]

\subsubsection*{Equazioni cartesiane}
Partendo dall'equazione parametrica e sostituendo i parametri \(\lambda\) e \(\mu\) si ottiene un sistema di \(n-2\) equazioni
cartesiane, con \(n\) dimensione dello spazio affine. Nel caso di \(\mathbb{R}^3\) si ha:
\[\pi: ax + by + cz = d\]
(si osserva che il vettore \((a, b, c, d)\) definisce la normale a \(\pi\))

\subsection{Sottospazi affini}
Uno sottospazio affine \(\mathcal{L}\) è definito con un punto di passaggio \(P\) e un sottospazio vettoriale \(W\) dello spazio
affine come spazio direttore. L'equazione parametrica è: (con \(W\) si intende combinazione lineare dei vettori della base di \(W\))
\[\mathcal{L} : X = P + W\]
Per ottenere le equazioni cartesiane si risolve un sistema lineare di \(n-r\) equazioni lineari indipendenti con \(n\) incognite
e \(r\) parametri. Il sistema è omogeneo se \(P \equiv O(0,0, \dots 0)\).

\subsection{Posizioni reciproche di sottospazi affini in \(\mathbb{R}^3\)}
\subsubsection*{Posizione reciproca genericamente}
Dati due sottospazi affini \(\mathcal{L}: X = P \, + \! <W>\) e \(\mathcal{L}': X = Q \, + \! <W'>\), questi possono essere:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] incidenti, ovvero \(\mathcal{L} \cap \mathcal{L}' \neq \varnothing\), se e solo se \((Q-P) \; \in \; <W, W'>\).
	\item[-] paralleli se e solo se \(W \leq W'\) o \(W' \leq W\), uno sottospazio affine è sottospazio dell'altro 
	\item[-] sghembi se e solo se \(W \cap W' = \left\{ \vec{0} \right\}\) e \(\mathcal{L} \cap \mathcal{L}' = \varnothing\), \(\dim (W + W') = \dim W + \dim W'\)
\end{itemize}
Per spazio con dimensione superiore a 4, ci possono essere spazi affini che non sono né incidenti, né paralleli, né sghembi.

\subsubsection*{Posizione reciproca retta-piano}
Date le equazioni cartesiane di un piano e di una retta e studiando la matrice del sistema ottenuto unendo le diverse equazioni:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(\rg A = 2\) la retta e il piano sono paralleli, in particolare
	\begin{itemize}[topsep=3pt, itemsep=0pt]
		\item[-] se \(\rg (A|d) = 2\) la retta e il piano sono paralleli coincidenti
		\item[-] se \(\rg (A|d) = 3\) la retta e il piano sono paralleli distinti
	\end{itemize}
	\item[-] se \(\rg A = \rg (A|d) = 3\) la retta e il piano sono incidenti
\end{itemize}

\subsubsection*{Posizione reciproca piano-piano}
Date le equazioni cartesiane di due piani e studiando la matrice del sistema ottenuto unendo le equazioni:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(\rg A = 1\) i piani sono paralleli, in particolare:
	\begin{itemize}[topsep=3pt, itemsep=0pt]
		\item[-] se \(\rg (A|d) = 1\) i piani sono paralleli coincidenti
		\item[-] se \(\rg (A|d) = 2\) i piani sono paralleli distinti
	\end{itemize}
	\item[-] se \(\rg A = \rg (A|d) = 2\) i piani sono incidenti in una retta
\end{itemize}

\subsubsection*{Posizione reciproca retta-retta}
Date le equazioni cartesiane di due rette e studiando la matrice del sistema ottenuto unendo le equazioni:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(\rg A = 2\) le rette sono parallele, in particolare:
	\begin{itemize}[topsep=3pt, itemsep=0pt]
		\item[-] se \(\rg (A|d) = 2\) le rette sono parallele coincidenti
		\item[-] se \(\rg (A|d) = 3\) le rette sono parallele distinti
	\end{itemize}
	\item[-] se \(\rg A = 3\) le rette non sono parallele, in particolare:
	\begin{itemize}[topsep=3pt, itemsep=0pt]
		\item[-] se \(\rg (A|d) = 3\) le rette sono incidenti in un punto
		\item[-] se \(\rg (A|d) = 4\) le rette sono sghembe
	\end{itemize}
\end{itemize}

\newpage

\subsection{Distanza tra sottospazi affini in \(\mathbb{R}^3\)}
\subsubsection*{Distanza punto-piano}
Dato un punto \(P\) e un piano \(\pi\), per trovare la proiezione ortogonale del punto sul piano:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{Metodo 1}:
	\begin{itemize}[topsep=3pt, itemsep=0pt]
		\item[-] verifico che il punto non appartiene al piano, ovvero il vettore \(Q-P\) non è combinazione lineare dei vettori
		del sottospazio direttore \(W\) del piano \(\pi\), se appartiene la sua proiezione coincide con il punto stesso
		\item[-] trovo la retta \(r\) perpendicolare al piano \(\pi\) e passante per \(P\) (trovando il sottospazio ortogonale
		al sottospazio direttore del piano)
		\item[-] trovo l'intersezione \(H = r \cap \pi\) che è la proiezione di \(P\) su \(\pi\) 
	\end{itemize}
	\item[-] \textbf{Metodo 2}:
	\begin{itemize}[topsep=3pt, itemsep=0pt]
		\item[-] verifico che il punto non appartiene al piano, ovvero il vettore \(Q-P\) non è combinazione lineare dei vettori
		del sottospazio direttore \(W\) del piano \(\pi\), se appartiene la sua proiezione coincide con il punto stesso
		\item[-] scelgo un generico punto \(X\) del piano \(\pi\) e trovo il vettore \(X-P = Q - P + \lambda v + \mu w\)
		\item[-] impongo l'ortogonalità del vettore con il piano: \(\begin{cases} (X-P) \cdot v = 0 \\ (X-P) \cdot w = 0 \end{cases}\)
		e risolvo per \(\lambda\) e \(\mu\)
		\item[-] trovate \(\lambda\) e \(\mu\), le sostituisco nell'equazione parametrica del piano e ottengo il punto \(H\) che
		è proiezione ortogonale di \(P\) su \(\pi\)
	\end{itemize}
\end{itemize}
La distanza tra un punto e un piano equivale alla distanza del punto dalla sua proiezione ortogonale sul piano:
\(\dist(P,\pi) = \dist(P,H) = \sqrt{||P-H||}\). \\
Nel caso in cui il piano è dato in forma cartesiana, il vettore direttore della retta è dato dai coefficienti della cartesiana del
piano e si ottiene che la distanza è data dalla formula \(\displaystyle \dist(P,\pi) = \frac{\left| \sum a_i c_i + b \right|}{\sqrt{\sum {a_i}^2}}\),
con \(a_i\) coefficienti della cartesiana del piano, \(c_i\) coordinate di \(P\).

\subsubsection*{Distanza retta-piano}
Una retta e un piano possono intersecarsi o essere paralleli. Se si intersecano, la loro distanza minima è 0, viceversa se non si
intersecano, ovvero sono paralleli distinti, basta prendere un generico punto della retta e calcolare la distanza punto-piano.

\subsubsection*{Distanza piano-piano}
Due piani possono intersecarsi o essere paralleli. Se si intersecano, la loro distanza minima è 0, viceversa se non si intersecano,
ovvero sono paralleli distinti, basta prendere un generico punto di uno dei due piani e calcolare la distanza punto-piano.

\subsubsection*{Distanza punto-retta}
Per calcolare la distanza punto-retta si utilizza il metodo 2 visto sopra per calcolare la distanza punto-piano. Si cerca un vettore
\(X-P\) e si impone la condizione di ortogonalità con il vettore direttore della retta \((X-P) \cdot v = 0\) e si risolve in funzione
dell'unico parametro \(\lambda\) dato dall'equazione parametrica della retta. Una volta trovato \(\lambda\), si può ottenere \(H\)
ed infine la distanza da \(P\) ad \(H\).

La formula generica è \(\dist (P,r) = || P - H || = \sqrt{(P-Q) \cdot (P-Q) - \frac{((P-Q) \cdot v)^2}{v \cdot v}}\)

\newpage

\subsubsection*{Distanza retta-retta}
Due rette possono intersecarsi, essere parallele o sghembe. Se si intersecano, la loro distanza è 0, se sono parallele distinte,
basta prendere un generico punto da una delle due rette e calcolare la distanza punto-retta, se sono sghembe, la distanza minima
è quella per cui il vettore distanza è perpendicolare al sottospazio generato unendo i sottospazi direttori: \((X_r-X_s) \perp <W_r, W_s>\)
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{Metodo 1}: \\
	si esprime \(X_r-X_s\) in funzione dei parametri \(\lambda_1\) e \(\lambda_2\) dati dalle parametriche delle rette e si impongono
	le condizioni di ortogonalità con i vettori direttori delle rette. Una volta trovati i due parametri, si può risalire ai due
	punti \(X_r\) e \(X_s\) e calcolare la distanza: \(\dist(r,s) = \min \dist(X_r, X_s) = \dist(X_r, X_s) \Leftrightarrow (X_r-X_s) \perp <W_r, W_s>\)
	\item[-] \textbf{Metodo 2}: \\
	Prendo il piano passante per un punto delle due rette, con sottospazio direttore dato dalla combinazione lineare dei vettori
	direttori delle due rette. La distanza tra le rette è pari alla distanza tra un punto dell'altra retta e il piano.
\end{itemize}

\subsection{Angoli tra sottospazi affini in \(\mathbb{R}^3\)}
\subsubsection*{Angolo tra due rette}
Date due rette incidenti con i relativi vettori direttori, l'angolo formato dalle due è \(\displaystyle \cos \alpha = \frac{u \cdot v}{||u|| \; ||v||}\)
con \(0 \leq \alpha \leq \frac{\pi}{2}\)

\subsubsection*{Angolo tra piano e retta}
Data una retta e il piano, trovo la proiezione ortogonale della retta sul piano e l'angolo cercato è l'angolo tra la retta e la sua
proiezione ortogonale sul piano. Se la proiezione è un punto, allora la retta è perpendicolare al piano e \(\alpha = \frac{\pi}{2}\).

\subsection{Fasci di sottospazi affini}
\subsubsection*{Fascio di piani passanti per una retta}
Date le equazioni cartesiane di una retta, si ottengono immediatamente le due equazioni cartesiane dei piani che si intersecano
sulla retta stessa. I due piani sono detti generatori del fascio, gli altri piani sono ottenuti come combinazione lineare dei
due piani generatori, dipendenti da due parametri \(\lambda\) e \(\mu\) non nulli contemporaneamente.
\begin{align*}
	r: \begin{cases}
		a_1 x + b_1 y + c_1 z + d_1 = 0 \\
		a_2 x + b_2 y + c_2 z + d_2 = 0
	\end{cases}
	\;\; &\longrightarrow \;\; \begin{aligned}
		\pi_1 : a_1 x + b_1 y + c_1 z + d_1 = 0 \\
		\pi_2 : a_2 x + b_2 y + c_2 z + d_2 = 0
	\end{aligned}
	\\ \quad &\longrightarrow \quad
	\pi(\lambda,\mu): \lambda (a_1 x + b_1 y + c_1 z + d_1) + \mu (a_2 x + b_2 y + c_2 z + d_2) = 0
\end{align*}

\subsubsection*{Fascio improprio di piani paralleli}
Il fascio improprio di piani si ottiene dall'equazione di un piano generatore a cui viene sommato un parametro. Il piano viene
traslato mantenendo costante il vettore normale al piano \((a,b,c)\).
\begin{align*}
	\text{piano generatore: } \pi &: ax + by + cz + d = 0 \\
	\text{fascio improprio: } \pi(\lambda) &: ax + by + cz + \lambda = 0
\end{align*}

\subsubsection*{Fascio di piani passanti per un punto}
Date le coordinate di un punto di passaggio \(P (x_0, y_0, z_0)\), si impone il passaggio per il punto e si ottiene che il fascio
di piani passanti per \(P\) ha la forma:
\[\pi(x_0,y_0,z_0): a(x-x_0) + b(y-y_0) + c(z-x_0) = 0\]

\newpage

\subsection{Prodotto vettoriale in \(\mathbb{R}^3\)}
\subsubsection*{Definizione}
Dati due vettori \(u = (u_1, u_2, u_3)\) e \(v = (v_1, v_2, v_3)\), il prodotto vettoriale è un vettore definito partendo dal
determinante e sfruttando le formule di Laplace, con \(\left\{ e_1, e_2, e_3 \right\}\) base canonica.
\begin{align*}
	u \times v &= \det \left( \begin{matrix} e_1 & e_2 & e_3 \\ u_1 & u_2 & u_3 \\ v_1 & v_2 & v_3 \end{matrix} \right) \\
	&= e_1 \det \left( \begin{matrix} u_2 & u_3 \\ v_2 & v_3 \end{matrix} \right) + e_2 \det \left( \begin{matrix} u_1 & u_3 \\ v_1 & v_3 \end{matrix} \right) + e_3 \det \left( \begin{matrix} u_1 & u_2 \\ v_1 & v_2 \end{matrix} \right) \\
	&= \left( u_2 v_3 - u_3 v_2, u_3 v_1 - u_1 v_3, u_1 v_2 - u_2 v_1 \right)
\end{align*}

\subsubsection*{Proprietà}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \((\lambda u + \mu u') \times v = \lambda (u \times v) + \mu (u' \times v)\)
	\item[-] \(u \times (\lambda v + \mu v') = \lambda (u \times v) + \mu (u \times v')\)
	\item[-] \(u \times v = - v \times u\)
	\item[-] se \(u\) e \(v\) sono linearmente indipendenti \(u \times v = \vec{0}\)
	\item[-] \(u \cdot (u \times v) = 0\) e \(v \cdot (u \times v) = 0\), ovvero il prodotto vettoriale è un vettore perpendicolare
	ai due vettori di partenza
\end{itemize}
Si può introdurre il prodotto misto di 3 vettori \(u, v, w\): \(u \cdot (v \times w) = \det \left( \begin{matrix} u \\ v \\ w \end{matrix} \right)\)

\subsubsection*{Determinazione del prodotto vettoriale}
Il prodotto vettoriale è univocamente determinato dalle seguenti 3 condizioni:
\begin{enumerate}
	\item \((u \times v) \perp u\) e \((u \times v) \perp v\)
	\item \(|| u \times v ||\) è l'area del parallelogramma delimitato da \(u\) e \(v\)
	\item la matrice del cambiamento di base da \(\left\{ e_1, e_2, e_3 \right\}\) a \(\left\{ u, v, u \times v \right\}\) ha
	determinante positivo (le due basi sono equiorientate)
\end{enumerate}
La matrice del cambiamento di base \(\text{M}_{\left\{ u, v, u \times v \right\}}^{\left\{ e_1, e_2, e_3 \right\}}(id) = \left( \begin{matrix}
	u_1 & v_1 & u_2 v_3 - u_3 v_2 \\
	u_2 & v_2 & u_3 v_1 - u_1 v_3 \\
	u_3 & v_3 & u_1 v_2 - u_2 v_1
\end{matrix} \right)\)

\subsubsection*{Aree con prodotto vettoriale}
Dati due vettori \(u, w\), lo scalare \(|| u \times v ||\) è l'area del parallelogramma ottenuto dai due vettori.

\newpage

\subsection{Cenni sulle coniche}
Una conica in \(\mathbb{A}^2\) è l'insieme dei punti le cui coordinate annullano un polinomio di 2° grado della forma:
\[F(x,y) = ax^2 + bxy + cy^2 + dx + ey + f\]
Sia \(P = (1, x, y)\) un punto di \(\mathbb{A}^2\), il polinomio sopra si può scrivere come forma bilineare simmetrica associata
ad una matrice simmetrica \(3 \times 3\) e I punti della conica corrispondo ai vettori isotropi della forma \((1, x, y)\).
\[G = \left( \begin{matrix}f & \frac{d}{2} & \frac{e}{2} \\ \frac{d}{2} & a & \frac{b}{2} \\ \frac{e}{2} & \frac{b}{2} & c \end{matrix} \right)
\qquad \longrightarrow \qquad ^{\displaystyle(1, \; x, \; y)} G \left( \begin{matrix} 1 \\ x \\ y \end{matrix} \right) \]

Considerando anche la matrice \(\displaystyle G' = \left( \begin{matrix} a & \frac{b}{2} \\ \frac{b}{2} & c \end{matrix} \right)\)
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(\rg G = 1\), la conica è una retta (contata due volte) (è degenere)
	\item[-] se \(\rg G = 2\), la conica è l'unione di due rette (è degenere)
	\item[-] se \(\det G \neq 0\) si hanno 3 casi:
	\begin{itemize}[topsep=3pt, itemsep=0pt]
		\item[-] se \(\det G' > 0\) si ha ellisse
		\item[-] se \(\det G' = 0\) si ha parabola
		\item[-] se \(\det G' < 0\) si ha un'iperbole
	\end{itemize}
\end{itemize}

\end{document}
