{"cells":[{"cell_type":"markdown","metadata":{"id":"fFKe_5IudXRL"},"source":["# **Laboratory Lecture 9**\n","\n","---\n","\n","# EXAMPLE 1: Time for coffee?\n","\n","I have half an hour to spare in my busy schedule, and I have a choice between working quietly in my office and\n","going out for a coffee.\n","\n","If I stay in my office, three things can happen: I can get some work done (Utility = 8), or I can get distracted\n","looking at the latest news (Utility = 1), or a colleague might stop by to talk about some work we are doing (Utility = 5).\n","\n","If I go out for coffee, I will most likely enjoy a good cup of smooth caffeination (Utility = 10), but there is\n","also a chance I will end up spilling coffee all over myself (Utility = −20).\n","\n","The probability of getting work done if I choose to stay in the office is 0.5, while the probabilities of getting\n","distracted, and a colleague stopping by are 0.3 and 0.2 respectively.\n","\n","If I go out for a coffee, my chance of enjoying my beverage is 0.95, and the chance of spilling my drink is 0.05.\n","\n","\n","> __QUESTION 1(a):__ What is the expected utility of staying in my office?\n","    \n","> __QUESTION 1(b):__ What is the expected utility of going out for a coffee?\n","\n","> __QUESTION 1(c):__ By the principle of maximum expected utility, what should I do?"]},{"cell_type":"markdown","source":["### Solution 1(a)\n","\n","Staying in the $office$ means that I will either $work$, get $distracted$, or talk with a $colleague$. These states\n","have the following utilities:\n","\\begin{align*}\n","U(work) & = 8\\\\\n","U(distracted) & = 1\\\\\n","U(colleague) & = 5\n","\\end{align*}\n","and the probabilities of these happening, given I stay in the $office$ are:\n","\\begin{align*}\n","P(work|office) & = 0.5\\\\\n","P(distracted|office) & = 0.3\\\\\n","P(colleague|office) & = 0.2\n","\\end{align*}\n","Let's declare these as Python arrays"],"metadata":{"id":"URWlI2d9ecNn"}},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHemoNiTdXRX","executionInfo":{"status":"ok","timestamp":1765663307867,"user_tz":-60,"elapsed":12,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"f71ee6aa-5ce1-4d30-f608-8791b320cf8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["office_outcomes =  ['work', 'distracted', 'colleague']\n","U(office_outcomes) =  [8 1 5]\n","P(office_outcomes|office) = [0.5 0.3 0.2]\n"]}],"source":["import numpy as np\n","# Setup arrays with: symbolic names for outcomes (not currently used), utilities of outcomes, and\n","# probabililites of those outcomes\n","office_outcomes = [\"work\", \"distracted\", \"colleague\"]\n","print('office_outcomes = ', office_outcomes)\n","\n","u_office_outcomes = np.array([8, 1, 5])\n","print('U(office_outcomes) = ', u_office_outcomes)\n","\n","p_office_outcomes_office = np.array([0.5, 0.3, 0.2])\n","print('P(office_outcomes|office) =', p_office_outcomes_office)\n"]},{"cell_type":"markdown","metadata":{"id":"diXAMDYVdXRi"},"source":["<br>\n","The expected utility of staying in the office is:\n","    \\begin{align*}\n","      EU(office) & = 0.5\\times 8 + 0.3\\times 1 + 0.2\\times 5\\\\\n","                 & = 5.3\n","    \\end{align*}\n","    \n","Now let's implement this in Python:"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMU-4lgOdXRj","executionInfo":{"status":"ok","timestamp":1765663307896,"user_tz":-60,"elapsed":28,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"ea51438d-136f-4768-d7ad-42595e653307"},"outputs":[{"output_type":"stream","name":"stdout","text":["EU by outcome = [4.  0.3 1. ]\n","EU(office) =  5.3\n","alternative: EU(office) =  5.3\n"]}],"source":["# The weighted utility ofeach outcome is each to compute by pairwise multiplication\n","eu_office_outcomes = u_office_outcomes * p_office_outcomes_office\n","print('EU by outcome =', eu_office_outcomes)\n","\n","# Summing the weighted utilities gets us the expected utility\n","eu_office = np.sum(eu_office_outcomes)\n","print('EU(office) = ', eu_office)\n","\n","# alternatives\n","eu_office_2 = np.dot(u_office_outcomes, p_office_outcomes_office)\n","print('alternative: EU(office) = ', eu_office_2)"]},{"cell_type":"markdown","metadata":{"id":"8J9Tuje8dXRk"},"source":["So the expected utility of staying in the office is 5.3"]},{"cell_type":"markdown","metadata":{"id":"eqagDsE1dXRl"},"source":["### Solution 1(b)\n","\n","This time, we'll jump straight to the Python code, using the same notation as before"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0yjaFhwdXRm","executionInfo":{"status":"ok","timestamp":1765663307925,"user_tz":-60,"elapsed":26,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"2c30039d-eff7-42dd-fdde-1083356d853c"},"outputs":[{"output_type":"stream","name":"stdout","text":["coffee_outcomes =  ['caffeination', 'spillage']\n","U(coffee_outcomes) =  [ 10 -20]\n","P(coffee_outcomes|coffee) = [0.95 0.05]\n","\n","EU by outcome = [ 9.5 -1. ]\n","EU(coffee) =  8.5\n"]}],"source":["# The coffee calculation is the same as the office calculation, first set up arrays\n","coffee_outcomes = [\"caffeination\", \"spillage\"]\n","print('coffee_outcomes = ', coffee_outcomes)\n","\n","u_coffee_outcomes = np.array([10, -20])\n","print('U(coffee_outcomes) = ', u_coffee_outcomes)\n","\n","p_coffee_outcomes_coffee = np.array([0.95, 0.05])\n","print('P(coffee_outcomes|coffee) =', p_coffee_outcomes_coffee)\n","print()\n","\n","# Then compute the expected utility\n","eu_coffee_outcomes = u_coffee_outcomes * p_coffee_outcomes_coffee\n","print('EU by outcome =', eu_coffee_outcomes)\n","\n","eu_coffee = np.sum(eu_coffee_outcomes)\n","print('EU(coffee) = ', eu_coffee)"]},{"cell_type":"markdown","metadata":{"id":"XcKKbyc0dXRn"},"source":["So the expected utility of going out for coffee is 8.5"]},{"cell_type":"markdown","metadata":{"id":"WwYH5i8LdXRp"},"source":["### Solution 1(c)\n","\n","The MEU criterion is that the option with the maximum expected utility is the one chosen. Clearly in the case of\n","numbers in the example, the option of going out for $coffee$ is the one with the maximum expected utility.\n","\n","However, we will also program it in Python so that we can see what happens as the probabilities of the outcomes\n","vary:"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XUECN2idXRq","executionInfo":{"status":"ok","timestamp":1765663307933,"user_tz":-60,"elapsed":7,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"e261f3e6-9a22-4ce9-d946-19d45a650c75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Coffee is the MEU choice\n"]}],"source":["if eu_office > eu_coffee:\n","    print('Office is the MEU choice')\n","else:\n","    print('Coffee is the MEU choice')"]},{"cell_type":"markdown","metadata":{"id":"iOLK89FUdXRs"},"source":["---\n","\n","# EXAMPLE 2: Time for coffee? Maximax - Maximin\n","\n","Revisit the decision for the coffee example using the maximax and maximin decision criteria\n","\n","## Solution 2\n","\n","The maximax decision criterion rates each choice by the utility of its best outcome,\n","and then picks the choice with best utility.\n","\n","In Python we would do this calculation as follows"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I746AhdPdXRt","executionInfo":{"status":"ok","timestamp":1765663308006,"user_tz":-60,"elapsed":42,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"552e245b-5a66-4db9-f14c-a92da36a5f85"},"outputs":[{"output_type":"stream","name":"stdout","text":["MaxU(office) = 8\n","MaxU(coffee) = 10\n","\n","Coffee is the Maximax choice\n"]}],"source":["# The utility of each choice is the max utility of their outcomes\n","max_u_office = np.max(u_office_outcomes)\n","print('MaxU(office) =', max_u_office)\n","\n","max_u_coffee = np.max(u_coffee_outcomes)\n","print('MaxU(coffee) =', max_u_coffee)\n","print()\n","\n","# The decision criterion is then to pick the outcome with the highest utility:\n","if max_u_office > max_u_coffee:\n","    print('Office is the Maximax choice')\n","else:\n","    print('Coffee is the Maximax choice')"]},{"cell_type":"markdown","metadata":{"id":"7GM6iPdJdXRu"},"source":["The maximin decision criterion rates each choice by the utility of its worst outcome, and then picks the choice with the best utility.\n","\n","In Python:"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6ix8RqodXRv","executionInfo":{"status":"ok","timestamp":1765663308010,"user_tz":-60,"elapsed":3,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"dde4f071-821c-46aa-fbb4-ca5feb2dc512"},"outputs":[{"output_type":"stream","name":"stdout","text":["MinU(office) = 1\n","MinU(coffee) = -20\n","\n","Office is the Maximin choice\n"]}],"source":["# The utility of each choice is the max utility of their outcomes\n","min_u_office = np.min(u_office_outcomes)\n","print('MinU(office) =', min_u_office)\n","\n","min_u_coffee = np.min(u_coffee_outcomes)\n","print('MinU(coffee) =', min_u_coffee)\n","print()\n","\n","# The decision criterion is then to pick the outcome with the highest utility:\n","if min_u_office > min_u_coffee:\n","    print('Office is the Maximin choice')\n","else:\n","    print('Coffee is the Maximin choice')"]},{"cell_type":"markdown","source":["---\n","\n","# EXAMPLE 2: Solving a simple MDP using the MDP toolbox\n","\n","The MDP Toolbox is an implementation of some MDP algorithms in Python. You will need to install this using:\n","\n","pip install pymdptoolbox\n","\n","Documentation is at: https://pymdptoolbox.readthedocs.io/en/latest/index.html\n","\n","Let's start with a really simple problem.\n","\n","We have 4 states and two actions.\n","\n","There are two actions, 0 is \"Stay\" and 1 is \"Right\". **0 always succeeds** and leaves the agent in the same state. **1 moves the agent right** with probability **0.8**, stays in place with probability **0.2**.\n","\n","The states are 0, 1, 2, 3. 0 is left of 1, which is left of 2 and so on. (Thus the states **are in a line** which runs 0, 1, 2, 3 from left to right).\n","\n","The agent remains in **state 3** with **probability 1**.\n","\n","State 3 has a **reward of 1**, and the **cost** of any action is **-0.04**."],"metadata":{"id":"klkkw74AbeuF"}},{"cell_type":"code","source":["!pip install pymdptoolbox"],"metadata":{"id":"D-QAjSI7bBlo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1765663319953,"user_tz":-60,"elapsed":11944,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"e3f1884b-12a6-4ecc-c306-22c2125b6b7b"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pymdptoolbox in /usr/local/lib/python3.12/dist-packages (4.0b3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pymdptoolbox) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pymdptoolbox) (1.16.3)\n"]}]},{"cell_type":"code","source":["import mdptoolbox\n","import numpy as np\n","\n","# The MDP Toolbox defines MDPs through a probability array and a reward array.\n","\n","\n","# ----- TRANSITION MODEL -----\n","# The probability array has shape (A, S, S), where A are actions and S\n","# are states. For each action specify the transitions probabilities of reaching\n","# the second state by applying that action in the first state.\n","\n","# So, to implement the action model described above, we need:\n","P1 = np.array([[[1, 0, 0, 0],   # f(stay, state0) -> 100% of staying on state0\n","                [0, 1, 0, 0],   # f(stay, state1) -> 100% of staying on state1\n","                [0, 0, 1, 0],   # f(stay, state2) -> 100% of staying on state2\n","                [0, 0, 0, 1]],  # f(stay, state3) -> 100% of staying on state3\n","               [[0.2, 0.8, 0,   0],   # f(right, state0) -> 20% of staying on state0, 80% of moving to state1\n","                [0,   0.2, 0.8, 0],   # f(right, state1) -> 20% of staying on state1, 80% of moving to state2\n","                [0,   0,   0.2, 0.8], # f(right, state2) -> 20% of staying on state2, 80% of moving to state3\n","                [0,   0,   0,   1]]]) # f(right, state3) -> 100% of staying on state3\n","\n","# The first matrix is that for the action \"Stay\" (when executed in a given\n","# state the agent stays there) and the second is for the action \"Right\"\n","# (which shifts the agent right with probability 0.8 except in state 3\n","# when the agent remains in state 3 with probability 1).\n","\n","# ----- REWARD FUNCTION -----\n","# The reward array has shape (S, A), so there is a set of S vectors,\n","# one for each state, and each is a vector with one element for each\n","# the actions --- each element is the reward for executing the relevant\n","# action in the state (so this is really modelling cost of the action).\n","\n","R1 = np.array([[-0.04, -0.04], # R(stay, state0, state0) - R(right, state0, state1)\n","               [-0.04, -0.04], # R(stay, state1, state1) - R(right, state1, state2)\n","               [-0.04, -0.04], # R(stay, state2, state2) - R(right, state2, state3)\n","               [1, 1]])        # R(stay, state3, state3) - R(right, state3, state3)\n","\n","# R1 says that executing either action in states 0, 1, or 2 has a reward\n","# of -0.04, and executing either action in state 3 has reward 1.\n","\n","# ----- CHECK TRANSITION MODEL and REWARD FUNCTION -----\n","# The util.check() function checks that the reward and probability matrices\n","# are well-formed, and match. Success is silent, failure provides somewhat\n","# useful error messages.\n","mdptoolbox.util.check(P1, R1)\n","\n","# To run value iteration we create a value iteration object, and run it. Note that\n","# discount value is 0.9\n","vi1 = mdptoolbox.mdp.ValueIteration(P1, R1, 0.9)\n","vi1.setVerbose()\n","vi1.run()\n","print()\n","\n","# We can then display the values (utilities) computed, and look at the policy:\n","print('Utility values:', vi1.V)\n","print('Policy:', vi1.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o7OIzXXpk9qD","executionInfo":{"status":"ok","timestamp":1765663319977,"user_tz":-60,"elapsed":18,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"e8ad80ca-627e-4e84-e0d4-623ecebb4b15"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["  Iteration\t\tV-variation\n","    1\t\t  1.04\n","    2\t\t  0.9359999999999999\n","    3\t\t  0.8424\n","    4\t\t  0.36998208\n","    5\t\t  0.1233677952000003\n","    6\t\t  0.03556922803200013\n","    7\t\t  0.009373768934400006\n","    8\t\t  0.002323987241471537\n","    9\t\t  0.0005512625078534228\n","Iterating stopped, epsilon-optimal policy found.\n","\n","Utility values: (2.766226988084275, 3.7438891127976524, 4.857502678650809, 6.12579511)\n","Policy: (1, 1, 1, 0)\n"]}]},{"cell_type":"markdown","source":["This says that the optimum policy is to go Right in every state until reaching state 3, then Stay."],"metadata":{"id":"hWeQ2Om78ZNd"}},{"cell_type":"markdown","source":["Although we have been looking at the policy, we go it through value iteration.\n","\n","Solving the same problem using **policy iteration** is easy with the MDP Toolbox:"],"metadata":{"id":"LBwgNWH-EWdG"}},{"cell_type":"code","source":["# To run policy iteration we create a policy iteration object, and run it. Note that\n","# discount value is 0.9\n","pi1 = mdptoolbox.mdp.PolicyIteration(P1, R1, 0.9)\n","pi1.setVerbose()\n","pi1.run()\n","print()\n","\n","# We can then display the values (utilities) computed, and look at the policy:\n","print('Utility values:', pi1.V)\n","print('Policy: ', pi1.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P39Cu6w9EZxR","executionInfo":{"status":"ok","timestamp":1765663320026,"user_tz":-60,"elapsed":41,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"2fa74d54-0699-4dad-cc93-f0e755f003cd"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["  Iteration\t\tNumber of different actions\n","    1\t\t  1\n","    2\t\t  1\n","    3\t\t  1\n","    4\t\t  0\n","Iterating stopped, unchanging policy found.\n","\n","Utility values: (6.6402692938291725, 7.6180844735276665, 8.731707317073173, 10.000000000000002)\n","Policy:  (1, 1, 1, 0)\n"]}]},{"cell_type":"markdown","source":["Note that the methods disagree on the value while agreeing on the policy."],"metadata":{"id":"UzgwlXVMG-vv"}},{"cell_type":"markdown","source":["Solving a problem using reinforcement learning (well, the **Q-learning** kind of RL) is also easy using the MDP Toolbox:"],"metadata":{"id":"lsE9iNjCg8Mk"}},{"cell_type":"code","source":["# To run q-learning we create a q-learning object, and run it. Note that\n","# discount value is 0.9\n","ql1 = mdptoolbox.mdp.QLearning(P1, R1, 0.9)\n","ql1.run()\n","\n","# We can then display the values (utilities) computed, and look at the policy:\n","print('Utility values:', ql1.V)\n","print('Policy:', ql1.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zcsyk01hIPO","executionInfo":{"status":"ok","timestamp":1765663320593,"user_tz":-60,"elapsed":567,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"1f273820-b87b-46d0-ecc2-f508161b66c2"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Utility values: (0.30291669312840475, 2.1176555805452866, 6.420623678760581, 9.999999837517318)\n","Policy: (1, 1, 1, 1)\n"]}]},{"cell_type":"markdown","source":["Note that the methods disagree on the value while agreeing on the policy."],"metadata":{"id":"yHL_Cjj6j8_R"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[{"file_id":"1nItAoBhyG7jP5dK_wbrvTweIFsF6VphP","timestamp":1764835849378}]}},"nbformat":4,"nbformat_minor":0}