{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jJodIib1_E6tPBvi3BqoxczebfRDQSIC","timestamp":1764835853219}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["---\n","\n","# Python dependences"],"metadata":{"id":"Az_1sjSIuhSM"}},{"cell_type":"code","source":["!pip install pymdptoolbox"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sfkd8ttAMFQi","executionInfo":{"status":"ok","timestamp":1765663278888,"user_tz":-60,"elapsed":16426,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"7b8cf7f9-ea28-4f97-88db-e3edb609ba9f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pymdptoolbox in /usr/local/lib/python3.12/dist-packages (4.0b3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pymdptoolbox) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pymdptoolbox) (1.16.3)\n"]}]},{"cell_type":"markdown","source":["---\n","\n","# EXERCISE 1: What to do at the airport?\n","\n","You are travelling and have some time to kill at the aiport. There are three things you could spend your time doing:\n","  \n","1) You could have a coffee.\n","\n","This has a probability of $0.8$ of giving you time to relax with a tasty beverage, and a utility of $10$.\n","It also has a probability of $0.2$ of providing you with a nasty cup from over-roasted beans that annoys you,\n","and outcome with a utility of $-5$.\n","\n","2) You could shop for clothes.\n","\n","This has a probability of $0.1$ that you will find a great outfit at a good price, utility $20$. However, it\n","has a probability of $0.9$ that you end up wasting money on over-priced junk, utility $-10$.\n","\n","3) You could have a bite to eat.\n","\n","This has a probability of $0.8$ that you find something rather mediocre that prevents you from being too hungry\n","during your flight, utility $2$, and a probability of $0.2$ that you find something filling and tasty, utility $5$.\n","\n","> __QUESTION 1(a):__ What should you do if you take the principle of maximum expected utility to be your decision criterion?\n","\n","> __QUESTION 1(b):__ What should you do if you take the principle of maximax decision criterion to be your decision criterion?\n","\n","> __QUESTION 1(c):__ What should you do if you take the principle of maximin decision criterion to be your decision criterion?\n","    "],"metadata":{"id":"twbLWgpO7YQH"}},{"cell_type":"code","source":["import numpy as np\n","\n","# ----- INITIALISING DATA OF THE PROBLEM -----\n","# choices\n","actions = ('coffee', 'clothes', 'eat')\n","\n","# utility of each state\n","U = np.array([[10, -5],  # utility of good beverage or annoying beverage, choosing coffee\n","              [20, -10], # utility of great outfit or wasting money, choosing clothes\n","              [2, 5]])   # utility of mediocre meal or tasty meal, choosing eat\n","\n","# probability of each state\n","P = np.array([[0.8, 0.2],  # probability of good beverage or annoying beverage, choosing coffee\n","              [0.1, 0.9],  # probability of great outfit or wasting money, choosing clothes\n","              [0.8, 0.2]]) # probability of mediocre meal or tasty meal, choosing eat\n","\n","# calculating Expected Utility for actions\n","EU = U*P\n","print('expected utilities:\\n', EU, '\\n')\n","\n","# ----- MAXIMUM EXPECTED UTILITY - MEU -----\n","# calculating utility for every choice\n","EU_choice = EU.sum(axis=1)\n","\n","# calculating action with the maximum expected utility\n","MEU_action = actions[np.argmax(EU_choice)]\n","print('MEU:', MEU_action)\n","\n","# ----- MAXIMAX UTILITY - MXX -----\n","# calculating action with the maximax utility\n","MXX_action = actions[np.argmax(np.max(U, axis=1))]\n","print('MaxiMax:', MXX_action)\n","\n","# ----- MAXIMIN UTILITY - MXN -----\n","# calculating action with the maximin utility\n","MXN_action = actions[np.argmax(np.min(U, axis=1))]\n","print('MaxiMax:', MXN_action)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2T7B3xBZJ9a4","executionInfo":{"status":"ok","timestamp":1765663278923,"user_tz":-60,"elapsed":20,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"2217c4a8-8099-4e4b-a85a-cff26a272d7f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["expected utilities:\n"," [[ 8.  -1. ]\n"," [ 2.  -9. ]\n"," [ 1.6  1. ]] \n","\n","MEU: coffee\n","MaxiMax: clothes\n","MaxiMax: eat\n"]}]},{"cell_type":"markdown","source":["---\n","\n","# EXERCISE 2: Solving a MDP with MDP toolbox\n","\n","We have four states and four actions.\n","\n","The actions are: 0 is Right, 1 is Left, 2 is Up and 3 is Down.\n","\n","The states are 0, 1, 2, 3, and they are arranged like this:\n","    \n","$$\n","\\begin{array}{cc}\n","2 & 3\\\\\n","0 & 1\\\\\n","\\end{array}\n","$$\n","\n","The motion model provides:\n","*   0.8 probability of moving in the direction of the action,\n","*   0.1 probability of moving in each of the directions perpendicular to that of the action.\n","\n","So that 2 is Up from 0 and 1 is Right of 0, and so on. The cost of any action (in any state) is -0.04.\n","\n","In case of \"infeasible\" movements, the agent remains in the current state with probability 0.8+0.1. Consider the perpendicular directions in any case.\n","\n","The reward for state 3 is 1, and the reward for state 1 is -1.\n","\n","The agent does not leave those state 3 and state 1. Keep attention when modeling the transition.\n","\n","Set discount factor equal to 0.99.\n","\n","> __QUESTION 2(a):__ What is the policy based on the Value iteration algorithm?\n","\n","> __QUESTION 2(b):__ What is the policy based on the Policy iteration algorithm?\n","\n","> __QUESTION 2(c):__ What is the policy based on the Q-Learning algorithm?\n","\n","> __QUESTION 2(d):__ Look at the **setVerbose**() function and the time attribute of the MDP objects in MDPToolbox and use them to compare the number of iterations (hint: see the iter attribute) and the CPU time used to come up with a solution (hint: see the time attribute) in the Value iteration algorithm and Policy iteration algorithm resolutions.\n"],"metadata":{"id":"wRLlnsWJ90DZ"}},{"cell_type":"code","source":["import numpy as np\n","import mdptoolbox\n","\n","# ----- INITIALISING DATA OF THE PROBLEM -----\n","# verbose\n","verbose = 0\n","\n","# TRANSITION MODEL p(final_state | action, initial_state)\n","P = np.array([[[0.1, 0.8, 0.1, 0],   # final_state | right, state0\n","               [0,   1,   0,   0],   # final_state | right, state1\n","               [0.1, 0,   0.1, 0.8], # final_state | right, state2\n","               [0,   0,   0,   1]],  # final_state | right, state3\n","\n","              [[0.9, 0,   0.1, 0],  # final_state | left, state0\n","               [0,   1,   0,   0],  # final_state | left, state1\n","               [0.1, 0,   0.9, 0],  # final_state | left, state2\n","               [0,   0,   0,   1]], # final_state | left, state3\n","\n","              [[0.1, 0.1, 0.8, 0],   # final_state | up, state0\n","               [0,   1,   0,   0],   # final_state | up, state1\n","               [0,   0,   0.9, 0.1], # final_state | up, state2\n","               [0,   0,   0,   1]],  # final_state | up, state3\n","\n","              [[0.9, 0.1, 0,   0],   # final_state | down, state0\n","               [0,   1,   0,   0],   # final_state | down, state1\n","               [0.8, 0,   0.1, 0.1], # final_state | down, state2\n","               [0,   0,   0,   1]]]) # final_state | down, state3\n","\n","# REWARD FUNCTION r(initial_state, action)\n","R = np.array([[-0.04, -0.04, -0.04, -0.04], # reward from state0\n","              [-1,    -1,    -1,    -1],    # reward from state1\n","              [-0.04, -0.04, -0.04, -0.04], # reward from state2\n","              [1,     1,     1,     1]])    # reward from state3\n","\n","# CHECK DATA\n","mdptoolbox.util.check(P, R)\n","\n","# ----- VALUE ITERATION ALGORITHM -----\n","vi = mdptoolbox.mdp.ValueIteration(P, R, 0.99)\n","if verbose: vi.setVerbose()\n","vi.run()\n","print()\n","\n","print('Utility values:', vi.V)\n","print('Iterations:', vi.iter)\n","print('Policy:', vi.policy)\n","\n","# ----- POLICY ITERATION ALGORITHM -----\n","pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.99)\n","if verbose: pi.setVerbose()\n","pi.run()\n","print()\n","\n","print('Utility values:', pi.V)\n","print('Iterations:', pi.iter)\n","print('Policy:', pi.policy)\n","\n","# ----- Q-LEARNING ALGORITHM -----\n","qi = mdptoolbox.mdp.QLearning(P, R, 0.99)\n","if verbose: qi.setVerbose()\n","qi.run()\n","print()\n","\n","print('Utility values:', qi.V)\n","print('Policy:', qi.policy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dJwzAI5hS-Le","executionInfo":{"status":"ok","timestamp":1765663279787,"user_tz":-60,"elapsed":833,"user":{"displayName":"Giacomo Simonetto","userId":"05073450947575185137"}},"outputId":"4b4d8811-6c34-4b19-98d4-ea8ba6ef0414"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Utility values: (88.23133912867954, -99.9949804281095, 97.54814303782808, 99.9949804281095)\n","Iterations: 985\n","Policy: (1, 0, 0, 0)\n","\n","Utility values: (88.23635870057004, -99.99999999999991, 97.5531626097185, 99.99999999999991)\n","Iterations: 3\n","Policy: (1, 0, 0, 0)\n","\n","Utility values: (1.3217831269597031, -19.63078227794021, 19.350445427776574, 61.500930791081785)\n","Policy: (1, 2, 0, 0)\n"]}]}]}