\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc} % standard unicode
\usepackage[italian]{babel} % corretta sillabazione in italiano
\usepackage{geometry} % per impostare margini e layout pagina
\usepackage{amssymb} % per l'ambiente matematico
\usepackage{amsmath} % per l'ambiente matematico
%\usepackage{amsthm} % per l'ambiente matematico (simbolo qed alla fine delle dimostrazioni)
\usepackage{enumitem} % per elenchi puntati
\usepackage{multirow} % per celle che si espandono su più righe
\usepackage{tabularx} % per tabelle con larghezza flessibile
\usepackage{booktabs} % per linee orizzontali tabelle
%\usepackage{hyperref} % per collegamenti
%\usepackage{dirtytalk} % per le ""

% per margini
\geometry{a4paper,left=25mm, right=25mm, bottom=25mm, top=30mm}

% per centrare testo nelle tabelleX
\renewcommand\tabularxcolumn[1]{m{#1}}

\newcommand\dom{\text{dom}}   % dominio
\newcommand\dist{\text{dist}} % distanza
\newcommand\intr{\text{int}}  % intorno
\newcommand\R{\mathbb{R}}     % R
\newcommand\Rd{\mathbb{R}^2}  % R^2
\newcommand\Rt{\mathbb{R}^3}  % R^3
\newcommand\Rn{\mathbb{R}^n}  % R^n
\newcommand\tc{\text{t.c.}}   % tale che
\newcommand\dt{\frac{d}{dt}}  % d/dt
\newcommand\dx{\frac{d}{dx}}  % d/dx
\newcommand\dy{\frac{d}{dy}}  % d/dy
\newcommand\nab{\vec{\nabla}} % nabla
\newcommand\rot{\text{rot}}   % rot
\newcommand\diver{\text{div}} % div
\newcommand\var{\text{Var}}   % Var
\newcommand\cov{\text{Cov}}   % Cov

\title{Appunti di fondamenti di analisi e probabilità}
\author{Giacomo Simonetto}
\date{Primo semestre 2024-25}

\begin{document}

% -------------------------------------- Copertina e indice ---------------------------------------
\maketitle
\begin{abstract}
	Appunti del corso di Fondamenti di analisi e probabilità della facoltà di Ingegneria Informatica dell'Università di Padova.
\end{abstract}

\newpage

\tableofcontents

\newpage

% --------------------------------------- Curve e sostegni ----------------------------------------
\section{Curve e sostegni}
\subsection{Introduzione sugli intorni}
\subsubsection*{Definizioni su palle e cubi}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] Norma di un vettore: \(\left| x \right| := \sqrt{{x_1}^2 + {x_2}^2 + \dots + {x_n}^2}\)
	\item[-] Distanza tra due punti: \(\dist(x,y) := \left| x-y \right|\)
	\item[-] Disuguaglianza triangolare: \(\left| x-y \right| \leq \left| x \right| + \left| y \right|\)
	\item[-] Disco o palla chiusa: \(B(p,r] := \left\{ x \in \Rn : \left| x-p \right| \leq r \right\}\)
	\item[-] Disco o palla aperta: \(B(p,r[ \; := \left\{ x \in \Rn : \left| x-p \right| < r \right\}\)
	\item[-] Bordo di una palla: \(\partial B(p,r] = \partial B(p,r[ \; := \left\{ x \in \Rn : \left| x-p \right| = r \right\}\)
	\item[-] Quadrato o cubo chiuso: \(Q(p,r] := \left\{ \left( x_1, \dots, x_n \right) \in \Rn : \left| x_1-p_1 \right| \leq r, \dots, \left| x_n-p_n \right| \leq r \right\}\)
	\item[-] Quadrato o cubo aperto: \(Q(p,r[ \; =: \left\{ \left( x_1, \dots, x_n \right) \in \Rn : \left| x_1-p_1 \right| < r, \dots, \left| x_n-p_n \right| < r \right\}\)
	\item[-] Bordo di un quadrato: \(\partial Q(p,r] = \partial Q(p,r[ \;:= \left\{ \left( x_1, \dots, x_n \right) \in \Rn : \left| x_1-p_1 \right| = r, \dots, \left| x_n-p_n \right| = r \right\}\)
\end{itemize}

\subsubsection*{Teorema di inclusione tra palle e cubi}
Ogni palla contiene un cubo di stesso centro e viceversa. \\
Fissato \(p \in \Rn\) e \(r > 0\) vale \(B(p,r] \subseteq Q(p,r]\) e \(Q(p,r] \subseteq B(p,r\sqrt{n}]\)

\subsubsection*{Definizione di intorno}
Un intorno di \(p \in \Rn\) è un insieme che contiene una palla centrata in \(p\). Per il teorema precedente, la proposizione vale
anche per i quadrati.

\subsubsection*{Definizione di punto interno ad un insieme}
Il punto \(p \in D\) è un punto interno all'insieme \(D\) se \(\exists \delta > 0 : B(p,\delta[ \; \subset D\). \\
L'insieme dei punti interni di un insieme \(D\) si indica con \(\intr(D)\).

\subsubsection*{Insieme aperto, chiuso, frontiera e chiusura}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] Un insieme è aperto se ogni suo punto è un punto interno: \(D = \intr(D)\)
	\item[-] Un insieme è chiuso se il suo complementare è aperto
	\item[] Osservazione: \(\varnothing\) e \(\Rn\) sono sia aperti che chiusi
	\item[-] La frontiera \(\partial D\) è l'insieme dei punti tali che ogni loro intorno interseca sia \(D\), sia \(\Rn \backslash D\)
	\item[-] La chiusura \(\overline{D}\) è il più piccolo insieme chiuso contente \(D\): \(\overline{D} = D \cup \partial D\)
\end{itemize}

\subsubsection*{Prodotto scalare}
Il prodotto scalare tra due vettori \(x = \left( x_1, \dots, x_n \right)\) e \(y = \left( y_1, \dots, y_n \right)\) di \(\Rn\)
è il numero reale definito come \(x \cdot y = x_1 y_1 + \dots + x_n y_n\). \\
Due vettori sono ortogonali se il loro prodotto scalare è 0.

\subsubsection*{Disuguaglianza di Cauchy-Schwarz}
Siano \(x\), \(y \in \Rn\), allora \(\left| x \cdot y \right| \leq \left| x \right| \left| y \right|\). Si ha l'uguaglianza se
solo se uno è multiplo dell'altro.

%\subsubsection*{Esempi di insiemi notevoli}
%\begin{itemize}[topsep=3pt, itemsep=0pt]
%	\item[-] ellisse in \(\Rd\) con centro in \(a,b\) e semiassi \(A\) e \(B\) \[\left\{ \left(x, y\right) : \frac{\left(x - a\right)^2}{A^2} + \frac{\left(y - b\right)^2}{B^2} = 1 \right\}\]
%	\item[-] retta in \(\Rd\) perpendicolare al vettore \(a,b\) \[ax + by + c = 0\]
%	\item[-] piano in \(\Rt\) perpendicolare al vettore \(a,b,c\) \[ax + by + cz = 0\]
%	\item[-] cilindro in \(\Rt\) di asse parallelo all'asse z passante per \(a,b,0\) \[\left(x-a\right)^2 + \left(y-b\right)^2 = r^2\]
%\end{itemize}

\newpage

\subsection{Funzioni vettoriali e curve}
\subsubsection*{Definizione di funzioni vettoriali, curve, sostegni di curve}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] Una funzione vettoriale è una funzione \(f: I_{intervallo} \subset \R \rightarrow \Rn, \; t \mapsto f(t) = \left(f_1(t), f_2(t), \dots f_n(t)\right)\)
	\item[-] Una curva (parametrica) è una funzione vettoriale in cui \(f_1(t), \dots f_n(t)\) sono continue in \(I = \left[a,b\right]\)
	\item[-] Il sostegno di una curva \(f\) è l'insieme immagine di \(f\): \(f\left(\left[a,b\right]\right) := \left\{f(t) : t \in \left[a,b\right]\right\} \subset \Rn\)
	\item[-] Una curva si dice cartesiana se è della forma \(f(t) = \left(t, h(t)\right)\) o \(f(t) = \left(h(t),t\right), t \in \left[a,b\right]\)
	\item[-] Una curva si dice chiusa se \(f(a) = f(b)\)
	\item[-] Una curva si dice semplice se \(f(t_1) \neq f(t_2), \; \forall t_1,t_2\) con \(a < t_1 < t_2 \leq b\), ovvero se non si interseca mai ad eccezione degli estremi
\end{itemize}

\subsubsection*{Curve e sostegni}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] Data una curva \(f(t)\), per ottenere il sostegno di tale curva, bisogna eliminare il parametro \(t\), passando dalla
	forma parametrica a quella cartesiana.
	\item[-] Viceversa se, dato un sostegno, si vuole ottenere una curva, bisogna introdurre una parametrizzazione del sostegno,
	passando dalla forma cartesiana a quella paremtrica.
\end{itemize}

\subsection{Limiti di funzioni vettoriali}
\subsubsection*{Definizione di limite (finito) in una e più dimensioni}
\begin{align*}
	\lim_{t \to t_0} f(t) = \ell \in \Rn \quad &\Rightarrow \quad \forall V \; \text{intorno di} \; \ell \; \exists U \; \text{intorno di} \; p \; \tc \; x \in U \setminus \left\{p\right\} \Rightarrow f(x) \in V \\
	&\Rightarrow \quad \forall \varepsilon > 0 \; \exists \delta > 0 \; \tc \; 0 < \left|t-t_0\right| < \delta \Rightarrow \left|f(t) - \ell\right| < \varepsilon
\end{align*}
\[\lim_{t \to t_0} (f_1(t), f_2(t), \dots f_n(t)) = (\ell_1, \ell_2, \dots \ell_n) \quad \Leftrightarrow \quad \lim_{t \to t_0} f_1(t) = \ell_1, \; \lim_{t \to t_0} f_2(t) = \ell_2, \dots \lim_{t \to t_0} f_n(t) = \ell_n\]

\subsubsection*{Continuità}
Una funzione è continua se \(\lim_{t \to t_0} f(t) = f(t_0)\). Nel caso di funzioni vettoriali, essa è continua se ogni sua componente
è continua.

\newpage


\section{ Derivate, gradienti, tangenti, massimi e minimi}
\subsection{Derivate di funzioni vettoriali}
\subsubsection*{Definizione di derivata in una e più dimensioni}
Una funzione \(f\) è derivabile in \(t_0\) se esiste il limite finito del rapporto incrementale.
\[f'(t_0) := \lim_{t \to t_0} \frac{f(t)-f(t_0)}{t-t_0} = \ell \in \Rn\]
\[f(t) = \left(f_1(t), f_2(t), \dots f_n(t) \right) \quad \Leftrightarrow \quad f'(t_0) = \left( f_1'(t_0), f'_2(t_0), \dots f'_n(t_0)\right)\]

\subsubsection*{Retta tangente ad una curva}
Se \(f\) è derivabile in \(t_0\) e \(f'(t_0) \neq 0\):
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] il vettore tangente alla curva è \(f'(t_0)\)
	\item[-] la retta tangente alla curva è \(\left\{ f(t_0) + f'(t_0) \lambda, \; \lambda \in \Rn \right\}\)
\end{itemize}
Si parla di tangenza alla curva e non al sostegno perché una funzione può passare per lo stesso punto in due momenti diversi. In
questo caso si avrebbero due tangenti diverse per uno stesso punto del sostegno, quando invece sarebbero due tangeti associate a
due valori diversi del parametro della curva.

\subsubsection*{Funzioni o curve differenziabili e approssimazioni di primo ordine}
Una funzione \(f(t)\) si dice differenziabile se vale (specialmente il limite):
\[f(t) = f(t_0) + f'(t_0) (t-t_0) + R(t) \qquad \lim_{t \to t_0} \frac{R(t)}{t-t_0} = 0\]

\subsubsection*{Regole di derivazione di curve}
Siano \(f,g : \R \to \Rn\) curve derivabili, \(\varphi,u : \R \to \R\) funzioni derivabili, \(\alpha \in \R\), allora:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] \(\dt \; \left(\text{costante}\right) = 0\)
	\item[2.] \(\dt \; \left(\alpha f\right) = \alpha f'\)
	\item[3.] \(\dt \; \left(\varphi(t)f(t)\right) = \varphi'(t)f(t) + \varphi(t)f'(t)\)
	\item[4.] \(\dt \; \left(f + g\right) = f' + g'\)
	\item[5.] \(\dt \; \left(f \cdot g\right) = f' \cdot g + f \cdot g'\)
	\item[6.] \(\dt \; \left(f \circ u\right) = f'(u)u'\)
\end{itemize}

\subsection{Derivate direzionali e derivate parziali}
\subsubsection*{Definizione di derivata direzionale}
La derivata direzionale di \(f\) in un punto \(p\) lungo la direzione \(\vec{u}\) è definita come il limite, se esiste finito
del rapporto incrementale.
\begin{align*}
	D_{\vec{u}} f(p) = \partial_{\vec{u}} f(p) &:= \lim_{t \to 0} \frac{f(p + t \vec{u}) - f(p)}{t} = \ell \in \R \\
	&:= g'(0) \text{ con } g(t) = f(p + t\vec{u})
\end{align*}
Per trovare la derivata direzionale bisogna fare:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] trovare la funzione \(g(t) = f(p+t\vec{u})\)
	\item[2.] calcolare la derivata \(g'(t)\)
	\item[3.] valutare la derivata per \(t \to 0\)
	\item[Oss] nel caso in cui la funzione \(f\) non sia continua in \(p\) e non è possibile definire esattamente \(g(t)\) e \(g'(t)\),
	è consigliato usare la definizione per calcolare \(\displaystyle g'(0) = \lim_{t \to 0} \frac{g(t) - g(0)}{t} = \lim_{t \to 0} \frac{f(p + t \vec{u}) - f(p)}{t}\)
\end{itemize}

\subsubsection*{Definizione di derivata parziale}
La i-esima derivata parziale di \(f\) in \(p\) è la derivata direzionale lungo \(\vec{e_i}\) di \(f(x_1, \dots x_n)\),
\[\partial_{x_i} f(p) = D_{\vec{e_i}} f(p) = \frac{d}{d x_i} f(p) \qquad \text{casi particolari: } \begin{aligned}
	\partial_x f(p) &= D_{(1,0)} f(p) = \dx f(p) \\
	\partial_y f(p) &= D_{(0,1)} f(p) = \dy f(p)
\end{aligned}\]

\subsubsection*{Continuità e derivate direzionali}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] Una funzione può non essere continua in un punto \(p\), ma avere lo stesso derivate parziali e direzionali. In questo
	caso si sfrutta la definzione di derivata per calcolarne il valore.
\end{itemize}

\subsubsection*{Proprietà delle derivate direzionali}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] \(D_u (f(p) + g(p)) = D_u f(p) + D_u f(p)\)
	\item[2.] \(D_u (f(p)g(p)) = D_u f(p) \; g(p) + f(p) D_u g(p)\)
	\item[3.] \(D_u (c f(p)) = c D_u f(p)\)
	\item[4.] \(D_u (\varphi \circ p)(p)) = D_u \varphi(f(p)) = \varphi(f(p)) D_u f(p)\)
\end{itemize}

\subsection{Gradiente}
\subsubsection*{Definizione di gradiente}
\[\nab f(p) := \left( \partial_{x_1} f(p), \partial_{x_2} f(p), \dots \partial_{x_n} f(p)\right)\]

\subsubsection*{Proprietà del gradiente} % tra cui gradiente della norma
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] \(\nab (f(p) + g(p)) = \nab f(p) + \nab g(p)\)
	\item[2.] \(\nab (f(p) g(p)) = \nab f(p) \; g(p) + f(p) \nab g(p)\)
	\item[3.] \(\nab (c f(p)) = c \nab f(p)\)
	\item[4.] \(\nab (\varphi \circ f)(p) = \nab (\varphi(f(p))) = \varphi'(f(p)) \nab f(p)\)
	\item[5.] gradiente della norma: \(\displaystyle \nab \left|x\right| = \frac{x}{\left|x\right|}\)
\end{itemize}

\subsubsection*{Relazione tra gradiente e derivate direzionali in funzioni C1}
Una funzione \(f\) è di classe \(C^1\) in un aperto se:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(f\) è continua
	\item[-] \(f\) ha derivate parziali continue
\end{itemize}
In una funzione \(C^1\), le derivate direzionali e il gradiente hanno la seguente relazione:
\[\forall u \in \Rn \qquad D_u f(p) = \nab f(p) \cdot \vec{u} = \partial_{x_1} f(p) u_1 + \partial_{x_2} f(p) u_2 + \dots \partial_{x_n} f(p) u_n\]
Osservazione: se in una funzione generica la derivata parziale \(D_{(u_1, u_2)} f(p)\) non è esprimibile come combinazione
lineare delle derivate parziali, allora non è \(C^1\). In altre parole se non vale l'equazione \(D_u f(p) = \nab f(p) \cdot \vec{u}\), allora \(f \notin C^1\).

\subsubsection*{Direzione di massima e minima crescita}
Se \(f\) è \(C^1\) in un intorno di \(p\), la direzione lungo cui si ha la massima pendenza è la direzione del vettore gradiente.
Viceversa la direzione di minore crescita è opposta a quella di massima crescita.
\begin{align*}
	D_{u_{max}} f(p) &= \left| \nab f(p)\right| \qquad \quad \Leftrightarrow \qquad u_{max} = \frac{\nab f(p)}{\left| \nab f(p) \right|} \\
	D_{u_{min}} f(p) &= -\left| \nab f(p)\right| \qquad \Leftrightarrow \qquad u_{min} = -\frac{\nab f(p)}{\left| \nab f(p) \right|}
\end{align*}

\subsection{Spazio tangente e differenziabilità}
\subsubsection*{Spazio tangente}
Lo spazio tangente al grafico di una curva \(f(p)\) in un punto \(p\) è l'insieme dei punti: \\
(n.b.: \(x\) è un punto sul piano, come lo è anche \(p\), non è una coordinata)
\[\left\{ (x,z) \; \tc \; z = f(p) + \nab f(p) \cdot (x-p)\right\}\] 

\subsubsection*{Differenziabilità}
Una funzione è differenziabile in un punto \(p\) se la funzione \(f\) è approssimabile al piano tangente in \(p\) in un suo intorno
con un errore trascurabile. \(L(x)\) è la funzione affine detta linearizzazione di \(f\) in \(p\)
\begin{align*}
	f(x) &=  f(p) + \nab f(p) \cdot (x-p) + R(x), \qquad \lim_{x \to p} \frac{R(x)}{\left|x-p\right| = 0} \\
	L(x) &:= f(p) + \nab f(p) \cdot (x-p)
\end{align*}
Per sapere se una funzione è differenziabile bisogna controllare che il resto \(R(x) = o(\left|x-p\right|)\), cioè bisogna risolvere
il limite per \(x \to p\) e verificare che faccia 0.

\subsubsection*{Continuità di funzioni differenziabili}
Una funzione differenziabile in \(p\) è anche continua in \(p\). Quindi valgono le seguenti inclusioni.
\[\text{Funzioni con derivate parziali} \subset \text{Funzioni continue} \subset \text{Funzioni differenziabili} \subset \text{Funzioni } C^1\]

\subsubsection*{Gradiente di funzioni differenziabili}
Nelle funzioni differenziabili il gradiente vale:
\[D_u f(p) = \nab f(p) \cdot u\]

\subsubsection*{Regola della catena di derivate parziali}
Per la regola della catena (con funzione a due variabili e in generale a \(n\) variabili):
\begin{align*}
	\dt f(x(t),y(t)) &= \partial_x f(x(t,y(t))) \cdot x'(t) + \partial_y f(x(t),y(t)) \cdot y'(t) = \nab f(x(t),y(t)) \cdot (x'(t), y'(t)) \\
	\dt f(r(t)) &= \nab f(r(t)) \cdot r'(t) = \partial_{x_1} f(x(t)) {x_1}'(t) + \partial_{x_2} f(x(t)) {x_2}'(t) + \dots + \partial_{x_n} f(x(t)) {x_n}'(t)
\end{align*}

\subsubsection*{Gradiente e curve di livello}
Il gradiente è perpendicolare alle curve di livello. Nelle curve di livello vale:
\[f(r(t)) = \text{costante} \quad \Rightarrow \quad \dt f(r(t)) = 0 \quad \Rightarrow \quad \nab f(r(t)) \cdot r'(t) = 0 \quad \Rightarrow \quad \nab f(r(t)) \perp r'(t)\]
con \(r'(t)\) un vettore con la stessa direzione della tangete alla curva di livello, per un certo \(t\).

\newpage

\subsection{Derivate seconde, matrice Hessiana}
\subsubsection*{Derivate seconde}
La derivata parziale di secondo ordine è definita come:
\[\partial^2_{x_i,x_j} f(x) = \frac{\partial^2 f(x)}{\partial_{x_i} \partial_{x_j}} = \partial_{x_i} (\partial_{x_j} f(x))\]

\subsubsection*{Matrice Hessiana}
\[\text{Hess}f(x) := \left( \begin{matrix}
	\partial^2_{{x_1}^2} f(x) & \cdots & \partial^2_{{x_1,x_n}} f(x) \\
	\vdots & \ddots & \vdots \\
	\partial^2_{{x_n,x_1}} f(x) & \cdots & \partial^2_{{x_n}^2} f(x) \\
\end{matrix} \right) \]

\subsubsection*{Teorema di Schwarz}
Data una funzione \(f\) di classe \(C^2\) (ovvero con derivate parziali doppie continue), allora vale:
\[\partial^2_{x_i,x_j} f(x) = \partial^2_{x_j,x_i} f(x) \qquad \forall i,j\]
Per questo principio, la matrice hessiana di funzioni \(C^2\) è una matrice simmetrica.

\subsection{Massimi e minimi}
\subsubsection*{Definizione di massimi, minimi e punti di sella}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] il punto \(p\) è punto di minimo assoluto se \(f(x) \geq f(p) \quad \forall x \in D\)
	\item[-] il punto \(p\) è punto di massimo assoluto se \(f(x) \leq f(p) \quad \forall x \in D\)
	\item[-] il punto \(p\) è punto di minimo relativo se \(\exists U_p \; \text{intorno di} \; p \; \tc \; f(x) \geq f(p) \quad \forall x \in U_p \cap D\)
	\item[-] il punto \(p\) è punto di massimo relativo se \(\exists U_p \; \text{intorno di} \; p \; \tc \; f(x) \leq f(p) \quad \forall x \in U_p \cap D\)
	\item[-] il punto \(p\) è punto di sella se \(\forall U_p \; \text{intorno di} \; p \; \exists x,y \in U_p \cap D \; \tc \; f(x) < f(p) < f(y)\)
\end{itemize}

\subsubsection*{Regola di Fermat e punti critici interni}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] Se \(f\) derivabile rispetto a \(\vec{u}\) in \(p\), con \(p\) punto di massimo o minimo locale \textbf{interno}
	al dominio, allora vale che \(D_u f(p) = 0\), per cui \(\nab f(p) = 0\)
	\item[-] I punti interni al dominio per cui \(\nab f(p) = 0\) si dicono punti critici e possono essere classificati
	come punti di massimo locale, di minimo locale o punti di sella.
	\item[-] Per trovare i punti critici \textbf{interni} al dominio, per definizione, bisogna risolvere \(\nab f(p) = 0\).
\end{itemize}

\subsubsection*{Criterio dell'Hessiana}
Data una funzione \(f\) di classe \(C^2\) e \(p\) punto critico interno al dominio, allora:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(\det H_f(p) > 0\) e \(\partial^2_{x,x}f(p) > 0\) allora \(p\) è minimo locale
	\item[-] se \(\det H_f(p) > 0\) e \(\partial^2_{x,x}f(p) < 0\) allora \(p\) è massimo locale
	\item[-] se \(\det H_f(p) < 0\) allora \(p\) è punto di sella
	\item[-] se \(\det H_f(p) = 0\) allora non si può concludere nulla
\end{itemize}

\subsubsection*{Massimi e minimi assoluti su domini chiusi e limitati}
Per Weierstrass una funzione continua in un dominio chiuso e limitato ammette massimo e minimo assoluto. Per trovare il massimo e minimo
assoluto di una funzione bisogna controllare i punti critici interni al dominio \(D\) e i punti sul bordo del dominio \(\partial D\).

\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\max f = \max \left\{ f(p) \; \tc \; p \in \left\{\text{punti di massimo interni a } D\right\} \cup \left\{\text{punti di massimo su } \partial D \right\} \right\}\)
	\item[-] \(\min f = \min \left\{ f(p) \; \tc \; p \in \left\{\text{punti di minimo interni a } D\right\} \cup \left\{\text{punti di minimo su } \partial D \right\} \right\}\)
\end{itemize}

\newpage


\section{Campi e integrali curvilinei}
\subsection{Integrali curvilinei di funzioni reali}
\subsubsection*{Definizione}
Data una funzione reale \(\mu(x)\) e una curva \(r(t)\) definita in un intervallo chiuso \([a,b]\), allora l'integrale curvilineo
di \(\mu\) sulla curva \(r\) vale:
\[\int_r \mu \; ds := \int_a^b \mu(r(t)) \cdot \left| r'(t) \right| \, dt\]

\subsubsection*{Lunghezza di una curva}
La lunghezza di una curva è l'integrale curvilineo con \(\mu = 1\):
\[\text{Lunghezza} := \int_r \; ds = \int_a^b \left| r'(t) \right| \, dt\]

\subsubsection*{Baricentro di una curva}
Il baricentro di una curva \(r\) con densità \(\mu(x)\) è definito come:
\[\text{Baricentro} := \frac{\displaystyle \int_{r} (x_1, x_2, \dots x_n) \mu(x) \; ds }{\displaystyle \int_r \mu \; ds} = \frac{\displaystyle \int_{r} x_1 \mu(x) \; ds + \int_{r} x_2 \mu(x) \; ds + \dots + \int_{r} x_n \mu(x) \; ds }{\displaystyle \int_r \mu \; ds}\]
Per \(\mu = 1\) si ottiene il baricentro geometrico:
\[\text{Baricentro geometrico} := \frac{\displaystyle \int_{r} (x_1, x_2, \dots x_n) \; ds }{\displaystyle \int_r \; ds} = \frac{\displaystyle \int_{r} x_1 \; ds + \int_{r} x_2 \; ds + \dots + \int_{r} x_n \; ds }{\text{Lunghezza(r)}}\]

\subsubsection*{Proprietà dell'integrale curvilineo}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] \(\displaystyle \int_r (\mu(x) + \nu(x)) \; ds = \int_r \mu(x) \; ds + \int_r \nu(x) \; ds\)
	\item[2.] se \(c =\) costante, allora \(\displaystyle \int_r c\mu(x) \; ds = c \int_r \mu(x) \; ds\)
	\item[3.] se \(\mu \leq \nu\), allora \(\displaystyle \int_r \mu(x) \; ds \leq \int_r \nu(x) \; ds\)
	\item[4.] cammino inverso: \(\displaystyle \int_{r^{-1}} \mu(x) \; ds = \int_r \mu(x) \; ds\)
	\item[5.] giustapposizione di cammini: \(\displaystyle \int_{r_1,r_2} \mu(x) \; ds = \int_{r_1} \mu(x) \; ds + \int_{r_2} \mu(x) \; ds\) 
\end{itemize}

\newpage

\subsection{Campi vettoriali}
\subsubsection*{Definizione}
Un campo vettoriale è una funzione \(F : \begin{matrix}
	D \subseteq \Rn & \to & \Rn \\
	x \in D & \mapsto & F(x)
\end{matrix}\) con \(F(x) = (F_1(x), F_2(x), \dots F_n(x))\)

\subsubsection*{Campi vettoriali radiali}
Un campo vettoriale definito in \(\Rn \; \backslash \left\{0\right\}\) è detto radiale se ha la forma \(\displaystyle F(x) = g(x) \cdot \frac{x}{\left|x\right|}\) \\
Esempi di campi radiali:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] campo gravitazionale \(\displaystyle F(r) = -\frac{G \cdot m}{\left|r^2\right|} \; \hat{r} = g(\left|r\right|) \; \frac{r}{\left|r\right|}\) nella forma \(\displaystyle F(x) = \frac{K}{\left|x^2\right|} \; \frac{x}{\left|x\right|}\)
	\item[-] campo elettrico \(\displaystyle E(r) = \frac{q}{4 \pi \varepsilon_0 \left|r^2\right|} \; \hat{r} = g(\left|r\right|) \; \frac{r}{\left|r\right|}\) nella forma \(\displaystyle F(x) = \frac{K}{\left|x^2\right|} \; \frac{x}{\left|x\right|}\)
	\item[-] campi della forma \(F(x) = h(\left|x\right|) \; x\), infatti per \(x \neq 0\) si ha \(\displaystyle F(x) = \left|x\right| \; g(\left|x\right|) \; \frac{x}{\left|x\right|}\)
\end{itemize}

\subsubsection*{Campi gradienti e potenziali}
Un campo \(F\) è detto gradiente se esiste \(U:D \to \Rn \in C^1\) per cui \(F = \nab U\). \\
Il potenziale, in fisica, è definito \(V := -U\)

\subsubsection*{Gradienti da ricordare}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{campo radiale} della forma \(F(x) = h(x) \; \frac{x}{\left|x\right|}\) con primitiva \(g(x) = \int h(x) \; dx\)
	\item[-] \textbf{campo gravitazionale} della forma \(F(x,y) = \frac{K}{x^2 + y^2} \; \frac{(x,y)}{\left|(x,y)\right|}\) con gradiente \(G(x,y) = -\frac{K}{\left|(x,y)\right|}\)
	\item[-] \textbf{campi continui} della forma \(F(x_1, \dots x_n) = (f_1(x_1), \dots f_n(x_n))\)
\end{itemize}

\subsection{Integrali curvilinei di campi}
Dato un campo \(F : \Rn \to \Rn\) e un cammino \(r : [a,b] \to \Rn \in C^1\) a tratti, allora l'integrale curvilineo del campo
\(F\) lungo il cammino \(r\) vale:
\[\int_{r} \vec{F} \cdot d \vec{r} = \int_{r} \vec{F}(x) \cdot d\vec{r} := \int_a^b \vec{F}(r(t)) \cdot \vec{r'}(t) \; dt\]

\subsubsection*{Circuitazione}
Se il cammino \(r\) è un percorso chiuso, l'integrale prende il nome di circuitazione di \(F\) su \(r\): \(\displaystyle \oint_r \vec{F} \cdot d\vec{r}\)

\subsubsection*{Lavoro di una forza}
Se \(F\) è una forza e \(r(t)\) è un cammino, il lavoro di \(F\) lungo \(r\) è: \(\displaystyle W = \int_r \vec{F} \cdot d\vec{r} = \int_a^b \vec{F}(\vec{r}(t)) \cdot \vec{r'}(t) \; dt\) \\
Se la forza è costante e \(r(t) = P + t \overline{PQ}\) il lavoro diventa: \(\displaystyle W = \int_r \vec{F} \cdot d\vec{r} = \int_0^1 \vec{F} \cdot \overrightarrow{PQ} \; dt = F \cdot \overrightarrow{PQ}\)

\subsubsection*{Notazioni alternative}
\begin{itemize}
	\item[1.] \(\displaystyle \int_r \vec{F} \cdot d\vec{r} = \int_r F_1(x,y) dx + F_2(x,y) dy\)
	\item[2.] \(\displaystyle \int_r \vec{F} \cdot d\vec{r} = \int_r \vec{F} \cdot \vec{T} \; ds \quad\) con \(\displaystyle \vec{T}(x) = \vec{T}(\vec{r}(t)) := \frac{\vec{r'}(t)}{\left|\vec{r'}(t)\right|}\) detto campo dei vettori tangenti
	\item[3.] \(\displaystyle \int_r \vec{F} \cdot d\vec{r} = \int_r (F_1(x), \dots F_n(x)) \cdot (dx_1, \dots dx_n) = \int_r F_1(x)dx_1 + \dots F_n(x) dx_n \quad\) not. termodinamica
\end{itemize}
%Dimostrazioni:
%\begin{align*}
%	\text{1.} \quad \int_r \vec{F} \cdot d\vec{r} &= \int_a^b \vec{F}(\vec{r}(t)) \cdot \vec{r'}(t) \; dt = \int_a^b \left(\begin{matrix} F_1(x(t),y(t)) \\ F_2(x(t),y(t)) \end{matrix}\right) \cdot \left(\begin{matrix} x'(t) \\ y'(t) \end{matrix}\right) \; dt = \\
%	&= \int_a^b F_1(x(t),y(t)) x'(t) + F_2(x(t),y(t)) y'(t) \; dt = \int_r F_1(x,y) dx + F_2(x,y) dy \\
%	\text{2.} \quad \int_r \vec{F} \cdot d\vec{r} &= \int_a^b \vec{F}(\vec{r}(t)) \cdot \vec{r'}(t) \; dt = \int_a^b \vec{F}(\vec{r}(t)) \cdot \frac{\vec{r'}(t)}{\left|\vec{r'}(t)\right|} \left|\vec{r'}(t)\right| \, dt = \int_r \vec{F} \cdot \vec{T} \; ds
%\end{align*}

\subsection{Campi conservativi}
\subsubsection*{Definizione}
Un campo \(F: \Rn \to \Rn\) è conservativo se \(\forall r_1, r_2\) cammini con stessi estremi o \(\forall r\) cammino chiuso, vale:
\[\int_{r_1} \vec{F} \cdot d\vec{r_1} = \int_{r_2} \vec{F} \cdot d\vec{r_2} \qquad \Leftrightarrow \qquad \oint_r \vec{F} \cdot d\vec{r} = 0\]

\subsubsection*{Teorema fondamentale dei campi gradienti}
Sia \(F: \Rn \to \Rn\) un campo gradiente con \(\nab U = F\) e \(r: [a,b] \to \Rn\) un cammino, allora vale
\[\int_r \vec{F} \cdot d\vec{r} = U(r(b)) - U(r(a))\]
Siccome l'integrale è indipendente dal cammino, allora il campo è conservativo.
\[\text{Dim} \quad \int_r \vec{F} \cdot d\vec{r} = \int_a^b \vec{F}(\vec{r}(t)) \cdot \vec{r'}(t) \; dt = \int_a^b \nab U(\vec{r}(t)) \cdot \vec{r'}(t) \; dt = \int_a^b \dt U(\vec{r}(t)) \; dt = U(r(b)) - U(r(a))\]

\subsubsection*{Equivalenze campi conservativi, campi gradienti, circuitazione}
campo conservativo \(\; \Leftrightarrow \;\) campo gradiente \(\;\;\Leftrightarrow \;\; \int_r \vec{F} \cdot d\vec{r}\) dipende solo da \(r(a)\) e \(r(b) \;\; \Leftrightarrow \;\; \oint_r \vec{F} \cdot d\vec{r} = 0\)

\subsection{Campi irrotazionali}
\subsubsection*{Definizione}
Un campo \(F: \Rn \to \Rn\) è irrotazionale se e solo se \(\partial_{x_i} F_j = \partial_{x_j} F_i \;\; \forall i,j\)

\subsubsection*{Rotore}
Il rotore \(\rot(\vec{F}) : \vec{F} \to \vec{G}\) è un operatore che da un campo vettoriale, restituisce un altro campo vettoriale.
Se si interpreta il campo \(\vec{F}\) come il moto di particelle di un fluido sullo spazio, il campo \(\rot(\vec{F})\) è un campo
vettoriale i cui vettori hanno direzione e verso uguale all'asse di rotazione delle particelle attorno ad un punto \(x\) e modulo
pari al doppio della velocità angolare delle particelle che si muovono lungo i vettori del campo \(F\).
\[\rot(\vec{F}) = \nab \times \vec{F} = \det \left(\begin{matrix} x & y & x \\[2pt] \frac{\partial}{\partial_x} & \frac{\partial}{\partial_y} & \frac{\partial}{\partial_z} \\[4pt] F_x & F_y & F_z \end{matrix}\right) =
\left( \frac{\partial F_z}{\partial y} - \frac{\partial F_y}{\partial z}, \; \frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x}, \; \frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y}\right)\]

\subsubsection*{Campo conservativo + C1 implica campo irrotazionale}
Se \(F: \Rn \to \Rn\) è un campo \(C^1\) e conservativo, allora è anche irrotazionale. \\
Dimostrazione:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] \(F\) conservativo \(\;\; \Leftrightarrow \;\; \nab U = F \;\; \Leftrightarrow \;\; \partial_{x_i}U = F_i\)
	\item[2.] \(F \in C^1 \;\; \Leftrightarrow \;\; \partial_{x_i} F_j = \partial_{x_j} F_i\) per Terorema di Schwarz
	\item[3.] per 1. si ha \(\partial_{x_i} F_j = \partial_{x_i} \partial_{x_j} U \;\;\) e \(\;\;\partial_{x_j} F_i = \partial_{x_j} \partial_{x_i} U\)
	\item[4.] per 2. si ha \(\partial_{x_i} F_j = \partial_{x_j} F_i\) ovvero che \(F\) è irrotazionale
\end{itemize}
Osservazione: tutti i campi conservativi e \(C^1\) sono irrotazionali, ma non tutti i campi irrotazionali sono conservativi e \(C^1\).

\subsubsection*{Domini semplicemente connessi}
Un dominio aperto \(D \subseteq \Rn\) è \textbf{connesso} se \(\forall x,y \in D \; \exists r:[a,b] \to D\) tale che \(r(a) = x, r(b) = y\) \\
Un dominio aperto \(D \subseteq \Rn\) è \textbf{semplicemente connesso} se ogni circuito \(r:[a,b] \to D\) si può contrarre con continuità
ad un punto in \(D\).

\subsubsection*{Irrotazionali su domini semplicemente connessi}
Se un campo \(F \in C^1\) è irrotazionale su un dominio semplicemente connesso, allora \(F\) è conservativo
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] campo conservativo e \(C^1 \; \Rightarrow \;\) campo irrotazionale
	\item[-] campo conservativo e \(C^1\) su \(D\) sempl. connesso \(\; \Leftrightarrow \;\) campo irrotazionale e \(C^1\) su \(D\) sempl. connesso
\end{itemize}
Osservazione: se ho un campo irrotazionale su un dominio \(D\), posso restringere il dominio ad uno semplicemente connesso (es.
un rettangolo o una palla centrata in un punto), per cui il campo diventa anche gradiente sul dominio ristretto.

\subsection{Calcolo delle primitive con metodo delle integrazioni marginali}
Sia \(F(x,y) = (F_x(x,y), F_y(x,y))\) campo vettoriale irrotazionale, gradiente, \(C^1\) su dominio semplicemente connesso. Si vuole trovare il potenziale \(U(x,y)\).
\begin{itemize}
	\item[1.] siccome è gradiente, vale \(\nab U = \vec{F}\):
	\(\begin{cases}
		\partial_x U(x,y) = F_x(x,y) \\
		\partial_y U(x,y) = F_y(x,y)
	\end{cases}\)

	\item[2.] integro una delle due equazioni per ottenere \(U\): \\
	\(\displaystyle \int \partial_x U(x,y) \; dx = \int F_x(x,y) \; dx \;\; \Rightarrow \;\; U(x,y) = U_\text{parziale}(x,y) + \varphi(y)\)

	\item[3.] osservo che \(\varphi(y)\) è indipendente da \(x\) e vale \(\partial_x (U(x,y) - U_\text{parziale}(x,y)) = 0\)
	
	\item[4.] risolvo l'altra equazione sostituendo \(U(x,y) = U_\text{parziale}(x,y) + \varphi(y)\) e ottenendo \(\varphi(y)\): \\
	\(\displaystyle \partial_y (U_\text{parziale}(x,y) + \varphi(y)) = F_y(x,y) \;\; \Rightarrow \;\; \varphi'(y) = g(y) + c \;\; \Rightarrow \;\; \varphi(y) = \int \varphi'(y) \; dy \)

	\item[5.] a questo punto ho \(U_\text{parziale}(x,y)\) dal punto 2 e \(\varphi(y)\) dal punto 4, per cui posso ottenere il potenziale: \(U(x,y) = U_\text{parziale}(x,y) + \varphi(y)\)
\end{itemize}
Osservazioni:
\begin{itemize}
	\item[-] se nel punto 4 ottengo una espressione \(\varphi'(y)\) che dipende anche da \(x\), allora non sono verificate tutte
	le condizioni del dominio e il campo potrebbe essere non gradiente o rotazionale.
	\item[-] se ho un campo irrotazionale, ma il dominio non è semplicemente connesso, posso partizionare il dominio in rettangoli
	(es. un rettangolo comprende le \(x > 0\) e un altro le \(x < 0\)) e trovare una primitiva per ogni rettangolo.
\end{itemize}

\newpage


\section{Integrali doppi}
\subsection{Integrali doppi su rettangoli}
\subsubsection*{Integrali doppi alla Riemann}
Data una funzione \(f:[a,b] \times [c,d] \to \R\), il volume del trapeziode sotteso dalla funzione è dato dalla somma di Riemann
dei parallepipedi ottenuti dividendo il dominio in piccoli rettangoli con lato tendente a 0 e si indica:
\[\int_{[a,b] \times [c,d]} f(x,y) \; dx \, dy\]

\subsubsection*{Integrale iterato sui rettangoli}
L'integrale doppio può essere scritto come integrale iterato:
\[\int_{[a,b] \times [c,d]} f(x,y) \; dx \, dy = \int_{a}^{b} \int_{c}^{d} f(x,y) \; dy \, dx = \int_{c}^{d} \int_{a}^{b} f(x,y) \; dx \, dy\]

\subsubsection*{Formula di riduzione sui rettangoli}
Si osserva che è possibile ottere l'area di una fetta verticale lungo un asse integrando solo lungo \(x\) o \(y\):
\[\int_{a}^{b} f(x,y) \; dx = {A(y)}_\text{area fetta \(\perp\) a y} \qquad \int_{c}^{d} f(x,y) \; dy = {A(x)}_\text{area fetta \(\perp\) a x}\]
Quindi per risolvere l'integrale doppio posso prima risolverlo integrando per una variabile trovando l'area di una fetta verticale
e successivamente integrare l'area trovata rispetto all'altra variabile.
\[\int_{[a,b] \times [c,d]} f(x,y) \; dx \, dy \;\; = \;\; \begin{matrix}
	\displaystyle \int_{a}^{b} \left\{ \int_{c}^{d} f(x,y) \; dy \right\} \, dx = \int_{a}^{b} {A(x)}_\text{area fetta \(\perp\) a x} \; dx \\[15pt]
	\displaystyle \int_{c}^{d} \left\{ \int_{a}^{b} f(x,y) \; dx \right\} \, dy = \int_{c}^{d} {A(y)}_\text{area fetta \(\perp\) a y} \; dy
\end{matrix}\]

\subsubsection*{Integrale di un prodotto di funzioni}
Se la funzione \(f(x,y)\) è a variabili separabili, ovvero se \(f(x,y) = g(x)h(y)\), allora l'integrale diventa:
\[\int_{[a,b] \times [c,d]} f(x,y) \; dx \, dy = \int_{[a,b] \times [c,d]} g(x) h(y) \; dx \, dy = \left(\int_{a}^{b} g(x) \; dx\right) \left(\int_{c}^{d} h(y) \; dy\right)\]

\newpage

\subsection{Integrali doppi su domini limitati}
\subsubsection*{Funzione integrabile}
Una funzione \(f(x,y)\) è integrabile in un integrale doppio su un dominio \(D\) se \(f\) è continua e limitata e se \(D\) è 
limitato e misurabile.

\subsubsection*{Proprietà dell'integrale doppio}
\begin{itemize}
	\item[1.] \(\displaystyle \int_D c f(x,y) \; dx \, dy = c \int_D f(x,y) \; dx \, dy\)
	\item[2.] \(\displaystyle \int_D f(x,y) + g(x,y) \; dx \, dy = \int_D f(x,y) \; dx \, dy + \int_D g(x,y) \; dx \, dy\)
	\item[3.] se \(f \geq 0\) allora \(\displaystyle \int_D f(x,y) \; dx \, dy \geq 0\)
	\item[4.] se \(D\) è sostegno di una curva, allora \(\displaystyle \int_D f(x,y) \; dx \, dy = 0\)
	\item[5.] se \(D = D_1 \cup D_2\) con \(D_1 \cap D_2\) pari al più ad un'unione di sostegni di curve, allora si ha: \\
	\(\displaystyle \int_D f(x,y) \; dx \, dy = \int_{D_1} f(x,y) \; dx \, dy + \int_{D_2} f(x,y) \; dx \, dy\)
\end{itemize}

\subsubsection*{Integrali su regioni semplici rispetto a x}
Una regione \(D\) è semplice rispetto a \(x\) se \(D = \left\{ (x,y) : x \in [a,b], \alpha(x) \leq y \leq \beta(x) \right\}\). \\
L'integrale iterato di \(f\) su \(D\) diventa:
\[\int_D f(x,y) \; dx \, dy = \int_a^b \int_{\alpha(x)}^{\beta(x)} f(x,y) \; dy \, dx = \int_a^b \left\{\int_{\alpha(x)}^{\beta(x)} f(x,y) \; dy\right\} \; dx\]

\subsubsection*{Integrali su regioni semplici rispetto a y}
Una regione \(D\) è semplice rispetto a \(y\) se \(D = \left\{ (x,y) : y \in [c,d], \alpha(y) \leq x \leq \beta(y) \right\}\). \\
L'integrale iterato di \(f\) su \(D\) diventa:
\[\int_D f(x,y) \; dx \, dy = \int_c^d \int_{\alpha(x)}^{\beta(x)} f(x,y) \; dx \, dy = \int_c^d \left\{\int_{\alpha(y)}^{\beta(y)} f(x,y) \; dx\right\} \; dy\]

\subsection{Cambiamento di variabile}
\subsubsection*{La matrice Jacobiana per il cambio di variabili}
La matrice Jacobiana di una funzione \(\varphi = (\varphi_1, \varphi_2)\) è definita come:
\[\varphi'(u,v) = J_{\varphi} (u,v) := \left( \begin{matrix}
	\partial_u \varphi_1 (u,v) & \partial_v \varphi_1 (u,v) \\
	\partial_u \varphi_2 (u,v) & \partial_v \varphi_2 (u,v)
\end{matrix} \right) = \left( \begin{matrix}
	\nab \varphi_1 (u,v) \\
	\nab \varphi_2 (u,v)
\end{matrix} \right) \qquad \text{con} \; \left| \varphi'(u,v) \right| := \left| \det \varphi' (u,v)\right|\]

\subsubsection*{Formula del cambiamento di variabili}
Dato un integrale doppio di \(f(x,y)\) su \(D\) nelle variabili \(x\) e \(y\), è possibile riscriverlo in funzione delle variabili
\(u\) e \(v\), attraverso \(\begin{matrix}
	\varphi (u,v) = (x(u,v), \; y(u,v)) = (x,y) \\[2pt]
	\varphi^{-1} (x,y) = (u(x,y), \; v(x,y)) = (u,v)
\end{matrix}\), con \(\begin{matrix}
	D = \varphi(E) \\[2pt]
	E = \varphi^{-1}(D)
\end{matrix}\) come segue:
\[\int_D f(x,y) \; dx \, dy = \int_E f(\varphi(u,v)) \left| \varphi'(u,v) \right| \, du, \, dv\]

\subsubsection*{Elemento d'area, dilatazioni e applicazioni lineari}
Il fattore \(\left| \varphi'(u,v) \right|\) è detto elemento d'area ed è il coefficiente di proporzionalità tra le aree di \(D\) ed \(E\):
\[\text{Aera}(D) = \left| \varphi'(u,v) \right| \text{Area}(E)\]

\subsubsection*{Simmetrie}
Se il dominio è simmetrico rispetto all'asse y (\((x,y) \in D \Leftrightarrow (-x,y) \in D\)) e la funzione è dispari, ovvero \(f(-x,y) = -f(x,y)\),
allora \(\displaystyle \int_D f(x,y) \; dx \, dy = 0\) \\
Se il dominio è simmetrico rispetto all'asse y (\((x,y) \in D \Leftrightarrow (-x,y) \in D\)) e la funzione è pari, ovvero \(f(-x,y) = f(x,y)\), 
definito \(D^+ := \left\{(x,y) \in D : x > 0\right\}\) allora \(\displaystyle \int_D f(x,y) \; dx \, dy = 2 \int_{D^+} f(x,y) \; dx \, dy\)

\subsubsection*{Baricentro in un piano}
Dato un dominio \(D\), per calcolare il baricentro della regione \(D\):
\[\text{Baricentro} = (x_D, y_D) := \left( \frac{\displaystyle \int_D x \; dx \, dy}{\displaystyle \int_D \, dx \, dy}, \;\; \frac{\displaystyle \int_D y \; dx \, dy}{\displaystyle \int_D dx \, dy} \right)\]

\subsubsection*{Passaggio in coordinate polari}
\begin{itemize}
	\item[1.] Definiamo \(\begin{cases} x = \rho \cos t \\ y = \rho \sin t \end{cases}\) con \(\rho > 0\) e \(t \in [0, 2\pi]\), per cui \(\varphi(\rho,t) = (\rho \cos t, \rho \sin t)\)
	\item[2.] Si ha che: \(\varphi'(\rho,t) = \left( \begin{matrix} \cos t & - \rho \sin t \\ \sin t & \rho \cos t \end{matrix} \right), \quad \left|\varphi'(\rho,t)\right| = \left|\det \varphi'(\rho,t)\right| = \rho, \quad E = \varphi^{-1}(D)\)
	\item[3.] Inoltre \(f\) è integrabile in \(D\) se e solo se \(f(\rho \cos t, \rho \sin t) \; \rho\) è integrabile su \(E\) e si ha:
	\[\displaystyle \int_D f(x,y) \; dx \, dy =  \int_E f(\rho \cos t, \rho \sin t) \; \rho \; d\rho \, dt\]
\end{itemize}

\newpage

\subsection{Integrali doppi generalizzati su domini illimitati}
\subsubsection*{Integrale doppio generalizzato e integrale iterato generalizzato}
L'integrale doppio generalizzato ha la forma:
\[\int_{\Rd} f(x,y) \; dx \, dy \;\; = \;\; \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} f(x,y) \; dx \, dy\]

\subsubsection*{Criterio di integrabilità}
La funzione \(f\) \textbf{positiva} (\(f: \Rd \to [0,+\infty[\)) è integrabile in senso generalizzato su \(\Rd\) se è verificata
almeno una delle seguenti condizioni
\begin{itemize}
	\item[1.] \(\displaystyle \int_{-\infty}^{+\infty} f(x,y) \; dy\) e \(\displaystyle \int_{-\infty}^{+\infty} \left\{\int_{-\infty}^{+\infty} f(x,y) \; dy\right\} \, dx\) esistono finiti
	\item[2.] \(\displaystyle \int_{-\infty}^{+\infty} f(x,y) \; dx\) e \(\displaystyle \int_{-\infty}^{+\infty} \left\{\int_{-\infty}^{+\infty} f(x,y) \; dx\right\} \, dy\) esistono finiti
\end{itemize}
Osservazione: se la funzione non è definita positiva o se il suo modulo non è integrabile, allora i processi risolutivi per
integrali iterati, cambi di variabile,... potrebbero non valere più e dare risultati finiti, ma diversi. In particolare se
scambio l'ordine delle variabili di integrazione ottengo due risultati diversi.

\subsubsection*{Integrale di Gauss}
L'integrale di Gauss, che normalmente non si può risolvere è \(\displaystyle \int_{-\infty}^{+\infty} e^{-t^2} \; dt = \sqrt{\pi}\)
\begin{itemize}
	\item[1.] risolvo \(\displaystyle \int_{\Rd} e^{-x^2-y^2} \; dx \, dy\) con formule di riduzione:
	\begin{align*}
		\int_{\Rd} e^{-x^2-y^2} \; dx \, dy &= \int_{\Rd} e^{-x^2} \cdot e^{-y^2} \; dx \, dy = \left(\int_{-\infty}^{+\infty} e^{-x^2} \; dx\right) \cdot \left(\int_{-\infty}^{+\infty} e^{-y^2} \; dy\right) \qquad \qquad \qquad \quad\\
		&= (\text{Integrale di Gauss})^2
	\end{align*}
	\item[2.] risolvo \(\displaystyle \int_{\Rd} e^{-x^2-y^2} \; dx \, dy\) con coordinate polari:
	\begin{align*}
		\int_{\Rd} e^{-x^2-y^2} \; dx \, dy &= \int_{\rho = 0}^{+\infty} \int_{t = 0}^{2\pi} e^{-\rho^2} \rho \; dt \, d\rho = \left(\int_{0}^{+\infty} \rho e^{-\rho^2} \; d\rho\right) \cdot \left(\int_{0}^{2\pi} \; dt\right) = 2\pi \cdot \left[- \frac{e^{-\rho^2}}{2}\right]_0^{+\infty} \\
		& = \pi
	\end{align*}
	\item[3.] unendo i risultati ottengo \( (\text{Integrale di Gauss})^2 = \pi \quad \Rightarrow \quad \text{Integrale di Gauss} = \sqrt{\pi}\)
\end{itemize}

\newpage


\section{Superfici parametriche}
\subsection{Notazioni}
\subsubsection*{Definizione}
Una superficie parametrica in \(\Rt\) su dominio chiuso e limitato \(D \subset \Rd\) è definita:
\[p: D \subset \Rd \to \Rt, \quad p(u,v) = (p_1(u,v), p_2(u,v), p_3(u,v))\]

\subsubsection*{Sostegno di una superficie parametrica}
Il sostegno di una superficie \(p\) è \(p(D) \subset \Rt\), che è \(C^1\) se \(p_1, p_2, p_3\) sono \(C^1\) in \(D\).

\subsubsection*{Superfici parametriche notevoli}
\begin{itemize}
	\item[-] \textbf{Superficie cartesiana}:
	\[p(u,v) = (u,v,f(u,v)) \quad \text{con sostegno} \; \left\{(u,v,f(u,v)) : (u,v) \in D \subset \Rd\right\}\]
	\item[-] \textbf{Nastro di Möbius}:
	\[p(u,v) = ((R + v \cos (u/2) \cos u), \;\; (R + v \cos (u/2)) \sin u, \;\; v \sin (u/2))\]
	\item[-] \textbf{Superficie sferica}:
	\[\varphi(\rho, \phi, \theta) = (\rho \sin \phi \cos \theta, \rho \sin \phi \sin \theta, \rho \cos \phi) \quad \rho \in [0,+\infty[, \; \phi \in [0, \pi], \; \theta \in [0, 2\pi]\]
\end{itemize}

\subsection{Area di una superficie parametrica}
\subsubsection*{Elemento d'area di una superficie parametrica}
L'elemento d'area di una superficie parametrica \(p(u,v)\) è l'area del parallelogramma formato dai due vettori
\(\vec{p_u}(u,v) := \partial_u p(u,v)\) e \(\vec{p_v}(u,v) := \partial_v p(u,v)\)
\[\text{Elemento d'area}(p) := \left| \vec{p_u}(u,v) \times \vec{p_v}(u,v)\right| = \left| \partial_u p(u,v) \times \partial_v p(u,v) \right|\]
Osservazioni:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] per \(\overline{u}\) e \(\overline{v}\) fissati, con \((\overline{u}, \overline{v}) \in D\): \\
	\(\partial_u p(\overline{u},\overline{v}) := (\partial_u p_1(\overline{u}, \overline{v}), \; \partial_u p_2(\overline{u}, \overline{v}), \; \partial_u p_3(\overline{u}, \overline{v}))\)
	è il vettore tangente alla curva \(u \mapsto p(u,\overline{v})\) in \(u = \overline{u}\) \\
	\(\partial_v p(\overline{u},\overline{v}) := (\partial_v p_1(\overline{u}, \overline{v}), \; \partial_v p_2(\overline{u}, \overline{v}), \; \partial_v p_3(\overline{u}, \overline{v}))\)
	è il vettore tangente alla curva \(v \mapsto p(\overline{u},v)\) in \(v = \overline{v}\)
	\item[2.] l'area del parallelogramma formato da due vettori \(\vec{a}\) e \(\vec{b}\) è: \(\left| \vec{a} \times \vec{b}\right|\)
\end{itemize}

\subsubsection*{Area di una superficie parametrica}
L'area di una superficie parametrica \(p\) è l'integrale su \(D\) dell'elemento d'area:
\[\text{Area(p)} := \int_D \left|\vec{p_u} (u,v) \times \vec{p_v} (u,v)\right| \, dx \, dy\]

\subsubsection*{Elementi d'area notevoli}
\begin{itemize}
	\item[-] elemento d'area di una superficie cartesiana \(p(x,y)\): \(\qquad \left| \vec{p_x} \times \vec{p_y} \right| = \sqrt{1 + \left| \nab f(x,y)\right|^2}\)
	\item[-] elemento d'area di una superficie sferica \(p(\phi, \theta)\): \(\qquad \quad \;\;\:\! \left| \vec{p_\phi} \times \vec{p_\theta} \right| = R^2 \sin \phi\)
\end{itemize}

\newpage

\subsection{Area di un trapezoide di una funzione sopra ad una curva}
\subsubsection*{Trapeziode di una funzione su una curva}
Definita \(r: [a,b] \to \R_x \times \R_y, \; r(t) = (r_1(t),r_2(t))\) una curva \(C^1\) sul piano \(x,y\) e \(h : r([a,b]) \to [0,+\infty[_z\)
una funzione continua positiva che associa ad ogni punto della curva un certo valore lungo l'asse \(z\), si definisce il trapezoide
definito come il sostegno della superficie \(p\) compresa tra la curva \(r\) e il sostegno della funzione \(h\):
\begin{align*}
	\text{Trap}_r(h) &:= \left\{(r(t),z) : t \in [a,b], 0 \leq z \leq h(r(t))\right\} \\
	p(t,z) &= (r_1(t), r_2(t), z) \qquad (t,z) \in D, \; D = \left\{(t,z) : t \in [a,b], 0 \leq z \leq h(r(t))\right\}
\end{align*}

\subsubsection*{Area del trapezioide}
L'area del trapezoide è definita come l'integrale della funzione \(h\) sulla curva \(r\):
\[\text{Area}(\text{Trap}_r(h)) := \int_r h \; ds = \int_a^b h(r(t)) \left|r'(t)\right| \, dt\]

\subsubsection*{Calcolo dell'elemento d'area della superficie del trapeziode}
Se si parte da \(p(t,z) = (r_1(t), r_2(t), z)\) e si calcola l'elemento d'area \(\left|p_t \times p_z\right| = \sqrt{{r_1'}^2(t) + {r_2'}^2(t)} = \left|r'(t)\right|\),
per cui per definizione di area di una superficie è:
\[\int_D \left|p_t \times p_z\right| \, dt \, dz = \int_D \left|r'(t)\right| \, dt \, dz = \int_a^b \int_0^{h(r(t))} \left|r'(t)\right| \, dz \, dt = \int_a^b h(r(t)) \left|r'(t)\right| \, dt\]

\subsection{Area di una superficie di rotazione}
\subsubsection*{Superficie di rotazione}
Data una curva \(r(t) = (r_1(t), r_2(t))\) sul piano \(xz\), la superficie di rotazione ottenuta facendo ruotare la curva attorno
all'asse \(z\) è:
\[p(t, \theta) = (r_1(t) \cos \theta, r_1(t) \sin \theta, r_2(t)) \qquad t \in [a,b], \;\; \theta \in [\theta_1, \theta_2]\]

\subsubsection*{Elemento d'area}
L'elemento d'area della superficie di rotazione è: \(\left|p_t \times p_\theta\right| = r_1(t) \left|r'(t)\right|\)
\begin{align*}
	\left|p_t \times p_\theta\right|^2 &= (r_1'(t) \cos \theta, \; r_1'(t) \sin \theta, \; r_2'(t)) \times (-r_1(t) \sin \theta, \; r_1(t) \cos \theta, \; 0) \\
	&= (r_1'(t) \; r_1(t) \cos^2 \theta + r_1'(t) \; r_1(t) \sin^2 \theta)^2 + (-r_1(t) \; r_2'(t) \cos \theta)^2 + (-r_1(t) \; r_2'(t) \sin \theta)^2 \\
	&= (r_1(t) \; r_1'(t))^2 + (r_1(t) \; r_2'(t))^2  = {r_1}^2(t) \; (r_1'(t) + r_2'(t))^2 \\
	&= {r_1}^2(t) \left|r'(t)\right|^2 \\[5pt]
	\left|p_t \times p_\theta\right| &= r_1(t) \left|r'(t)\right|
\end{align*}

\subsubsection*{Area della superficie di rotazione}
L'area della superficie di rotazione è:
\begin{align*}
	\text{Area}(p) &= \int_D \left|p_t \times p_\theta\right| \, dt \, d\theta = \int_D r_1(t) \left|r'(t)\right| \, dt \, d\theta = \left(\int_{\theta_1}^{\theta_2} \; d\theta\right) \cdot \left( \int_a^b r_1(t) \left|r'(t)\right| \, dt \right) \\
	&= (\theta_2 - \theta_1) \cdot \int_a^b r_1(t) \left|r'(t)\right| \, dt
\end{align*}

\subsubsection*{Teorema di Pappo - Guldino}
Indicata con \(x_r\) la distanza tra il baricentro di r dall'asse di rotazione e Lunghezza\((r)\) la lunghezza della curva che viene
fatta ruotare per generare \(p\):
\[\text{Area}(p) = (\theta_2 - \theta_1) \cdot x_r \cdot \text{Lunghezza}(r)\]
\begin{align*}
	\text{osservando che} \; &1. \quad \int_a^b f(r_1(t),r_2(t)) \left|r'(t)\right| \, dt = \int_r f \; ds \\
	&2. \quad \int_a^b r_1(t) \left|r'(t)\right| \, dt = \int_r x \; ds \;\; \text{per} \; r_1(t) = f_x(t) \\
	&3. \quad x_\text{baricentro} = \frac{\int_r x \; ds}{\text{Lunghezza}} \quad \Rightarrow \quad \int_r x \; ds = x_\text{baricentro} \cdot \text{Lunghezza} \\
	\text{si ottiene che} \; &(\theta_2 - \theta_1) \cdot \int_a^b r_1(t) \left|r'(t)\right| \, dt = (\theta_2 - \theta_1) \cdot \int_r x \; ds = (\theta_2 - \theta_1) \cdot x_r \cdot \text{Lunghezza}(r)
\end{align*}

\subsubsection*{Superficie di rotazione generata da una curva cartesiana}
Sia \(z = h(x)\) una curva cartesiana, la superficie ottenuta ruotando \(h\) attorno all'asse \(z\) è una superficie cartesiana \(z = h(\sqrt{x^2+y^2})\)
con \(\sqrt{x^2+y^2}\) la distanza del punto \((x,y)\) rispetto all'asse \(z\) e con elemento d'area \(\sqrt{ 1 + \left| \nab \; h \left(\sqrt{x^2 + y^2}\right)\right|}\)
e area \(\displaystyle \int_D \sqrt{ 1 + \left| \nab \; h \left(\sqrt{x^2 + y^2}\right)\right|} \; dx \, dy\)

\subsection{Integrale superficiale}
\subsubsection*{Definizione}
L'integrale di una funzione \(\mu(x,y,z)\) su una superficie \(p\) è:
\[\int_p \mu \; d\sigma_p = \int_p \mu(x,y,z) \; d\sigma_p= \int_D \mu(p(u,v)) \left|p_u \times p_v\right| \, du \, dv\]

\subsubsection*{Invarianza dell'integrale superficiale}
Date due superfici equivalenti \(p_1\) e \(p_2\) e \(f\) una funzione continua, allora \(\displaystyle \int_{p_1} f \; d\sigma_{p_1} = \int_{p_2} f \; d\sigma_{p_2}\)

\newpage


\section{Teoremi della divergenza e di Green - Stokes}
\subsection{Flusso di un campo}
Dato un cammino \(r\) e un campo \(F\) continuo, il flusso di \(F\) attraverso \(r\) è:
\[\text{Flusso} := \int_r \det \left(\begin{matrix} F_1 & dx \\ F_2 & dy \end{matrix}\right)\]
\begin{align*}
	\text{Flusso} := \; &1. \;\int_a^b \det \left(\begin{matrix} F_1(r(t)) & r_1'(t) \\[3pt] F_2(r(t)) & r_2'(t) \end{matrix}\right) \, dt = \int_a^b F_1(r(t)) \, r_2'(t) - F_2(r(t)) r_1'(t) \; dt \\
	&2. \; \int_r \det \left(\begin{matrix} F_1(x,y) & dx \\[3pt] F_2(x,y) & dy \end{matrix}\right) = \int_r F_1(x,y) \; dy - F_2(x,y) \; dx \\
	&3. \; \int_r \vec{F} \cdot \vec{N}_r \; ds, \quad \text{con} \; \vec{N}_\text{normale a \(p\)} = \frac{1}{\left|r'(t)\right|} \left(\begin{matrix} r_1'(t) \\ -r_2'(t) \end{matrix}\right)
\end{align*}
\begin{align*}
	\text{Dimostrazione punto 3:} \qquad &\int_a^b F_1(r(t)) \, r_2'(t) - F_2(r(t)) r_1'(t) \; dt = \int_a^b \vec{F}(r(t)) \cdot \left(\begin{matrix} r_1'(t) \\ -r_2'(t) \end{matrix}\right) \, dt = \\
	= &\int_a^b \vec{F}(r(t)) \cdot \frac{1}{\left|r'(t)\right|} \left(\begin{matrix} r_1'(t) \\ -r_2'(t) \end{matrix}\right) \left|r'(t)\right| \, dt = \int_a^b \vec{F}(r(t)) \cdot \vec{N}(r(t)) \left|r'(t)\right| \, dt = \\
	= &\int_r \vec{F} \cdot \vec{N}_r \; ds
\end{align*}

\subsection{Domini Stokiani}
\subsubsection*{Definizione}
Un dominio \(D \subset \Rd\) si dice stokiano se:
\begin{itemize}
	\item attorno ad ogni punto del bordo di \(D\) c'è una separazione tra interno ed esterno di \(D\), ovvero non esistono punti
	isolati o cammini "all'interno" di \(D\) che non fanno parte del dominio.
	\item il bordo \(\partial D\) è l'unione di un numero finito di sostegni di curve chiuse regolari semplici \(r_1, \dots r_n\)
	tali che il loro orientamento fa sì che il vettore normale \(\vec{N}\) punta sempre all'esterno del dominio; il bordo viene
	detto bordo positivamente orientato e si indica con \(\partial^+D\)
\end{itemize}

\subsubsection*{Integrali su bordi positivamente orientati}
Dato \(D\) aperto stokiano con \(\partial^+D\) bordo positivamente orientato e \(F = (F_1,F_2)\) campo continuo, l'integrale di \(G\)
lungo il bordo di \(D\) è:
\[\text{Circuitazione:} \quad \int_{\partial^+D} F_1 \, dx + F_2 \, dy = \int_{\partial^+D} \vec{F} \cdot \vec{T} \; ds := \int_{r_1} \vec{F} \cdot \vec{T} \; ds + \dots + \int_{r_n} \vec{F} \cdot \vec{T} \; ds\]

\subsubsection*{Flusso di campi attraverso domini stokiani}
Dato \(D\) aperto stokiano con \(\partial^+D\) bordo positivamente orientato e \(F = (F_1,f_2)\) campo continuo, il flusso di \(F\)
lungo il bordo di \(D\) è:
\[\text{Flusso:} \quad \int_{\partial^+D} \vec{F} \cdot \vec{N} \; ds := \int_{\partial^+D} F_1(x,y) \; dy - F_2(x,y) \; dx = \int_{\partial^+D} \det \left(\begin{matrix} F_1 & dx \\ F_2 & dy \end{matrix}\right)\]

\newpage

\subsection{Divergenza}
\subsubsection*{Definizione}
Dato un campo \(F = (F_1, F_2)\) di classe \(C^1\) su aperto \(D\), la divergenza di \(F\) è:
\[\diver F(x,y) := \partial_x F_1(x,y) + \partial_y F_2(x,y) = \det \left(\begin{matrix} \partial_x & -F_2 \\ \partial_y & F_1 \end{matrix}\right)\]

\subsubsection*{Teorema della divergenza}
Il flusso di un campo \(F = (F_1, F_2)\) di classe \(C^1\) su aperto stokiano \(D\) è:
\[\int_{\partial^+D} \vec{F} \cdot \vec{N} \; ds = \int_D \diver F \; dx \, dy\]

\subsubsection*{Interpretazione}
I punti in cui \(\diver F(x,y) < 0\) sono punti di compressione, ovvero dove il flusso di entrata è maggiore di quello in uscita.
Quelli in cui \(\diver F(x,y) > 0\) sono punti di espansione, dove il flusso in uscita è maggiore di quello in entrata. Infine
dove \(\diver F(x,y) = 0\) il flusso in entrata è pari a quello in uscita.

\subsubsection*{Dimostrazione del Principio di Archimede}
Ho un solido \(D\) immmerso in un fluido con densità costante \(\mu\). Il fluido agisce con una forza data dalla formula
\(\mu g y \vec{N}\) sul bordo di \(D\). Si vuole calcolare la risultante di tutte le forze agenti sul bordo di \(D\). \\
Il vettore \(\vec{N}\) è definito come segue \(\vec{N} := (N_1, N_2)\).
\begin{align*}
	\int_{\partial^+D} \mu g y \vec{N} \; ds &:= \left(\begin{matrix}
		\displaystyle \int_{\partial^+D} \mu g y N_1 \; ds \\[10pt]
		\displaystyle \int_{\partial^+D} \mu g y N_2 \; ds
	\end{matrix}\right) = \left(\begin{matrix}
		\displaystyle \int_{\partial^+D} (\mu g y, \; 0) \cdot \vec{N} \; ds \\[10pt]
		\displaystyle \int_{\partial^+D} (0, \; \mu g y) \cdot \vec{N} \; ds
	\end{matrix}\right) = \left(\begin{matrix}
		\displaystyle \int_D \diver (\mu g y, \; 0) \; dx \, dy \\[10pt]
		\displaystyle \int_D \diver (0, \; \mu g y) \; dx \, dy
	\end{matrix}\right) = \\
	&= \left(\begin{matrix}
		\displaystyle \int_D 0 \; dx \, dy \\[10pt]
		\displaystyle \int_D \mu g \; dx \, dy
	\end{matrix}\right) = \left(\begin{matrix}
		0 \\[10pt]
		\mu g \text{Area}(D)
	\end{matrix}\right) = \left(\begin{matrix}
		0 \\[10pt]
		\text{Peso}(D)
	\end{matrix}\right)
\end{align*}
Si ottiene che la risultante delle forze agenti sul bordo è un vettore che ha direzione concorde all'asse \(y\) (è rivolto
verticalmente) e modulo pari al peso dell'acqua che occuperebbe il "volume" di \(D\).

\subsection{Formula di Green}
\subsubsection*{Definizione}
Dato un campo \(F = (F_1, F_2)\) di classe \(C^1\) su aperto \(D\), l'integrale di \(F\) lungo il bordo di \(D\) vale:
\[\int_{\partial^+D} F_1(x,y) \; dx + F_2(x,y) \; dy = \int_D \partial_x F_2(x,y) - \partial_y F_1(x,y) \; dx \, dy = \int_D \rot(F) \cdot (0,0,1) \; dx \, dy\]
\[\text{Circuitazione:} \quad \int_{\partial^+D} F_1 \, dx + F_2 \, dy = \int_D \det \left(\begin{matrix} \partial_x & F_1 \\ \partial_y & F_2 \end{matrix}\right) \, dx \, dy\]

\subsubsection*{Calcolo delle aree con la formula di Green}
Sia \(D\) un dominio stokiano, allora:
\[\text{Area}(D) = \int_{\partial^+D} x \; dy = \int_{\partial^+D} -y \; dx = \frac{1}{2} \int_{\partial^+D} x \; dy - y \; dx =
\frac{1}{2} \int_{\partial^+D} \det \left(\begin{matrix} x & dx \\ y & dy \end{matrix}\right)\]

\newpage


\section{Equazioni differenziali}
\subsection{Problema di Cauchy ed esistenza e unicità della soluzione}
\subsubsection*{Soluzione di un'equazione differenziale}
Data \(f:D \subset \Rd \to \R\), una soluzione dell'equazione \(y' = f(t,y)\) è una funzione \(y I \subset \R \to \R\) \(C^1\) tale che
\((t,y(t)) \in D\) e \(y'(t) = f(t, y(t))\). L'insieme delle soluzioni è detta soluzione generale dell'equazione differenziale.

\subsubsection*{Problema di Cauchy}
Data un'equazione differenziale, il problema di Cauchy di un'equazione differenziale è:
\(\begin{cases}
	y' = f(t,y) \\
	y(t_0) = y_0
\end{cases}\)
La soluzione generale che risolve anche il problema di Cauchy è detta soluzione del problema di Cauchy,

\subsubsection*{Esistenza e unicità della soluzione locale}
Sia \(D = I \times \R\), \(f: D \to \R\) funzione continua, se valgono
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\partial_y f(t,y)\) esiste continua
	\item[-] \(\partial_y f(t,y)\) limitata su ogni striscia \([\alpha', \beta'] \times \R\), con \([\alpha', \beta'] \subset I\)
\end{itemize}
Allora per ogni \((t_0,y_0) \in D\) esiste ed è unica la soluzione definita su tutto \(I\) del problema di Cauchy.

\subsection{Equazione differenziale a variabili separabili}
\subsubsection*{Definizione}
Un'equazione differenziale \(y' = f(t,y)\) è detta a variabili separabili se \(f(t,y) = g(t) \, h(y)\)

\subsubsection*{Soluzione caso h(y) = 0}
Dato \(\begin{cases} y' = 0 \\ y(t_0) = y_0 \end{cases}\), la soluzione è \(y(t) = y_0\)

\subsubsection*{Soluzione caso generale}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] si isolano le \(y\) da una parte e le \(t\) dall'altra parte: \(\frac{y'(t)}{h(y(t))} = g(t)\)
	\item[2.] si integra da entrambe le parti: \(\int \frac{y'(t)}{h(y(t))} dt = \int \frac{1}{h(y)} dy := H(y)\) e \(\int g(t) dt := G(t)\)
	\item[3.] si uniscono le soluzioni invertendo \(H(y)\) per ottenere \(y\): \(y(t) = H^{-1}(G(t)) + C\)
	\item[4.] si determina \(C\) in base alla seconda condizione del sistema di Cauchy
	\item[5.] si determina l'intervallo massimo di validità della soluzione
\end{itemize}

\subsection{Equazioni lineari del primo ordine}
\subsubsection*{Definizione}
Un'equazione differenziale \(y' = f(t,y)\) è detta lineare del primo ordine se \(f(t,y) = a(t)y + b(t)\)

\subsubsection*{Soluzione}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] si osserva che \((y'-a(t)y)e^{-A(t)} = (y e^{-A(t)})'\) con \(A' = a\)
	\item[2.] si sceglie una primitiva \(A\) tale che \(A' = a\), si sposta \(a(t)y\) dall'altra parte dell'uguaglianza e si
	moltiplica per \(e^{-A(t)}\) in modo da avere \((y'-a(t)y) \, e^{-A(t)} = b(t) \, e^{-A(t)} \Rightarrow (y e^{-A(t)})' = b(t) e^{-A(t)}\)
	\item[3.] si integra da tutte e due le parti e divido per \(e^{-A(t)}\) per ricondurmi alla forma \(y = \dots\)
	\item[4.] si determina \(C\) in base alla seconda condizione del sistema di Cauchy
	\item[5.] si determina l'intervallo massimo di validità della soluzione
\end{itemize}

\newpage

\section{Combinatoria}
\subsection{Cardinalità}
\subsubsection*{Notazione}
La cardinalità di un'insieme \(X\) finito è il numero dei suoi elementi distinti e si indica con \(|X|\) o \(\#X\)

\subsubsection*{Prioprietà}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] se \(A\) e \(B\) sono disgiunti, ovvero \(A \cap B = \varnothing\), allora \(|A \cup B| = |A| + |B|\)
	\item[2.] \(|A\cup B| = |A| + |B| - |A \cap B|\)
	\item[3.] \(|A \times B| = |A| \times |B|\)
	\item[4.] la cardinalità del complementare è \(|A^c| = |X|-|A|\)
	\item[5.] principio di inclusione/esclusione: \\
	\(|A \cup B \cup C| = |A| + |B| + |C| - |A \cap B| - |B \cap C| - |A \cap C| + |A \cap B \cap C|\)
	\item[6.] principio di biiezione: \\
	due insiemi hanno la stessa cardinalità se e solo se sono in corrispondenza biunivoca
\end{itemize}

\subsection{Sequenze}
Una \(k\)-sequenza di \(I_n\) è una \(k\)-upla ordinata \((a_1, \dots a_k)\) di elementi non necessariamente distinti di \(I_n\),
ovvero è un elemento del prodotto cartesiano \(\underbrace{I_n \times \dots \times I_n}_{k \; \text{volte}}\)

\subsubsection*{Sequenze con ripetizione}
Una sequenza è detta con ripetizione se non necessariamente tutti gli elementi sono distinti. Il numero delle \(k\)-sequenze con
ripetizione di \(I_n\) è \(n^k\)

\subsubsection*{Sequenze senza ripetizione}
Una sequenza è detta senza ripetizione gli elementi sono tutti distinti. Il numero delle \(k\)-sequenze senza ripetizione di \(I_n\)
è \(\displaystyle \frac{n!}{(n-k)!}\) per \(k \leq n\).

\subsection{Permutazioni}
Le permutazioni sono le \(k\)-sequenze ottenute riordinando gli elementi di una qualunque \(k\)-sequenza di \(I_n\).

\subsubsection*{Permutazioni di sequenze senza ripetizione}
Il numero di permutazioni di \(k\)-sequenze senza ripetizione è \(k!\). Si nota che esiste una corrispondenza biunivoca tra le
\(k\)-sequenze senza ripetizione di \(I_k\) e le permutazioni di esse.

\subsubsection*{Permutazioni di sequenze con ripetizione - anagrammi}
Il numero di permutazioni di \(k\)-sequenze con ripetizione è \(\frac{k!}{m_1 \!! \, m_2\!! \dots m_k\!!}\) con \(m_1, m_2\dots\)
il numero di ripetizioni/molteplicità degli elementi della sequenza. Si nota che se \(m_1 = m_2 = \dots = m_k = 1\), allora si
ricade nel caso delle permutazioni senza ripetizione.

\subsection{Spartizioni}
Una \(n\)-spartizione è una \(n\)-upla ordinata \(C_1, \dots C_n\) di sottoinsiemi a due a due disgiuti di \(I_k\) eventualmente
anche vuoti tali che \(C_1 \cup \dots \cup C_n = I_k\). Per la corrispondenza biiettiva tra spartizioni e sequenze con ripetizione
illustrata sotto, il numero di \(n\)-sparizioni di \(I_k\) è pari al numero delle \(k\)-sequenze con ripetizione di \(I_n\), ovvero
è \(n^k\).

\subsubsection*{Corrispondenza biunivoca tra spartizioni e sequenze con ripetizione}
Si nota che esiste una corrispondenza biunivoca tra le \(n\)-spartizioni di \(I_k\) e le \(k\)-sequenze con ripetizione di \(I_n\).
Data una \(n\)-spartizione di \(I_k\), è possibile indicare il sottoinsieme di ciascun elemento di \(I_k\) associandogli un numero
di \(I_n\), ottenendo così una \(k\)-sequenza con ripetizione di \(I_n\).

\subsection{Sottoinsiemi}
Un \(k\)-sottoinsieme di \(I_n\) è un sottoinsieme formato da \(k\) elementi distinti di \(I_n\). Il numero di \(k\)-sottoinsiemi
di \(I_n\) si indica con \(\displaystyle C(n,k) = \binom{n}{k} = \frac{n!}{k! (n-k)!}\)

\subsubsection*{Corrispondenza biunivoca tra sottoinsiemi, spartizioni e sequenze}
Si osserva che esiste una corrispondenza biunivoca tra le \(2\)-spartizioni di \(I_n\) e i sottoinsiemi di \(I_n\). Infatti, data
una sparzione \((C_1,C_2 = C_1^c)\) è possibile ottenere un sottoinsieme \(A \subseteq I_n\) tale che \(A = C_1\). Per cui il numero
di sottoinsiemi di \(I_n\) è pari al numero delle \(2\)-spartizioni di \(I_n\). Siccome le \(2\)-spartizioni di \(I_n\) sono in 
corrispondenza biunivoca con le \(n\)-sequenze di \(I_2\), allora: \\
\(|\text{sottoinsiemi di } I_n| = |2\text{-spartizioni di } I_n| = |n\text{-sequenze di } I_2| = n^2\)

\subsubsection*{Corrispondenza tra i sottoinsiemi e le sequenze senza ripetizione}
Si osserva che ad ogni \(k\)-sottoinsieme di \(I_n\) corrispondono \(k!\) \(k\)-sequenze senza ripetizione di \(I_n\). Per cui

\subsection{Principio di moltiplicazione}
Supponiamo che gli elementi di un insieme \(X\) possano essere individuati con una procedura di \(n\) fasi dove:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] la prima fase ha \(m_1\) esiti possibili
	\item[2.] la seconda fase ha \(m_2\) esiti possibili
	\item[\dots] \dots
	\item[n.] la \(n\)-esima fase ha \(m_n\) esiti possibili
	\item[!] dato un elemento è possibile determinare univocamente la fase in cui è estratto
\end{itemize}
Allora \(|X| = m_1 \times m_2 \times \dots \times m_n\)

\subsection{Principio di divisione}
Siano \(X\), \(Y\) due insiemi finiti, se ad ogni elemento \(y\) di \(Y\) sono associati \(m\) elementi \(x_1, \dots x_m\) di \(X\),
allora vale che \(|X| = |Y| \times m\).

\subsection*{Fattoriale e formula di Stirling}
\[n! = \begin{cases}
	n \times (n-1)! & \text{per} \; n \geq 1 \\
	1 & \text{per} \; n = 0
\end{cases} \qquad \qquad n! \approx \sqrt{2 \pi n} \left(\frac{n}{e}\right)^n \;\; \text{per } n \to +\infty\]

\subsection*{Binomiale}
\[\binom{n}{k} = \frac{n!}{k! (n-k)!}\]
\begin{itemize}
	\item[1.] \(\displaystyle \binom{n}{0} = \binom{n}{n} = 1 \quad \binom{n}{1} = n \qquad \qquad \qquad \qquad \qquad \text{2.} \; \binom{n}{0} + \dots + \binom{n}{n} = n^2\)
	\item[3.] \(\displaystyle \binom{n}{k} = \binom{n}{n-k} \qquad \qquad \qquad\qquad \qquad \qquad \qquad \quad \text{4.} \;\binom{n-1}{k-1} + \binom{n-1}{k} = \binom{n}{k}\)
	\item[5.] \(\displaystyle (x+y)^n = \sum_{j=0}^n \binom{n}{j} x^j y^{n-j}\)
\end{itemize}

\newpage


\section{Probabilità}
\subsection{Definizione assiomatica di probabilità}
\subsubsection*{Definizione}
La probabilità su uno spazio campionario \(\Omega\) è la funzione
\[\displaystyle P:\mathcal{P}(\Omega) \to [0,1], \quad P(A) = \frac{|A|}{|\Omega|}\]
tale che:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(0 \leq P(A)\leq 1\)
	\item[-] \(P(0) = 0, \quad P(\Omega) = 1\)
	\item[-] dati \(A_1, \dots A_n \subseteq \Omega\) una famiglia numerabile di insiemi a due a due disgiunti, allora vale \\
	\(\displaystyle P\left(\bigcup_{i=1}^{+\infty} A_i\right) = \lim_{n \to +\infty} (P(A_1) + \dots + P(A_n))\)
\end{itemize}

\subsubsection*{Proprietà}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] \(P(0) = 0, \quad P(\Omega) = 1\)
	\item[2.] \(P(A^c) = 1-P(A)\)
	\item[3.1] se \(A\) e \(B\) sono disgiunti \(A \cap B = \varnothing\), allora \(P(A \cup B) = P(A) + P(B)\)
	\item[3.2] se \(A_1, \dots A_n\) a due a due disgiunti, \(P(A_1 \cup \dots \cup A_n) = P(A_1) + \dots + P(A_n)\)
	\item[4.1] se \(E \subseteq F \subseteq \Omega\), allora \(P(E) \leq P(F)\)
	\item[4.2] se \(E \subseteq F \subseteq \Omega\), allora \(P(F \backslash E) = P(F) - P(E)\)
\end{itemize}

\subsubsection*{Principio di inclusione / esclusione}
\begin{itemize}
	\item[1.] \(P(A_1 \cup A_2) = P(A_1) + P(A_2) - P(A_1 \cap A_2)\)
	\item[2.] \(\displaystyle P(A_1 \cup A_2 \cup A_3) = \sum_{i=1}^3 P(A_i) - \sum_{1 \leq i < j \leq 3} P(A_i \cap A_j) + P(A_1 \cap A_2 \cap A_3)\)
	\item[3.] \(\displaystyle P(A_1 \cup A_2 \cup A_3 \cup A_4) = \\ \sum_{i=1}^4 P(A_i) - \sum_{1 \leq i < j \leq 4} P(A_i \cap A_j) + \sum_{1 \leq i < j < k \leq 4} P(A_i \cap A_j \cap A_k) - P(A_1 \cap A_2 \cap A_3 \cap A_4).
\)
\end{itemize}

\subsection{Probabilità uniforme}
\subsubsection*{Definizione}
La probabilità è uniforme su \(\Omega\) se per ogni elemento \(\omega\) di \(\Omega\) si ha: \[\displaystyle P(\omega) = \frac{1}{|\Omega|}\]

\subsubsection*{Considerazioni per la risoluzione di esercizi}
In una esrazione senza reimmissione, la probabilità di estrarre una pallina alla prima estrazione è uguale alla probabilità di
estrarre la medesima pallina all'ultima estrazione.

\newpage

\subsection{Probabilità su insiemi finiti e infiniti numerabili e non numerabili}
\subsubsection*{Probabilità su un insieme finito}
Dato un insieme finito \(\Omega = \{x_1, \dots x_m\}\) e definite \(p_1, \dots p_m \geq 0\) tali che \(p_1 + \dots + p_m \geq 0\) e \\
\(P(x_i) = p_i \;\; \forall i \in [1,m]\), la probabilità su un insieme \(A \subseteq \Omega\) è:
\[P(A) = \sum_{k \; : \; x_k \in A} p_k\]

\subsubsection*{Probabilità in un insieme infinito numerabile}
Dato un insieme numerabile \(\Omega = \{x_1,x_2 \dots\}\) e definite \(p_1,p_2 \geq 0\) tali che \(\sum_{i=1}^{+\infty} p_i = 1\) e \\
\(P(x_i) = p_i \;\; \forall i \in \mathbb{N}\) la probabilità su un insieme \(A \subseteq \Omega\) è:
\[P(A) = \sum_{k \; : \; x_k \in A} p_k\]
Osservazione: un insieme è numerabile può essere messo in corrispondenza biunivoca con \(\mathbb{N}\)

\subsubsection*{Probabilità in un insieme infinito non numerabile - intervalli/aree}
In un insieme infinito non numerabile, la probabilità di un singolo evento è \(P(x_i) = 0\), la probabilità su un intervallo \([a,b]\)
è: \[P([a,b]) = P(]a,b[) = b-a\]

\subsection{Continuità della probabilità}
\subsubsection*{Probabilità di unione crescente}
Siano \(E_1 \subseteq E_2 \subseteq \dots\) successione di sottoinsiemi di uno spazio campionario \(\Omega\), allora:
\[P(\bigcup_{i=1}^{+\infty} E_i) = \lim_{i \to +\infty} P(E_i)\]

\subsubsection*{Probabilità di unione decrescente}
Siano \(E_1 \supseteq E_2 \supseteq \dots\) successione di sottoinsiemi di uno spazio campionario \(\Omega\), allora:
\[P(\bigcap_{i=1}^{+\infty} E_i) = \lim_{i \to +\infty} P(E_i)\]

\subsubsection*{Complementare di intersezione e unione}
\[\left(\bigcap_{n \in \mathbb{N}} A_n\right)^c = \bigcup_{n \in \mathbb{N}} (A_n)^c \qquad \qquad \qquad \left(\bigcup_{n \in \mathbb{N}} A_n\right)^c = \bigcap_{n \in \mathbb{N}} (A_n)^c\]

\newpage

\subsection{Probabilità condizionata e indipendenza di eventi}
\subsubsection*{Definizione}
La probabilità condizionata di \(E\) ad \(F\), con \(E,F \subseteq \Omega\) e \(P(F) \neq 0\) è:
\[P(E | F) = \frac{P(E \cap F)}{P(F)} = \frac{P(EF)}{P(F)}\]
Si osserva che se \(P\) è uniforme su \(\Omega\), allora lo è anche \(P(\;.\;|F)\) su \(F\)

\subsubsection*{Probabilità di una intersezione}
\[P(E \cap F) = P(E | F) \cdot P(F) = P(F | E) \cdot P(E)\]

\subsubsection*{Formula del prodotto}
\[P(A_1 A_2 \dots A_n) = P(A_1) \cdot P(A_2 | A_1) \cdot P(A_3 | A_1 A_2) \; \dots \; P(A_n | A_1 A_2 \dots A_{n-1})\]

\subsubsection*{Formula di inversione}
\[P(F | E) = \frac{P(E|F) \cdot P(F)}{P(E)}\]

\subsubsection*{Formula della partizione o della probabilità totale}
Sia \(F_1, F_2 \dots F_n\) una \(n\)-partizione di \(\Omega\):
\[P(E) = P(E|F_1) \cdot P(F_1) + P(E|F_2) \cdot P(F_2) + \dots + P(E|F_n) \cdot P(F_n)\]

\subsubsection*{Formula di Bayes}
Sia \(F_1, F_2 \dots F_n\) una \(n\)-partizione di \(\Omega\):
\[P(F_1 | E) = \frac{P(E | F_1) \cdot P(F_1)}{P(E|F_1) \cdot P(F_1) + P(E|F_2) \cdot P(F_2) + \dots + P(E|F_n) \cdot P(F_n)}\]

\subsubsection*{Indipendenza di due eventi}
\[A \; \text{e} \; B \; \text{indipendenti} \quad \Leftrightarrow \quad P(A \cap B) = P(A) \cdot P(B) \quad \Leftrightarrow \quad P(A|B) = P(A), \; P(B|A) = P(B)\]

\subsubsection*{Indipendenza di una famiglia di eventi}
Indipendenza di eventi a due a due non implica l'indipendenza di tutti gli eventi:
\[P(A|B) = P(A), \; P(B|C) = P(B), \; P(C|A) = P(C) \;\; \nRightarrow \;\; A, B, C \; \text{eventi indipendenti}\]
\[(A_1, \dots A_n) \; \text{indipendenti} \quad \Leftrightarrow \quad P(A_1 A_2 \dots A_n) = P(A_1) \cdot P(A_2) \dots P(A_n)\]

\subsubsection*{Indipendenza condizionata}
\[A|F \; \text{e} \; B|F \; \text{indipendenti} \quad \Leftrightarrow \quad P(AB|F) = P(A|F) \cdot P(B|F)\]

\newpage

\subsection{Variabili aleatorie discrete}
\subsubsection*{Definizione}
Una variabile aleatoria definita su uno spazio campionario \(\Omega\) è una funzione \(X: \Omega \to \R\). \\
Una variabile aleatoria è detta discreta se \(\text{Im}(X)\) è un insieme finito numerabile \(\{x_1, x_2, \dots\}\)

\subsubsection*{Denistà di una variabile aleatoria discreta}
La densità discreta di una variabile aletoria discreta è una funzione \[p_x : \R \to [0,1], \quad x \mapsto P(X = x)\]

\subsubsection*{Proprietà della densità discreta}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\displaystyle \sum_k p_x(x_k) = 1\)
	\item[-] \(\displaystyle P(X \in A) = \sum_{k : x_k \in A} p_x(x_k)\)
\end{itemize}

\subsubsection*{Distribuzione di una variabile aleatoria}
La distribuzione discreta di una variabile aletoria discreta è una funzione \[F_X : \R \to [0,1], \quad x \mapsto P(X \leq x)\]

\subsubsection*{Proprietà della distribuzione discreta}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(F_X\) è crescente
	\item[-] \(\displaystyle \lim_{x \to -\infty} F_X(x) = 0\) e \(\displaystyle \lim_{x \to +\infty} F_X(x) = 1\)
	\item[-] \(F_X\) è continua a destra: \(\displaystyle \lim_{x \to a^+} F_X(x) = F_X(a)\)
	\item[-] \(P(X < a) = F_X(a^-) := \lim_{x \to a^-} F_X(x)\)
	\item[-] \(P(X = a) = F_X(a) - F_X(a^-)\)
\end{itemize}
Osservazione: due variabili aleatorie \(X\) e \(Y\) con stesse distribuzioni \(F_X = F_Y\) assumono i valori con uguali probabilità:
\(P(X \in A) = P(Y \in A)\)

\subsection{Variabile di Bernoulli}
\subsubsection*{Definizione}
La variabile di Bernoulli con parametro \(p\) si indica con \(X \sim Be(p)\) e possiede le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\text{Im}(X) = \{0,1\}\)
	\item[-] \(p_x(1) = p, \;\; p_x(0) = 1-p\)
\end{itemize}

\subsection{Variabile binomiale}
\subsubsection*{Definizione}
La variabile binomiale con parametri \(n\) e \(p\) si indica con \(X \sim B(n,p)\) e possiede le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\text{Im}(X) = \{0,1,\dots n\}\)
	\item[-] \(\displaystyle p_x(k) = \binom{n}{k} \; p^{k} \; (1-p)^{n-k}\)
\end{itemize}

\subsubsection*{Binomiale come approssimazione di Bernoulli indipendenti}
Siano \(X_1, X_2, \dots X_n \sim Be(p)\) Bernoulli indipendenti di parametro \(p\) e \(X = X_1 + X_2 + \dots + X_n\), allora
\(X \sim B(n,k)\) è una binomiale di parametri \(n\), \(p\)

\subsection{Variabile geometrica}
\subsubsection*{Definizione}
La variabile geometrica con parametro \(p\) si indica con \(X \sim Ge(p)\) e possiede le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\text{Im}(X) = \mathbb{N}_{\geq 1}\)
	\item[-] \(p_x(1) = p \; (1-p)^{k-1}\)
\end{itemize}

\subsubsection*{Assenza di memoria di una variabile geometrica}
\[P(X > k + m \;| \;X > k) = P( X > m)\]

\subsection{Variabile di Poisson}
\subsubsection*{Definizione}
La variabile di Poisson con parametro \(\lambda\) si indica con \(X \sim Po(\lambda)\) e possiede le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\text{Im}(X) = \mathbb{N}\)
	\item[-] \(\displaystyle p_x(k) = e^{-\lambda} \; \frac{\lambda^k}{k!}\)
\end{itemize}

\subsubsection*{Poisson come approssimazione di una binomiale}
Siano \(X_n \sim B(n, \frac{\lambda}{n})\) e \(Y \sim Po(\lambda)\), allora \(\displaystyle \lim_{n \to \infty} P(X_n = k) = e^{-\lambda} \frac{\lambda^k}{k!} = P(Y = k)\) \\[5pt]
In pratica \(X \sim B(n,p)\) si può approssimare con \(Y \sim Po(\lambda)\) se \(np \leq 10, \;\; n \geq 20, \;\; p \leq 0.05\)

\subsubsection*{Somme di Poisson indipendenti}
Siano \(X \sim Po(\lambda)\) e \(Y \sim Po(\mu)\) indipendenti, allora \(X + Y \sim Po(\lambda + \mu)\)

\subsubsection*{Processo di Poisson}
Un processo di Poisson di intensità \(\lambda\) è una famiglia di variabili aleatorie \(\{X_t : t > 0\}\) con:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(X_t \sim Po(\lambda t)\)
	\item[-] \(X_{t+\tau} - X_t \sim Po(\lambda \tau)\)
	\item[-] \(X_{t_1} - X_{t_0}, \; X_{t_2} - X_{t_1}, \; \dots \; X_{t_n} - X_{t_{n-1}}\) sono indipendenti
\end{itemize}

\subsection{Variabile aleatoria uniforme}
\subsubsection*{Definizione}
La variabile uniforme definita su un intervallo \([a,b]\) si indica con \(X \sim U([a,b])\) e possiede le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\displaystyle P(X \in [c,d]) = \frac{d-c}{b-a}\)
	\item[-] \(\displaystyle f_X(x) = \begin{cases} \displaystyle \frac{1}{b-a} &\text{per} \; x \in ]a,b[ \\ 0 &\text{per} \; x < a \; \vee \; x > b \end{cases}\)
	\item[-] \(\displaystyle F_X(x) = \begin{cases} 0 &\text{per} \; x < a \\ \displaystyle \frac{x-a}{b-a} &\text{per} \; x \in [a,b[ \\ 1 &\text{per} \; x \geq b \end{cases}\)
\end{itemize}

\subsection{Variabile aleatoria esponenziale}
\subsubsection*{Definizione}
La variabile esponenziale con parametro \(\lambda\) si indica con \(X \sim Exp(\lambda)\) e possiede le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\text{Im}(X) = \mathbb{N}\)
	\item[-] \(F_X(t) = \begin{cases} 0 &\text{per} \; t < 0 \\ 1 - e^{-\lambda t} &\text{per} \; t \geq 0 \end{cases}\)
	\item[-] \(f_X(t) = \begin{cases} 0 &\text{per} \; t < 0 \\ \lambda e^{-\lambda t} &\text{per} \; t \geq 0 \end{cases}\)
\end{itemize}

\subsubsection*{Variabile esponenziale e processo di Poisson}
Dato un processo di Poisson di intensità \(\lambda\) si hanno le seguenti caratteristiche:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(T :=\) istante nel quale si verifica il primo fenomeno
	\item[-] \(X_t \sim Po(\lambda t)\) numero di fenomeni verificati fino all'istante \(t\)
	\item[1.] se \(t < 0\) allora \(F_T(t) = 0\)
	\item[2.] se \(t \geq 0\) allora \(\{T > t\} = \{X_t = 0\}\), per cui \(P(T > t) = P(X_t = 0) = e^{-\lambda t}\) 
\end{itemize}

\subsubsection*{Assenza di memoria della esponenziale}
\[P(T > t + s \;| \; t > s) = P(T > t)\]

\subsection{Variabile normale o Gaussiana}
\subsubsection*{Definizione variabile normale standard}
La variabile normale standard \(Z\) si indica con \(Z \sim N(0,1)\) e possiede le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\displaystyle f_Z(x) = \frac{1}{\sqrt{2 \pi}} \; e^{-x^2/2}\)
	\item[-] \(\displaystyle \int_{-\infty}^{+\infty} f_Z(x) \, dx = 1\)
	\item[-] \(\Phi(x) := F_Z(x), \quad \Phi(-x) = 1 - \Phi(x)\)
	\item[-] \(E[Z] = 0, \quad \var[Z] = 1\)
\end{itemize}

\subsubsection*{Definizione variabile normale o gaussiana}
La variabile normale o gaussiana \(X\) si indica con \(X \sim N(\mu,\sigma^2)\) e possiede le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(X := \mu + \sigma Z \qquad Z \sim N(0,1)\)
	\item[-] \(E[Z] = \mu, \quad \var[Z] = \sigma^2\)
	\item[-] \(\displaystyle f_Z(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \; e^{-\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}}\)
\end{itemize}
Per calcolare la distribuzione di \(X \sim N(\mu,\sigma^2)\) ci si riconduce alla distribuzione \(\Phi(x)\) della normale standard \(Z\)
attraverso la prima proprietà \(X = \mu + \sigma Z\): \(F_X(x) = P(\mu + \sigma Z \leq x) = \Phi(\frac{x - \mu}{\sigma})\)

\subsubsection*{Variabili normalizzate}
Una variabile aleatoria \(X\) si dice normalizzata se \(E[X] = 0\) e \(\var[X] = 1\). Per normalizzare una variabile non normalizzata
si deve calcolare: \[ \overline{X} := \frac{X - E[X]}{\sigma_X}\]
Una variabile normalizzata non coincide necessariamente con una variabile normale: vedere teorema del limite centrale per correlazione
tra variabili indipendenti identicamente distribuite e variabili normali.

\newpage

\subsection{Valore atteso varianza e covarianza di variabili discrete}
\subsubsection*{Valore atteso}
Sia \(X\) una variabile aleatoria discreta, il valore atteso (o media) di \(X\) è:
\[E[X] = := \sum_{i=1}^m x_i \, p_x(x_i) = \sum_{i=1}^m x_i \, P(X = x_i) = \sum_{x \in \text{Im}(X)} x \, P(X = x)\]
Il valore atteso si dice finito se \(\displaystyle \sum_{x \in \text{Im}(X)} |x_i| \, p_x(x_i) < +\infty\)

\subsubsection*{Algebra di valori attesi}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] combinazioni lineari: \(E[aX + b] := aE[x] + b\)
	\item[-] somma di variabili aleatorie: \(E[X_1 + X_2 + \dots + X_n] = E[X_1] + E[X_2] + \dots + E[X_n]\)
	\item[-] composte di una variabile: \(\displaystyle E[g \circ X] := E[g(x)] = \sum_{x \in \text{Im}(X)} g(x) \, p_x(x)\)
	\item[-] composte di più variabili: \(\displaystyle E[g(X,Y)] := \sum_{x \in \text{Im}(X)} g(x,y) \, P(X = x, Y = y)\)
	\item[-] prodotto di variabili: \(\displaystyle E[X \cdot Y] = \sum_{x \in \text{Im}(X)} x y \, P(X = x, Y = y)\)
	\item[-] prodotto di v. indipendenti: \(E[X \cdot Y] = E[X] \cdot E[Y]\)
\end{itemize}

\subsubsection*{Valori attesi da ricordare}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(X \sim Be(p) \quad \rightarrow \quad E[X] = p\)
	\item[-] \(X \sim B(n,p) \quad \rightarrow \quad E[X] = np\)
	\item[-] \(X \sim Ge(p) \quad \rightarrow \quad E[X] = \frac{1}{p}\)
	\item[-] \(X \sim Po(\lambda) \quad \rightarrow \quad E[X] = \lambda\)
\end{itemize}

\subsubsection*{Varianza e deviazione standard}
Sia \(X\) una variabile aleatoria discreta, la varianza di \(X\) è:
\[\var[X] := E[(X-\mu_x)^2] = E[X^2] - \mu_x^2 \qquad  \qquad \mu_x := E[X]\]
Sia \(X\) una variabile aleatoria discreta, la deviazione standard di \(X\) è:
\[\sigma_x := \sqrt{\var[X]}\]

\subsubsection*{Algebra della varianza}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] combinazioni lineari: \(\var[aX + b] := a^2\var[x]\)
	\item[-] variabili indipendenti: \(\var[X + Y] = \var[X] + \var[Y]\)
\end{itemize}

\subsubsection*{Varianze da ricordare}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(X \sim \text{costante} \quad \rightarrow \quad \var[X] = 0\)
	\item[-] \(X \sim Be(p) \quad \rightarrow \quad \var[X] = p \, (1-p)\)
	\item[-] \(X \sim B(n,p) \quad \rightarrow \quad \var[X] = n \, p \, (1-p)\)
	\item[-] \(X \sim Ge(p) \quad \rightarrow \quad \var[X] = \displaystyle \frac{1-p}{p^2}\)
	\item[-] \(X \sim Po(\lambda) \quad \rightarrow \quad \var[X] = \lambda\)
\end{itemize}

\subsubsection*{Covarianza}
Sia \(X\) una variabile aleatoria discreta, la covarianza di \(X\) è:
\[\cov[X,Y] := E[(X-\mu_x)(Y-\mu_y)] = E[XY] - \mu_x \mu_y, \qquad \qquad \mu_x := E[X] \;\; \mu_y := E[Y]\]

\subsubsection*{Proprietà della covarianza}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\cov[X,Y] = \cov[Y,X]\)
	\item[-] \(\cov[X,X] = \var[X]\)
	\item[-] \(\cov[X,Y] = 0\) se \(X\) e \(Y\) sono indipendenti
\end{itemize}

\subsection{Valore atteso varianza e covarianza di variabili continue}
\subsubsection*{Valore atteso}
Sia \(X\) una variabile aleatoria continua, il valore atteso (o media) di \(X\) è:
\[E[X] = \int_{-\infty}^{+\infty} x \; f_X(x) \, dx\]

\subsubsection*{Algebra di valori attesi}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(X \leq Y\), allora \(E[X] \leq E[Y]\)
	\item[-] combinazioni lineari: \(E[aX + b] := aE[x] + b\)
	\item[-] combinazioni lineari: \(E[aX + bY] := aE[x] + bE[Y]\)
	\item[-] composte di una variabile: \(\displaystyle E[g \circ X] := E[g(x)] = \int_{-\infty}^{+\infty}  g(x) f_x(x) \, dx\)
\end{itemize}

\subsubsection*{Valori attesi da ricordare}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(X \sim U([a,b]) \quad \rightarrow \quad E[X] = \displaystyle\frac{a+b}{2}\)
	\item[-] \(X \sim Exp(\lambda) \quad \rightarrow \quad E[X] = \displaystyle\frac{1}{\lambda}\)
	\item[-] \(Z \sim N(0,1) \quad \rightarrow \quad E[X] = 0\)
	\item[-] \(X \sim N(\mu,\sigma^2) \quad \rightarrow \quad E[X] = \mu\)
\end{itemize}

\subsubsection*{Varianza}
Sia \(X\) una variabile aleatoria continua, la varianza di \(X\) è:
\[\var[X] := E[(X-E[X])^2] = \int_{-\infty}^{+\infty} (x - E[X])^2 f_X(x) \, dx\]
\[\var[X] = E[X^2] - E[X]^2 = \int_{-\infty}^{+\infty} x^2 f_X(x) \, dx - E[X]^2\]
Sia \(X\) una variabile aleatoria continua, la deviazione standard di \(X\) è:
\[\sigma_x := \sqrt{\var[X]}\]

\subsubsection*{Algebra della varianza}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] combinazioni lineari: \(\var[aX + b] := a^2\var[x]\)
	\item[-] variabili indipendenti: \(\var[X + Y] = \var[X] + \var[Y]\)
\end{itemize}

\subsubsection*{Varianze da ricordare}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(X \sim U([a,b]) \quad \rightarrow \quad \var[X] = \displaystyle\frac{(b-a)^2}{12}\)
	\item[-] \(X \sim Exp(\lambda) \quad \rightarrow \quad \var[X] = \displaystyle\frac{1}{\lambda^2}\)
	\item[-] \(Z \sim N(0,1) \quad \rightarrow \quad \var[X] = 1\)
	\item[-] \(X \sim N(\mu,\sigma^2) \quad \rightarrow \quad \var[X] = \sigma^2\)
\end{itemize}

\subsection{Variabili identicamente distribuite}
\subsubsection*{Definizione}
Due variabili si dicono identicamente distribuite se hanno la stessa funzione di distribuzione \(F_X = F_Y\) e di conseguenza
hanno \(E[X] = E[Y]\) e \(\var[X] = \var[Y]\).

\subsubsection*{Composizione di variabili indipendenti identicamente distribuite}
Siano \(X_1, X_2, \dots X_n\) famiglia numerabile di variabili indipendenti identicamente distribuite con \(E[X_i] = \mu\) e
\(\var[X_i] = \sigma^2\), allora si ha che:
\[E[X_1 + X_2 + \dots + X_n] = n \mu \qquad \qquad \var[X_1 + X_2 + \dots + X_n] = n \sigma^2\]

\subsubsection*{Standardizzazione di variabili indipendenti identicamente distribuite}
\[\overline{X_1 + X_2 + \dots + X_n} = \frac{X_1 + X_2 + \dots + X_n - n \mu}{\sqrt{n \sigma^2}}\]

\subsection{Teorema del limite centrale}
\subsubsection*{Enunciato}
Siano \(X_1, X_2, \dots X_n\) famiglia numerabile di variabili indipendenti identicamente distribuite con \(E[X_i] = \mu\) e
\(\var[X_i] = \sigma^2\), allora si ha che:
\[P\left(\overline{X_1 + X_2 + \dots + X_n} \leq a\right) = P\left(\frac{X_1 + X_2 + \dots + X_n - n \mu}{\sqrt{n \sigma^2}} \leq a\right) \rightarrow \Phi(a) \; \text{per} \; n \to +\infty\]
Ovvero è possibile approssimare la somma di variabili indipendenti identicamente distribuite con una variabile normale con 
stesso valore atteso e stessa varianza:
\[P(X_1 + X_2 + \dots + X_n \leq a) \approx P(n\mu + \sqrt{n \sigma^2} Z \leq a) \qquad n \to +\infty \;\; Z \sim N(0,1)\]

\subsubsection*{Approssimazione della binomiale con una normale}
\[P(B(n,p) \leq a) \approx P(N(np,np(1-p)) \leq a) = P(np + \sqrt{np(1-p)} Z \leq a) \qquad n \to +\infty\]
Come valore di \(n\) sufficientemente grande si intende \(1/n << p << 1-1/n\)

\subsubsection*{Approssimazione della Poisson con una normale}
\[P(Po(\lambda) \leq a) \approx P(N(\lambda,\lambda) \leq a) = P(\lambda + \sqrt{\lambda} Z \leq a) \qquad n \to +\infty\]
Come valore di \(n\) sufficientemente grande si intende \(1/n << p << 1-1/n\)

\newpage

\subsection{Variabili congiunte continue}
\subsubsection*{Definzione}
Una variabile congiunta continua è una coppia di variabili aleatorie \((X,Y) : \Omega \to \Rd\) tali per cui esiste la densità
congiunta continua \[f_{X,Y} : \Rd \to [0, +\infty[ \qquad \qquad P((X,Y) \in A) = \int_A f_{X,Y} (x,y) \; dx \, dy\]
Si osserva che \(f_{X,Y} \geq 0\) e \(\displaystyle \int_{\R \times \R} f_{X,Y}(x,y) \; dx \, dy = 1\)

\subsubsection*{Variabile congiunta uniforme su un insieme}
Sia \(C \subset \Rd\) con area finita, la variabile uniforme su \(C\) si indica con \((X,Y) \sim U(C)\):
\[f_{X,Y}(x,y) := \begin{cases}
	\displaystyle \frac{1}{\text{Area}(C)} &\text{per} \; (x,y) \in C \\
	0 &\text{per} \; (x,y) \notin C
\end{cases} \qquad \qquad P((x,y) \in A) := \int_A \frac{1}{\text{Area}(C)} \; dx \, dy = \frac{\text{Area}(A)}{\text{Area}(C)}\]

\subsubsection*{Relazione tra densità e distribuzione}
\[f_{X,Y}(x,y) = \partial_{x,y}^2 F_{X,Y}(x,y)\]

\subsubsection*{densità marginali}
\[f_X(x) = \int_{-\infty}^{+\infty} f_{X,Y}(x,y) \; dy \qquad \qquad f_Y(y) = \int_{-\infty}^{+\infty} f_{X,Y}(x,y) \; dx\]

\subsubsection*{Valore atteso}
\[E[g(X,Y)] = \int_{\R \times \R} g(x,y) f_{X,Y}(x,y) \; dx \, dy \qquad \qquad E[XY] = \int_{\R \times \R} xy f_{X,Y}(x,y) \; dx \, dy \]

\subsubsection*{Covarianza}
\[\cov[X,Y] := E[(X-\mu_X)(Y-\mu_Y)] = E[XY] - \mu_X \mu_Y\]

\subsubsection*{Varianza di una somma}
\begin{align*}
	\var[X_1 + X_2 + \dots + X_n] &= \sum_{i,j} \cov[X_i,X_j] \\
	&= \var[X_1] + \var[X_2] + \dots + \var[X_n] + \sum_{i \neq j} \cov[X_i,X_j] \\
	&= \var[X_1] + \var[X_2] + \dots + \var[X_n] + 2\sum_{i < j} \cov[X_i,X_j]
\end{align*}

\subsubsection*{Indipendenza di variabili congiunte continue}
Siano \(X,Y\) continue indipendenti, \(\forall (x,y) \in \Rd\) si ha:
\[f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y)\]
\[P((x,y) \in A \times B) = \int_{A \times B} f_X(x) f_Y(y) \; dx \, dy = \left(\int_A f_X(x)\; dx\right) \cdot \left(\int_B f_Y(y) \; dy\right) = P(x \in A) \cdot P(y \in B)\]

\subsubsection*{Criterio dell'indipendenza}
Quando due variabili congiunte \((X,Y)\) sono indipendenti, l'insime dove \(f_{X,Y} \neq 0\) è un prodotto:
\[C := \{(x,y) \in \Rd : f_{X,Y} \neq 0\} = \{x \in \R : f_X(x) \neq 0\} \cdot \{y \in \R :f_Y(y) \neq 0\}\]
Non vale il vicecersa: se l'insieme dove \(f_{X,Y} \neq 0\) è un prodotto, non è detto che \((X,Y)\) siano indipendenti.

\subsubsection*{Varianza di somme indipendenti}
Siano \((X,Y)\) indipendenti
\[\cov[X,Y] = 0 \qquad \var[X_1 + X_2 + \dots + X_n] = \var[X_1] + \var[X_2] + \dots + \var[X_n]\]

\subsection{Legge dei grandi numeri}
\subsubsection*{Legge debole dei grandi numeri}
Siano \(X_1, X_2, \dots\) una famiglia numerabile di variabili aleatorie indipendenti e identicamente distribuite con \(E[X_i] = \mu\), allora:
\[\lim_{x \to +\infty} P\left(\left|\frac{X_1 + X_2 + \dots + X_n}{n} - \mu\right| < \varepsilon\right) = 1\]

\subsubsection*{Legge forte dei grandi numeri}
Siano \(X_1, X_2, \dots\) una famiglia numerabile di variabili aleatorie indipendenti e identicamente distribuitecon \(E[X_i] = \mu\), allora:
\[P\left(\lim_{x \to +\infty} \frac{X_1 + X_2 + \dots + X_n}{n} = \mu\right) = 1\]

\end{document}
