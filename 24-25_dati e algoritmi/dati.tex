\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc} % standard unicode
\usepackage[italian]{babel} % corretta sillabazione in italiano
\usepackage{geometry} % per impostare margini e layout pagina
\usepackage{amssymb} % per l'ambiente matematico
\usepackage{amsmath} % per l'ambiente matematico
\usepackage{enumitem} % per elenchi puntati
\usepackage{multirow} % per celle che si espandono su più righe
\usepackage{tabularx} % per tabelle con larghezza flessibile
\usepackage{booktabs} % per linee orizzontali tabelle
\usepackage{hyperref} % per collegamenti
\usepackage{graphicx} % per immagini
\usepackage{listings} % per codice
\usepackage{algorithm} % per codice
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{xcolor} % per colori nel codice
\usepackage{dirtytalk} % per le ""

% definizione colori
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% definizione stile
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}

% utilizzo stile
\lstset{style=mystyle}

% per margini
\geometry{a4paper,left=25mm, right=25mm, bottom=25mm, top=30mm}

% per centrare testo nelle tabelleX
\renewcommand\tabularxcolumn[1]{m{#1}}

% percorso delle immagini da inserire
\graphicspath{ {./ } }

% per algoritmi in pseudocodice
\renewcommand{\algorithmicprocedure}{\textbf{Algoritmo:}\;}
\renewcommand{\algorithmicrequire}{\qquad\textbf{Input:} \quad\;\;\,}
\renewcommand{\algorithmicensure}{\qquad\textbf{Output:} \;\;\;}
\algnewcommand\algorithmicin{\textbf{in}}
\algnewcommand\algorithmicto{\textbf{to}}
%\algrenewtext{For}[3] {\algorithmicfor\ #1 \gets #2 \algorithmicto\ #3 \algorithmicdo}
\algrenewtext{For}[2]{\algorithmicfor\ #1 \algorithmicto\ #2 \algorithmicdo}
\algrenewtext{ForAll}[2] {\algorithmicforall\ #1 \algorithmicin\ #2 \algorithmicdo}

\newenvironment{algo}[4]{
	\noindent\rule{\textwidth}{0.4pt}
	\begin{algorithmic}[1]
		\addtocounter{ALG@line}{-1}
		\Procedure{#1}{#2}
		\Require #3
		\Ensure #4
		\Statex }{
		\EndProcedure
	\end{algorithmic}
	\rule{\textwidth}{0.4pt}}

\title{Appunti di Dati e algoritmi}
\author{Giacomo Simonetto}
\date{Primo semetre 2024-25}

\begin{document}

% -------------------------------------- Copertina e indice ---------------------------------------
\maketitle
\begin{abstract}
	Appunti del corso di Dati e algoritmi della facoltà di Ingegneria Informatica dell'Università di Padova.
\end{abstract}

\newpage

\tableofcontents

\newpage

% ----------------------------------------- Introduzione ------------------------------------------
\section{Introduzione all'algoritmica}
\subsection{Progettazione di un algoritmo}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] specifica del problema computazionale
	\item[-] progetto dell'algoritmo (pseudocodice)
	\item[-] analisi dell'efficacia e efficienza dell'algoritmo (correttezza e complessità)
	\item[-] codifica programma (linguaggio di programmazione)
\end{itemize}

%\item[-] \textbf{algoritmo}: \\ istruzioni ad alto livello generiche, non scritte in un linguaggio di programmazione
%\item[-] \textbf{programma}: \\ algoritmo scritto in un linguaggio di programmazione che può essere eseguito da un computer

\subsection{Problema computazionale}
Un problema computazionale è composto da tre componenti:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(I\) l'insieme delle istanze (input)
	\item[-] \(S\) l'insieme delle soluzioni (output)
	\item[-] \(\pi \subseteq I \times S\) una relazione che associa ad ogni elemento di \(I\) un corrispettivo elemento di \(S\).
\end{itemize}
Se esistono più soluzioni per una generica istanza, l'algorimto ne calcola una arbitraria

\subsection{Algoritmo}
Un algoritmo è una procedura computazionale che trasforma un dato input in un output eseguendo una sequenza di passi elementari.
Sfrutta il modello di calcolo \(RAM\) (Random Access Machine) con le seguenti caratteristiche:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] è simile al modello di Von Neumann
	\item[-] in memoria sono contenuti i dati di input, output e dati intermedi
	\item[-] la CPU utilizza operazioni elementari per risolvere il problema
\end{itemize}
Il problema che risolve l'algoritmo è un problema computazionale, ovvero trova le soluzioni abbinate ad una determinata istanza.

\subsubsection*{Pseudocodice}
Per descrivere le azioni di un algoritmo si utilizza lo pseudocodice nella seguente struttura:

\begin{algo}{MyAlgoritm}{parametri}{descrizione dell'input}{descrizione dell'output}
	\State descrizione dell'algorimo in pseudocodice
	\State \dots
	\State \dots
\end{algo}

\subsection{Complessità - efficienza di un algoritmo}
\subsubsection*{Taglia di un'istanza}
La taglia di un'istanza è l'indice di misura della dimensione dei dati di input, scelto in base all'algoritmo.

\subsubsection*{Complessità temporale}
La complessità temporare indica una stima asintotica del numero di operazioni elementari eseguite dall'algoritmo per una certa
istanza. Suggerisce una stima del tempo di esecuzione di un algoritmo per una certa istanza ed è utilizzata per confrontare
algoritmi diversi in base alla loro efficienza computazionale.

\subsubsection*{Caratteristiche}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] la complessità si esprime in maniera generale in funzione della taglia dell'istanza
	\item[-] è indipendente dall'implementazione e dal linguaggio usato
	\item[-] non richiede di conoscere il tempo esatto di esecuzione, per cui non richiede l'implementazione dell'algoritmo in
	un certo linguaggio di programmazione, ma basta lo pseudocodice
\end{itemize}

\subsubsection*{Notazione}
Per indicare una complessità asintotica, si utilizzano gli ordini di grandezza e le notazioni \(\Omega\), \(\Theta\), \(O\).
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(f(n) \in O(g(n)) \quad \Rightarrow \quad \exists c > 0, n_0 \geq 1 \;\; \text{t.c.} \;\; f(n) \leq c \, g(n) \quad \forall n \geq n_0\)
	\item[-] \(f(n) \in \Omega(g(n)) \quad \Rightarrow \quad \exists c > 0, n_0 \geq 1 \;\; \text{t.c.} \;\; f(n) \geq c \, g(n) \quad \forall n \geq n_0\)
	\item[-] \(f(n) \in \Theta(g(n)) \quad \Rightarrow \quad f(n) \in O(g(n)) \;\; \text{e} \;\; f(n) \in \Omega(g(n))\)
	\item[-] \(O(1) \; \; \leq \; \; O(\log n) \; \; \leq \; \; O(n) \; \; \leq \; \; O(n \log n) \; \; \leq \; \; O(n^2) \; \; \leq \; \; O(n^3) \; \; \leq \; \; O(2^n) \;\; \leq \;\; \dots\)
\end{itemize}

\subsubsection*{Calcolo}
Per trovare la complessità asintotica al caso pessimo si effettuano le seguenti analisi:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] si cerca un upper-bound attraverso l'analisi del caso pessimo (in assoluto) e lo si esprime come \(O(f(n))\)
	\item[2.] si cerca un lower-bound attraverso l'analisi di una istanza \say{cattiva} e lo si esprime come \(\Omega(g(n))\)
	\item[3.] se \(f(n) = g(n)\) e la complessità dell'algoritmo \(\in O(f(n))\) e \(\in \Omega(g(n))\), allora si conclude che
	l'algoritmo ha complessità al caso pessimo \(\in \Theta(f(n))\)
\end{itemize}

\subsubsection*{Osservazioni}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] un algoritmo ha complessità ottima se è \(O(\log n)\), buona se \(O(n)\) o \(O(n \log n)\), discreta se \(O(n^2)\),
	pessima se \(O(2^n)\) o \(O(n^n)\)
	\item[-] alcune volte il valore \(n_0\) è per istanze molto grandi e può capitare che un algoritmo di complessità \(\Theta(n^3)\)
	sia più efficiente di uno di complessità \(\Theta(\log n)\)
	\item[-] alcune volte, il caso pessimo si verifica per istanze patologiche che si verificano estremamente raramente, per cui
	si ricorre all calcolo della complessità al caso medio
\end{itemize}

\subsection{Correttezza e invariante - efficacia di un algoritmo}
\subsubsection*{Criteri di correttezza}
Un algoritmo è corretto se:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] termina in un numero finito di passi
	\item[-] restituisce una soluzione valida per il problema computazionale
\end{itemize}

\subsubsection*{Invariante}
L'invariante è una proprietà costante espressa in funzione delle variabili dell'algoritmo e descrive lo stato ottenuto in ogni
segmento dell'algoritmo stesso. Per avere senso, deve essere funzionale alla correttezza dell'algoritmo.
L'invariante è spesso usato per verificare la correttezza di un ciclo o di una serie di chiamate ricorsive: lo si esprime
in funzione delle variabili del ciclo o dei parametri della chiamata ricorsiva e si verifica che esso sia soddisfatto alla
fine di ogni iterazione o chiamata ricorsiva.

\subsubsection*{Verifica della correttezza in un ciclo}
Per verificare la correttezza di un ciclo si utilizza un processo induttivo:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] si determina l'invariante per il ciclo
	\item[2.] si dimostra che l'invariante vale per l'iterazione -1 (ovvero prima dell'inizio del ciclo)
	\item[3.] si verifica che se vale per una certa iterazione \(i\), allora vale per l'iterazione \(i+1\)
	\item[4.] si verifica che l'invariante alla fine del ciclo combacia con la condizion di correttezza della soluzione
\end{itemize}

\subsection{Algoritmi ricorsivi}
\subsubsection*{Definizione}
Un algoritmo si dice ricorsivo se richiama se stesso su istanze di minore dimensione fino ad arrivare ad un caso base in cui la
soluzione è immediata.

\subsubsection*{Esecuzione e stack}
Ad ogni nuova chiamata ricorsiva, viene aggiunto un nuovo record di attivazione per tale chiamata nello stack. Nell'RDA sono
contenute tutte le variabili e i riferimenti agli oggetti memorizzati nell'heap. Alla chiusura del metodo, si elimina l'RDA
di tale metodo e si apre quello sottostante.

\subsubsection*{Esecuzione e recursion tree}
All'esecuzione di un algoritmo ricorsivo si associa un albero della ricorsione con le seguenti caratteristiche:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] ogni nodo corrisponde ad una chiamata ricorsiva
	\item[-] la radice corrisponde alla prima chiamata ricorsiva
	\item[-] i figli di un nodo corrispondono alle chiamate ricorsive effettuate nell'esecuzione della chiamata associata al nodo padre
	\item[-] le foglie corrispondo ai casi base
\end{itemize}

\subsubsection*{Differenza tra algoritmo iterativo e algoritmo ricorsivo}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] un algoritmo iterativo, per definizione, deve contenere cicli al suo interno
	\item[-] un algoritmo ricorsivo, per definizione, deve contenere almeno una chiamata a se stesso
	\item[-] iterativo e ricorsivo sono mutualmente esclusivi: non esiste un algoritmo ricorsivo e iterativo
	\item[-] un algoritmo ricorsivo può avere o non avere cicli, ma un algoritmo iterativo non ha chiamate ricorsive perché altrimenti
	sarebbe un algoritmo ricorsivo
\end{itemize}

\subsubsection*{Complessità di algoritmi ricorsivi}
Per determinare la complessità di un algoritmo ricorsivo si può procedere in vari modi:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] contare il numero di operazioni per livello e moltiplicarlo per il numero di livelli dell'albero della ricorsione, come
	nel caso di mergesort
	\item[-] contare tutte le operazioni eseguite ad ogni chiamata ricorsiva e sommarne i valori
\end{itemize}

\subsubsection*{Correttezza di algoritmi ricorsivi}
Per determinare la correttezza di un algoritmo ricorsivo si procede con una dimostrazione per induzione:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] si dimostra che l'algoritmo risolve correttamente i casi base
	\item[2.] si verifica che se l'algoritmo è corretto per una determinata istanza, allora vale anche un'istanza di taglia superiore
\end{itemize}

\newpage


\section{Alberi}
\subsection{Introduzione}
\subsubsection*{Definizione di albero}
Un albero radicato è una collezione di nodi vuota o con le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\exists \, r\) nodo detto radice
	\item[-] se un generico nodo \(n \neq r\), allora deve avere un unico padre
	\item[-] da ogni nodo, risalendo di padre in padre si arriva ad \(r\)
\end{itemize}

\subsubsection*{Altre definizioni}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{antenato}: \\
	un nodo \(x\) è antenato di \(y\) se \(x = y\) o se \(x\) è antenato del padre di \(y\)
	\item[-] \textbf{discendente}: \\
	un nodo \(x\) è discendente di \(y\) se \(y\) è antenato di \(x\)
	\item[-] \textbf{nodo interno}: \\
	un nodo \(x\) è interno se ha dei figli
	\item[-] \textbf{nodo esterno o foglia}: \\
	un nodo \(x\) è esterno o una foglia se non ha figli
	\item[-] \textbf{sottoalbero con radice v}: \\
	indicato con \(T_v\) è un albero formato dai discendenti di \(v\)
	\item[-] \textbf{albero ordinato}: \\
	un albero si dice ordinato se è presente un ordinamento lineare tra i figli di un nodo
	\item[-] \textbf{albero radicato}: \\
	un albero si dice radicato se è composto da un nodo radice \(r\) che ha come figli, altri alberi radicati nei nodi figli di \(r\)
	\item[-] \textbf{profondità di un nodo}: \\
	la profondità di un nodo è la distanza tra il nodo e la radice
	\item[-] \textbf{altezza di un nodo}: \\
	l'altezza di un nodo è la distanza tra il nodo e la foglia più distante
	\item[-] \textbf{livello}: \\
	il livello \(i\) di un albero è l'insieme di nodi con profondità \(i\)
	\item[-] \textbf{altezza di un albero}: \\
	l'altezza di un albero è la massima profondità delle foglie
\end{itemize}

\newpage

\subsection{Algoritmi base: profondità e altezza di un nodo}
\subsubsection*{Profondità di un nodo - ricorsiva}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo calcola la distanza tra il nodo \(v\) e la radice, risalendo di padre in padre
	\item[-] La complessità dell'algoritmo è \(\Theta(n)\), più precisamente \(\Theta(d_v)\), con \(d_v\) profondità del nodo \(v\).
\end{itemize}

\begin{algo}{depth\_ric}{$v$}{$v$ nodo dell'albero $T$}{profondità di $v$}
	\If{(T.isRoot($v$))}
		\State return 0
	\Else
		\State return 1 + depth(T.parent($v$))
	\EndIf
\end{algo}

\subsubsection*{Profondità di un nodo - iterativa}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo calcola la distanza tra il nodo \(v\) e la radice, risalendo di padre in padre
	\item[-] La complessità dell'algoritmo è \(\Theta(n)\), più precisamente \(\Theta(d_v)\), con \(d_v\) profondità del nodo \(v\).
\end{itemize}

\begin{algo}{depth\_iter}{$v$}{$v$ nodo dell'albero $T$}{profondità di $v$}
	\State $d \gets 0$
	\State $u \gets v$
	\While{(!T.isRoot($u$))}
		\State $u \gets $ T.parent($u$)
		\State $d \gets d+1$
	\EndWhile
	\State return $d$
\end{algo}

\subsubsection*{Atezza di un nodo}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo calcola prima l'altezza dei discendenti di \(v\) e prosegue verso l'alto fino a \(v\)
	\item[-] La complessità dell'algoritmo è \(\Theta(n)\), più precisamente \(\Theta(n_v)\), con \(n_v=\) \# nodi del sottoalbero \(T_v\).
\end{itemize}

\begin{algo}{height}{$v$}{$v$ nodo dell'albero $T$}{altezza di $v$}
	\State $h \gets 0$
	\State $w \gets v$
	\ForAll{$w$}{T.children($v$)}
		\State $h \gets \max\{h, 1 + \text{height}(w)\}$
	\EndFor
	\State return $d$
\end{algo}

\newpage

\subsection{Visite: preorder e postorder}
\subsubsection*{Visita in preorder}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo prevede di visitare prima il nodo \(v\) e successivamente i figli
	\item[-] La complessità dell'algoritmo è \(\Theta\left(n + \displaystyle \sum_{v \in T}t_v\right)\), con \(t_v\) costo della visita di un nodo
\end{itemize}

\begin{algo}{preorder}{$v$}{$v$ nodo dell'albero $T$}{/}
	\State visita $v$
	\ForAll{$w$}{T.children($v$)}
		\State preorder($w$)
	\EndFor
\end{algo}

\subsubsection*{Visita in postorder}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo prevede di visitare prima i figli e successivamente il nodo \(v\)
	\item[-] La complessità dell'algoritmo è \(\Theta\left(n + \displaystyle \sum_{v \in T}t_v\right)\), con \(t_v\) costo della visita di un nodo
\end{itemize}

\begin{algo}{postorder}{$v$}{$v$ nodo dell'albero $T$}{/}
	\ForAll{$w$}{T.children($v$)}
		\State postorder($w$)
	\EndFor
	\State visita $v$
\end{algo}

\newpage

\section{Alberi binari}
\subsection{Introduzione}
\subsubsection*{Definzione di albero binario}
Un albero binario è un albero ordinato in cui ogni nodo ha al più die figli. Ogni nodo è etichettato come figlio sinistro o figlio
destro, in base alla posizione di ordinamento.

\subsubsection*{Altre definizioni}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{albero binario proprio}: \\
	un albero binario si dice proprio se ogni nodo ha esattamente due figli
	\item[-] \textbf{albero binario proprio estremo}: \\
	un albero binario proprio si dice estremo se nessun figlio destro (o sinistro) ha discendenti, ovvero se è molto sbilanciato
	verso sinistra (o verso destra)
	\item[-] \textbf{albero binario proprio perfettamente bilanciato}: \\
	un albero binario proprio si dice perfettamente bilanciato se tutte le foglie sono allo stesso livello
	\item[-] \textbf{albero binario completo}: \\
	un albero binario con \(i\) livelli si dice completo se tutti i livelli da 0 a \(i-1\) sono completi (contengono \(2^i\) nodi)
	e l'ultimo livello \(i\) è popolato a partire da sinistra (approfondito nella sezione heap).
\end{itemize}

\subsubsection*{Proprietà di un albero binario proprio}
\begin{center}
	\begin{tabularx}{\textwidth}{l c c X}
		Dati: &\(n\) = numero di nodi in T & \(h\) = altezza di T \(\qquad\qquad\;\;\) &\\
		& \(m\) = numero di foglie in T & \(n-m\) = numero di nodi interni in T &
	\end{tabularx}
\end{center}
Un albero binario proprio ha le seguenti proprietà:
\begin{center}
	\begin{tabular}{c c c}
		\textbf{binario generico} & \textbf{binario estremo} & \textbf{binario perf. bil.} \\
		\toprule
		\(m = n - m + 1\quad\) & \(\quad m = n - m + 1 \quad\) & \(\quad m = n - m + 1\) \\
		\midrule
		\(h+1 \leq m \leq 2^h\quad\) & \(\quad h+1 = m \quad\) & \(\quad m = 2^h\) \\
		\midrule
		\(h \leq n-m \leq 2^h-1\quad\) & \(\quad h = n-m \quad\) & \(\quad n-m = 2^h-1\) \\
		\midrule
		\(2h + 1 \leq n \leq 2^{h+1}-1\quad\) & \(\quad 2h + 1 = n \quad\) & \(\quad n = 2^{h+1}-1\) \\
		\midrule
		\(\log_2(n + 1)-1 \leq h \leq \frac{n-1}{2}\quad\) & \(\quad h = \frac{n-1}{2} \quad\) & \(\quad h = \log_2(n + 1)-1\) \\
		\bottomrule
	\end{tabular}
\end{center}
Dall'ultima proprietà si osserva che la miglior stima dell'altezza \(h\) di un generico albero binario è \(O(n)\) e non è sempre
detto che sia \(O(\log n)\).

\subsection{Visite: inorder}
\subsubsection*{Visita inorder}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo prevede di visitare prima il figlio sinistro, poi il nodo stesso e infine il figlio destro.
	\item[-] La complessità dell'algoritmo è \(\Theta\left(n + \displaystyle \sum_{v \in T}t_v\right)\), con \(t_v\) costo della visita di un nodo
\end{itemize}

\begin{algo}{inorder}{$v$}{$v$ nodo dell'albero $T$}{/}
	\If{(T.left(\(v\)) \(\neq\) null)}
		\State inorder(T.left(\(v\)))
	\EndIf
	\State visita $v$
	\If{(T.right(\(v\)) \(\neq\) null)}
		\State inorder(T.right(\(v\)))
	\EndIf
\end{algo}

\subsection{Parser Tree}
\subsubsection*{Definizione}
Il Parse Tree \(T\) associato ad una espressioe aritmetica \(E\) (con operatori solo binari) è un albero binario proprio in cui
i nodi foglia contengono le variabili/costanti di \(E\) e i nodi interni contengono gli operatori di \(E\), in modo tale che:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(E = a\), con \(a\) costante/variabile, allora \(T\) è costituito da un'unica foglia contenente \(a\)
	\item[-] se \(E = E_1 op E_2\), la radice \(T\) contiene l'operatore \(op\) e ha come sottoalbero sinistro \(E_1\) e come
	sottoalbero destro \(E_2\)
\end{itemize}

\subsubsection*{Notazione infissa}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo sfrutta la visita inorder degli alberi binari.
	\item[-] La complessità dell'algoritmo è \(\Theta\left(n\right)\), in quanto \(t_v\) costo della visita di un nodo è \(\Theta(1)\).
\end{itemize}
\begin{algo}{infix}{$T$, $v$, $L$}{Parse Tree $T$, \(v \in T\), lista $L$}{$E_v$ in notazione infissa nella lista $L$}
	\If{($T$.isExternal($v$))}
		\State $L$.addLast($v$.getElement())
	\Else
		\State $L$.addLast(\say{(}))
		\State infix($T$, $T$.left($v$), $L$)
		\State $L$.addLast($v$.getElement())
		\State infix($T$, $T$.right($v$), $L$)
		\State $L$.addLast(\say{)})
	\EndIf
\end{algo}


\subsubsection*{Notazione postfissa}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo sfrutta la visita in postorder applicata agli alberi binari.
	\item[-] La complessità dell'algoritmo è \(\Theta\left(n\right)\), in quanto \(t_v\) costo della visita di un nodo è \(\Theta(1)\).
\end{itemize}
\begin{algo}{postfix}{$T$, $v$, $L$}{Parse Tree $T$, \(v \in T\), lista $L$}{$E_v$ in notazione postfissa nella lista $L$}
	\If{($T$.isExternal($v$))}
		\State $L$.addLast($v$.getElement())
	\Else
		\State postfix($T$, $T$.left($v$), $L$)
		\State postfix($T$, $T$.right($v$), $L$)
		\State $L$.addLast($v$.getElement())
	\EndIf
\end{algo}

\newpage

\subsection{Implementazione in Java}
\subsubsection*{Interfaccia Iterator}
\begin{lstlisting}[language=Java]
// cursore che permette di muoversi tra gli elementi di una collezione
public interface Iterator<E> {
	/** Returns true if the scan of the collection is not over */
	boolean hasNext();
	/** Returns the next element in the collection */
	E next();
}
\end{lstlisting}

\subsubsection*{Interfaccia Iterable}
\begin{lstlisting}[language=Java]
// struttura dati iterabile
public interface Iterable<E> {
	/** Returns an iterator of the collection */
	Iterator<E> iterator()
}
\end{lstlisting}

\subsubsection*{Interfaccia Tree}
\begin{lstlisting}[language=Java]
// Albero
public interface Tree<E> extends Iterable {
	/** Returns the number of positions in the tree */
	int size();
	/** Returns true if the tree contains no positions */
	boolean isEmpty();
	/** Returns the Position of the root (or null if empty)*/
	Position<E> root();
	/** Returns the Position of p's parent (or null if p is the root) */
	Position<E> parent(Position<E> p);
	/** Returns an iterable containing p's children */
	Iterable<Position<E>> children(Position<E> p);
	/** Returns the number of children of p */
	int numChildren(Position<E> p);
	/** Returns true if p is internal */
	boolean isInternal(Position<E> p);
	/** Returns true if p is external */
	boolean isExternal(Position<E> p);
	/** Returns true if p is root */
	boolean isRoot(Position<E> p);
	/** Returns an iterator to all element in the tree */
	Iterator<E> iterator();
	/** Returns an iterable containing all positions in the tree */
	Iterable<Position<E>> positions();
}
\end{lstlisting}

\subsubsection*{Interfaccia BinaryTree}
\begin{lstlisting}[language=Java]
// Albero binario
public interface BinaryTree<E> extends Tree<E> {
	/** Returns the Position of p's left child(or null if empty)*/
	public Position<E> left(Position<E> p);
	/** Returns the Position of p's right child(or null if empty)*/
	public Position<E> right(Position<E> p);
	/** Returns the Position of p's sibling (or null p is an only child)*/
	public Position<E> sibling(Position<E> p);
}
\end{lstlisting}

\newpage

\section{Priority Queue}
\subsection{Introduzione}
\subsubsection*{Definizione}
Una priority queue è una collezione di entry in cui le chiavi rappresentano una priorità e provengono da un insieme totalmente
ordinato. Minore è il valore della chiave, maggiore è la priorità.

\subsubsection*{Metodi e interfaccia priority queue}
\begin{lstlisting}[language=Java]
// Priority queue
public interface PriorityQueue<k,v> {
	int size();
	boolean isEmpty();
	/** Inserts and returns a new entry (key,value) */
	Entry<K,V> insert(K key, V value);
	/** Returns an entry with min key, without removing it */
	Entry<K,V> min();
	/** Returns and removes an entry with min key */
	Entry<K,V> removeMin();
}
\end{lstlisting}

\subsection{Implementazioni e relativa complessità}
\begin{center}
	\begin{tabular}{c | c | c | c}
		\textbf{metodo} & \textbf{unordered list} & \textbf{ordered list} & \textbf{heap} \\
		\toprule
		\verb|min| & \(O(n)\) & \(O(1)\) & \(O(1)\) \\
		\midrule
		\verb|removeMin| & \(O(n)\) & \(O(1)\) & \(O(\log(n))\) \\
		\midrule
		\verb|insert| & \(O(1)\) & \(O(n)\) & \(O(\log(n))\) \\
		\bottomrule
	\end{tabular}
\end{center}

Nell'implementazione con la \textbf{lista non ordinata}, l'inserimento impiega tempo costante, ma per l'accesso alla chiave minima
e di conseguenza la relativa rimozione è necessario applicare un algoritmo di ricerca che impiega tempo lineare.

Nell'implementazione con la \textbf{lista ordinata}, l'accesso alla chiave minima e la relativa rimozione impiegano tempo costante,
ma per l'inserimento è necessario applicare l'algoritmo di \verb|insertionsort| che impiega tempo lineare.

Nell'implementazione con lo \textbf{heap}, l'inserimento e la rimozione impiegno un tempo logaritmico, per come definiti i metodi
di inserimento e rimozione nell'heap (up heap bubbling e down heap bubbling), mentre l'accesso alla chiave minima impiega tempo
costante. Quest'ultima implementazione offre una migliore efficienza computazionale e anche una migliore efficienza spaziale (se
l'heap è implementato efficientemente con un array).

\newpage

\section{Heap}
\subsection{Introduzione}
\subsubsection*{Definizione}
Lo heap (mucchio in italiano) è un albero binario completo in cui i nodi contengono delle entry \(<k,v>\) e soddisfano una proprietà
chiamata heap order property: siano \(<k_2,v>\) e \(<k_3,v>\) figli di \(<k_1,v>\), allora vale \(k_1 \leq \min \{k_2, k_3\}\).

\subsubsection*{Proprietà}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] un albero binario completo ha altezza \(h = \lfloor \log n \rfloor\)
	\item[-] esiste ed è unico, l'albero binario completo di \(n\) entry
	\item[-] le chiavi lungo un cammino dalla radice alle foglie formano una sequenza non decrescente
	\item[-] per qualsiasi figlio \(u\) di \(v\) vale \(v\).getKey() \(\leq\) \(u\).getKey()
	\item[-] la radice contiene la entry con chiave minima
	\item[-] se le entry sono tutte distinte, la entry con chiave massima si trova in una foglia
\end{itemize}

\subsubsection*{Level numbering e rappresentazione tramite array}
La rappresentazione Level numbering permette di assegnare un indice a ciascun nodo dello heap secondo le seguenti regole:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] la radice ha indice 0 e si trova in \(P[0]\)
	\item[2.] i figli di \(P[i]\) hanno indici \(2i+1\) e \(2i+2\), si trovano in \(P[2i+1]\) e \(P[2i+2]\)
	\item[3.] il padre di \(p[i]\) ha indice \(\lfloor (i-1)/2 \rfloor\) e si trova in posizione \(P[\lfloor (i-1)/2 \rfloor]\)
\end{itemize}
Si osserva che i nodi di ciascun livello hanno indici crescenti da sinistra verso destra. In questo modo è possibile rappresentare
uno heap su un array, ottenuto giustapponendo i nodi di ogni livello mantenendo le posizioni recicproche dei nodi da sinistra verso
destra. Tale rappresentazione è space efficient perché c'è corrispondenza 1:1 tra un albero binario completo e un array.

\subsubsection*{Nodo Last}
Per praticità, il nodo più a destra dell'ultimo livello è chiamato nodo Last e nella rappresentazione tramite array e level
numbering si trova nell'ultima cella dell'array con indice \(\lfloor n-1 \rfloor\).

\newpage

\subsection{Algoritmi base: getmin, insert e remove}
\subsubsection*{Accesso alla entry con chiave minima}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo restituisce la entry contenuta nella radice dell'heap
	\item[-] La complessità dell'algoritmo è \(O(1)\)
\end{itemize}
\begin{algo}{getmin}{$ $}{riferimento implicito all'heap rappresentato tramite array}{entry con chiave minima}
	\State return P[0]
\end{algo}

\subsubsection*{Inserimento di una nuova entry}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo inserisce una nuova entry attraverso uno up-heap bubbling.
	\item[-] La complessità dell'algoritmo è \(\Theta(\log n)\), proporzionale all'altezza dell'heap.
\end{itemize}
\begin{algo}{insert}{$k$, $v$}{entry da inserire e riferimento implicito all'heap $P[]$}{/}
	\State $e \gets (k,v)$
	\State $P[++last] \gets e$
	\State $i \gets last$
	\While{($i > 0$ and $P[\lfloor (i-1)/2\rfloor]$.getKey() $\geq$ $P[i]$.getKey())} \Comment{up-heap bubbling}
		\State swap($P[i]$, $P[\lfloor (i-1)/2\rfloor]$)
		\State $i \gets \lfloor (i-1)/2\rfloor$
	\EndWhile
	\State return $e$
\end{algo}

\subsubsection*{Rimozione della entry con chiave minima}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo rimuove la radice dell'heap attraverso un down-heap bubbling.
	\item[-] La complessità dell'algoritmo è \(\Theta(\log n)\), proporzionale all'altezza dell'heap.
\end{itemize}
\begin{algo}{removemin}{$ $}{riferimento implicito all'heap $P[]$}{entry con chiave minima rimossa}
	\State $minEntry \gets P[0]$
	\State $P[0] \gets P[last--]$
	\State $i \gets 0$
	\State $j \gets$ indexMinChild($P$, $i$)
	\While{($j \neq$ null and $P[i]$.getKey() $>$ $P[j]$.getKey())} \Comment{down-heap bubbling}
		\State swap($P[i]$, $P[j]$)
		\State $i \gets j$
		\State $j \gets$ indexMinChild($P$, $i$)
	\EndWhile
	\State return $minEntry$
\end{algo}

\newpage

\subsubsection*{Rimozione di una generica entry}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo rimuove una generica entry dell'heap e ripristino la heap order property con un down-heap bubbling e
	un up-heap bubbling.
	\item[-] La complessità dell'algoritmo è \(\Theta(\log n)\), proporzionale all'altezza dell'heap.
\end{itemize}
\begin{algo}{remove}{$l$}{indice della entry da rimuovere e riferimento implicito all'heap $P[]$}{entry rimossa}
	\State $retValue \gets P[l]$
	\State $P[l] \gets P[last--]$
	\State $i \gets l$
	\State $j \gets$ indexMinChild($P$, $i$)
	\While{($j \neq$ null and $P[i]$.getKey() $>$ $P[j]$.getKey())} \Comment{down-heap bubbling}
		\State swap($P[i]$, $P[j]$)
		\State $i \gets j$
		\State $j \gets$ indexMinChild($P$, $i$)
	\EndWhile
	\While{($i > 0$ and $P[\lfloor (i-1)/2\rfloor]$.getKey() $\geq$ $P[i]$.getKey())} \Comment{up-heap bubbling}
		\State swap($P[i]$, $P[\lfloor (i-1)/2\rfloor]$)
		\State $i \gets \lfloor (i-1)/2\rfloor$
	\EndWhile
	\State return $retValue$
\end{algo}

\vspace{30pt}

\subsection{Costruzione di un heap inplace}
\subsubsection*{Costruzione di un heap da un array, con approccio Top-Down}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo inserisce le chiavi una ad una, incrementanto \(j\) e costruisce l'heap con un up-heap bubbling per
	ogni chiave inserita
	\item[-] La complessità dell'algoritmo è \(\Theta(n \log n)\), ovvero la ripetizione di \(n\) volte dell'up-heap bubbling
	con complessità \(\Theta(\log n)\)
\end{itemize}
\begin{algo}{makeHeap\_TopDown}{$ $}{riferimento implicito all'heap $P[]$}{entry con chiave minima rimossa}
	\For{$j \gets 1$}{$n-1$}
		\State $i \gets j$
		\State // up-heap bubbling
		\While{($i > 0$ and $P[\lfloor (i-1)/2\rfloor]$.getKey() $\geq$ $P[i]$.getKey())}
			\State swap($P[i]$, $P[\lfloor (i-1)/2\rfloor]$)
			\State $i \gets \lfloor (i-1)/2\rfloor$
		\EndWhile
	\EndFor
\end{algo}

\newpage

\subsubsection*{Costruzione di un heap da un array, con approccio Bottom-Up}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo costruisce tanti sottoheap che si sviluppano dal livello \(i\) fino alla base. Ad ogni iterazione del
	for i sottoheap si uniranno a due a due con l'inserimento della entry dal livello \(i-1\) con un down-heap bubbling.
	\item[-] La complessità dell'algoritmo è \(\Theta(n)\), più vantaggioso rispetto al precedente approccio top-down.
\end{itemize}
\begin{algo}{makeHeap\_BottomUp}{$ $}{riferimento implicito all'heap $P[]$}{entry con chiave minima rimossa}
	\For{$j \gets \lfloor (n-2)/2 \rfloor$}{0} \Comment{$P[\lfloor (n-2)/2 \rfloor]$ nodo più a dx nel penultimo livello}
		\State $i \gets j$
		\State $k \gets$ indexMinChild($P$,$i$)
		\State // down-heap bubbling
		\While{($k \neq$ null and $P[i]$.getKey() $>$ $P[k]$.getKey())}
			\State swap($P[i]$, $P[k]$)
			\State $i \gets k$
			\State $k \gets$ indexMinChild($P$,$i$)
		\EndWhile
	\EndFor
\end{algo}

\subsection{HeapSort}
\subsubsection*{Ordinamento di un array attraverso una priority queue implementata come heap su array}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si sviluppa in due fasi:
	\subitem fase1: creazione di un max-heap inplace con approcco bottom-up, con complessità \(\Theta(n)\)
	\subitem fase2: estrazione delle chiavi inplace con leggere modifiche, con complessità \(\Theta(n \log n)\)
	\item[-] La complessità dell'algoritmo è \(\Theta(n \log n)\),
\end{itemize}
\begin{algo}{heapSort}{$P$}{array $P$ con chiavi da riordinare}{array $P$ con chiavi ordinate}
	\Comment 1. Creazione del max-heap con approccio bottom-up
	\For{$j \gets \lfloor (n-2)/2 \rfloor$}{0}
		\State $i \gets j$
		\State $k \gets$ indexMaxChild($P$, $i$) \Comment NB: max-heap: uso indexMaxChild()
		\While{($k \neq$ null AND $P[i]$.getKey() $<$ $P[k]$.getKey())} \Comment NB: max-heap: $P[i] < P[k]$
			\State swap($P[i]$, $P[k]$)
			\State $i \gets k$
			\State $k \gets$ indexMaxChild($P$, $i$) \Comment NB: max-heap: uso indexMaxChild()
		\EndWhile
	\EndFor
	\Comment 2. Estrazione delle entry e down-heap bubbling
	\State $last \gets n-1$
	\For{$j \gets 1$}{0}
		\State swap($P[0]$, $P[last]$) \Comment sposto chiave massima in fondo
		\State $last \gets n-j-1$ \Comment escludo la chiave massima appena spostata
		\State $i \gets 0$ \Comment down-heap bubbling di $P[0]$
		\State $k \gets$ indexMaxChild($P$, $i$)
		\While{($k \neq$ null AND $P[i]$.getKey() $<$ $P[k]$.getKey())}
			\State swap($P[i]$, $P[k]$)
			\State $i \gets k$
			\State $k \gets$ indexMaxChild($P$, $i$) \Comment NB: max-heap: uso indexMaxChild()
		\EndWhile
	\EndFor
\end{algo}

\newpage

\section{Mappe}
\subsection{Introduzione}
\subsubsection*{Definizione}
Una mappa è una collezione di entry con chiavi distinte da un insieme \(U\) per cui è definito l'operatore \(=\). La mappa
implementa l'accesso, l'inserimento e la rimozione delle entry attraverso le chiavi.

\subsubsection*{Metodi e interfaccia in Java}
\begin{lstlisting}[language=Java]
public interface Map<K,V>{
	int size();
	boolean isEmpty();
	/** Returns the value of the entry (k,v) with k = key if exists, otherwise null */
	V get (K key);
	/** If there is an entry (k,v) with k = key then update the value of that entry and returns the old value, otherwise inserts a new entry (key, value) and returns null */
	V put (K key, V value);
	/** Removes the entry (k,v) with k = key and returns the value of the entry, otherwise return null*/
	V remove (K key);
	Iterable<K> keySet();
	Iterable<V> values();
	Iterable<Entry<K,V>> entrySet();
}
\end{lstlisting}

\subsection{Implementazioni e relativa complessità}
\begin{center}
	\begin{tabular}{c | c | c | c | c}
		\textbf{metodo} & \textbf{position-based list} & \textbf{array} & \textbf{tabella hash} & \textbf{MWST, (2,4)-Treee, RB-Tree} \\
		\toprule
		\verb|get| & \(O(n)\) & \(O(1)\) & \(O(1 + \lambda)\) & \(O(\log n)\) \\
		\midrule
		\verb|put| & \(O(n)\) & \(O(1)\) & \(O(1 + \lambda)\) & \(O(\log n)\) \\
		\midrule
		\verb|remove| & \(O(n)\) & \(O(1)\) & \(O(1 + \lambda)\) & \(O(\log n)\) \\
		\bottomrule
	\end{tabular}
\end{center}

\subsubsection*{Implementazione con position-based list}
L'implementazione con una position-based list è efficiente dal punto di vista spaziale, in quanto lo spazio occupato in memoria
è proporzionale al numero di entry, ma è molto svantaggiosa dal punto di vista computazionale in quanto i metodi utilizzano la
ricerca in una lista che impiega complessità \(O(n)\).

\subsubsection*{Implementazione con array}
Si suppone di poter creare un array con gli indici \([0, |U|-1]\) e di disporre di un mapping 1:1 tra l'insieme delle chiavi
e l'indice dell'array. È possibile riservare una cella dell'array ad ogni possibile entry della mappa. In questo modo i vari
metodi hanno complessità costante, ma nel caso in cui \(|U| \gg \#\) entry, la struttura diventa molto svantaggiosa dal punto
di vista della memoria occupata.

\subsubsection*{Implementazione con tabella hash}
Simile all'implementazione precedente, ma utilizzando le hash table e le funzioni hash è possibile ridurre notevolmente la memoria
occupata a discapito di una lievemente peggiore complessità computazionale. La complessità dipende dal load factor \(\lambda\)
o indice di riempimento medio della tabella hash. Il load factor \(\lambda\) rappresenta un trade-off tra lo spazio occupato in
memoria e la complessità computazionale, per \(\lambda\) grandi si avrà migliore complessità spaziale e peggiore complessità
computazionale, vicecersa per \(\lambda\) piccoli si avrà il contrario. In genere si sceglie \(\lambda < 0.9\) e se
\(\lambda \in O(1)\), la complessità finale è \(O(1)\). 

\subsubsection*{Implementazione con alberi binari di ricerca}
Supponendo che le chiavi provengano da un insieme totalmente ordinato, è possibile implementare la mappa attraverso particolari
alberi binari di ricerca (2,4-Tree o Red-Black Tree) in cui la ricerca e i tre metodi hanno complessità \(O(\log n)\).

\newpage

\section{Hash Table}
\subsection{Introduzione}
\subsubsection*{Definizione}
Una tabella hash è una struttura che possiede i seguenti elementi:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[1.] una funzione di hash \(h : \{\text{chiavi}\} \to [0,N-1]\)
	\item[2.] un bucket array \(A\) con capacità \(N\), in cui ogni bucket \(A[i]\) sono memorizzate tutte le entry \(<k,v>\) per
	cui \(h(k) = i\), con \(0 \leq i < N\)
	\item[3.] un metodo di risoluzione delle collisioni (chiavi distinte associate allo stesso bucket)
\end{itemize}

\subsubsection*{Funzione di hash}
Una buona funione di Hash deve essere il più possibile un processo random:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(P[h(k) = i] = 1/N\), ovvero la distribuzione delle chiavi dei bucket deve tendere all'uniforme
	\item[-] \(P[h(k) = i \; | \;  h(k') = j] = P[h(k) = i]\), ovvero gli assegnamenti devono essere indipendenti tra loro
\end{itemize}
Esempi di funzioni Hash per tipi built-in di Java:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \verb|byte|, \verb|short|, \verb|char|, \verb|int|: \(k \mapsto (\text{int})k\)
	\item[-] \verb|float| (32 bit): \(k \mapsto \text{Float.floatToIntBits}(k)\)
	\item[-] \verb|long| (64 bit): \(k \mapsto (\text{int})(k \gg 32 + (\text{int})k)\)
	\item[-] \verb|double| (64 bits): \(k \mapsto ((\text{int})(k \gg 32 + (\text{int})k)) \circ (\text{Double.doubleToIntBits}(k))\)
	\item[-] \verb|string|: due possibilità:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[-] Polinomial hash code: \(h(S) = \sum_{i=0}^{k-1} s_i \cdot a^{k-i-1}\) con \(a = 31, 33, 37, 39, 41\), Java usa \(a=31\)
		\item[-] Cyclic Shift: somma carattere per carattere e dopo ogni somma si esegue una rotazione a sinistra di 5 posizioni
		(più facile implementazione rispetto al calcolo delle potenze di \(a\))
	\end{itemize}
\end{itemize}

\subsubsection*{Funzione di compressione}
Per convertire un generico numero \(\in \mathbb{N}\) generato dalla funzione di hash in un intero valido per accedere alla sequenza,
si utilizzano due funzioni di compressione:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{Division Method}: \(i \mapsto i \mod N\) \\
	è bene scegliere \(N\) primo e distante da potenze di \(2\): se \(N = 2^p\) è come considerare solo i \(p\) bit meno
	significativi, se \(N = 10^p\) è come considerare solo le \(p\) cifre meno significative in base 10.
	\item[-] \textbf{Multiply-Add-Divide MAD}: \(i \mapsto [(ai+b) \mod p] \mod N\) \\
	con \(p\) primo, \(p > N\), \(a,b \in [0,p-1]\) scelti a caso, \(a > 0\), più costoso ma con migliore distribuzione
\end{itemize}

\subsubsection*{Risoluzione delle collisioni}
Per la risoluzione delle collisioni ricorre a due metodi:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{Separate Chaining}: ogni bucket è visto come una \(Map\) più piccola implementata come lista
	\item[-] \textbf{Open Adressing}: consiste nel salvare le entry direttamente nelle celle del bucket array, senza far ricorso
	a strutture aggiuntive, ma si complica la loro gestione; non affrontato a lezione
\end{itemize}

\newpage

\subsection{Algoritmi base: get, put, remove}
\subsubsection*{Accesso ad un elemento - get}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sulla ricerca della chiave nei bucket con funzione di hash e sulla ricerca nella lista concatenata
	associata al bucket in questione.
	\item[-] La complessità dell'algoritmo al caso medio è \(\Theta(1+\lambda)\).
\end{itemize}
\begin{algo}{get}{$k$}{chiave $k$ dell'elemento cercato e riferimento implicito al bucket array $A$}{valore della entry con chiave $k$}
	\If{($\exists$ entry $(k,x) \in A[h(k)]$))}
		\State return $x$
	\Else
		\State return null
	\EndIf
\end{algo}

\subsubsection*{Inserimento di un elemento - put}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sulla ricerca della chiave nei bucket con funzione di hash e sulla ricerca nella lista concatenata
	associata al bucket in questione.
	\item[-] La complessità dell'algoritmo al caso medio è \(\Theta(1+\lambda)\).
\end{itemize}
\begin{algo}{put}{$k$, $v$}{entry $(k,v)$ da inserire e riferimento implicito al bucket array $A$}{vecchio valore della entry $(k,v)$ se presente}
	\If{($\exists$ entry $(k,x) \in A[h(k)]$))}
		\State sostituisci $x$ con $v$
		\State return $x$
	\Else
		\State inserisci $(k,v)$ in coda al bucket $A[h(k)]$
		\State incrementa di 1 la $size$ della hashTable
		\State return null
	\EndIf
\end{algo}

\subsubsection*{Rimozione di un elemento - remove}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sulla ricerca della chiave nei bucket con funzione di hash e sulla ricerca nella lista concatenata
	associata al bucket in questione.
	\item[-] La complessità dell'algoritmo al caso medio è \(\Theta(1+\lambda)\).
\end{itemize}
\begin{algo}{get}{$k$}{chiave $k$ dell'elemento cercato e riferimento implicito al bucket array $A$}{valore della entry con chiave $k$}
	\If{($\exists$ entry $(k,x) \in A[h(k)]$))}
		\State rimuovi $(k,x)$ da \(A[h(k)]\)
		\State decrementa di 1 la $size$ della hashTable
		\State return $x$
	\Else
		\State return null
	\EndIf
\end{algo}

\subsection{Load Factor e complessità}
\subsubsection*{Load Factor}
Per una tabella hash di capacità \(N\), con \(n\) entry memorizzate, il load factor è \(\lambda := \displaystyle\frac{n}{N}\). \\ 
Rappresenta la lunghezza media delle liste nei bucket.

\subsubsection*{Complessità al caso pessimo}
Si osserva che per una tabella hash di capacità \(N\), con \(n\) entry memorizzate, la complessità dei tre metodi al caso pessimo
è \(\Theta(n)\). Se tutte le entry fossero contenute nello stesso bucket la ricerca (prevista da ogni metodo) di una chiave in una
lista è \(\Theta(n)\). Il risultato di \(n\) inserimenti nella tabella diventerà \(\Theta(n^2)\).

\subsubsection*{Complessità al caso medio}
Al caso medio, il costo di un inserimento si ottene considerando il costo della ricerca della chiave \(k\) in nella lista \(i\)
contenente \(n_i\) entry, ovvero \(O(n_i + 1)\). Si ha, quindi:
\[O\left(\frac{1}{N} \sum_{i=0}^{N-1} (n_i + 1)\right) = O\left(\frac{1}{N} \sum_{i=0}^{N-1} n_i + \frac{1}{N} \sum_{i=0}^{N-1} 1\right) = O(\lambda + 1)\]
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se \(\lambda \in O(1)\), allora la complessità finale sarà \(\Theta(1)\)
	\item[-] in generale si mantiene \(\lambda < 0.9\), in Java \(\lambda \leq 0.75\)
	\item[-] il load factor \(\lambda\) rappresenta un trade-off tra lo spazio occupato in memoria e la complessità computazionale,
	per \(\lambda\) grandi si avrà migliore complessità spaziale e peggiore complessità computazionale, vicecersa per \(\lambda\)
	piccoli si avrà migliore complessità computazionale, ma peggiore complessità spaziale.
\end{itemize}

\subsection{Rehashing}
\subsubsection*{Funzionamento}
Con il popolamento della hashTable, il load factor cresce e si rischia di superare la soglia impostata. Per mantenere un load
factor accettabile, è necessario eseguire un ridimensionamento del bucket array e di conseguenza è necessario eseguire un rehashing
delle entry. Per ottenere un buon rehash è bene scegliere la nuova dimensione \(N' > 2N\) con \(N'\) numero primo.

\subsubsection*{Complessità}
Siccome so già che le chiavi in una hash table sono distinte, quando le inserisco nel nuovo bucket array ridimensionato non ho
bisogno di eseguire la ricerca per verificare se esiste già una entry con la stessa chiave. Questo bypass mi permette di eseguire
gli \(n\) inserimenti individualmente in tempo costante e complessivamente il rehash ha una complessità di \(\Theta(n)\).

Il rehash viene eseguito solo dopo \(n\) inserimenti con load factor ottimale che possiedono un costo aggregato di \(\Theta(n \lambda)
= \Theta(n)\). Il costo del rehash (\(\Theta(n)\)) viene, ammortizzato da quello aggregato dei precedenti \(n\) inserimenti.

\newpage

\section{Alberi binari di ricerca}
\subsection{Alberi binari di ricerca in generale}
\subsubsection*{Definizione}
Un albero binario di ricerca è un albero binario in cui i nodi interni memorizzano entry con chiavi provenienti da un universo
ordinato e per ogni nodo \(v\) con chiave \(k\), le chiavi di \(T\).left(\(v\)) sono \(< k\), mentre le chiavi di
\(T\).right(\(v\)) sono \(> k\). Le foglie sono sentinelle e non contengono entry.

\subsubsection*{Algoritmo di ricerca}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sull'ordinamento reciproco dei nodi all'interno dell'albero ed esegue una discesa dalla radice
	fino al nodo che contiene la entry cercata o alla foglia dove inserirla.
	\item[-] La complessità dell'algoritmo è \(\Theta(h)\), che può diventare \(\Theta(n)\) se l'albero è molto sbilanciato.
\end{itemize}
\begin{algo}{TreeSearch}{$k$, $v$}{chiave $k$ della entry cercata, nodo $v$ da cui cercare e riferimento implicito all'ABR $T$}{nodo in $T_v$ dove è presente la entry con chiave $k$ o dove va inserita}
	\If{($T$.isExternal($v$) OR $v$.getElement().getKey() = $k$)}
		\State return $v$
	\EndIf
	\If{($k < v$.getElement().getKey())}
		\State return TreeSearch($k$,$T$.left($v$))
	\Else
		\State return TreeSearch($k$,$T$.right($v$))
	\EndIf
\end{algo}

\subsubsection*{Limiti degli alberi binari di ricerca in generale}
Il problema degli alberi binari di ricerca è che la complessità dei vari metodi varia tra \(\Theta(\log n)\) se l'albero è
perfettamente bilanciato e \(\Theta(n)\) se l'albero è fortemente sbilanciato. In quest'ultimo caso l'ABR non migliora le
prestazioni di una comune lista concatena. Le possibili soluzioni sono:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] ribilanciare l'albero quando lo sbilanciamento supera una soglia prefissata (AVL o Red-Black Tree)
	\item[-] rendere i nodi più capienti per assorbire meglio gli effetti di inserimento e rimozione (MW Search Tree o (2,4)-Tree)
\end{itemize}

\subsubsection*{Algortimo di accesso - get}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sulla ricerca vista sopra.
	\item[-] La complessità dell'algoritmo è \(\Theta(h)\), che può diventare \(\Theta(n)\) se l'albero è molto sbilanciato.
\end{itemize}
\begin{algo}{get}{$k$}{chiave $k$ della entry cercata e riferimento implicito all'ABR $T$}{valore della entry con chiave $k$}
	\State $w \gets$ TreeSearch($k$, $T$.root())
	\If{($T$.isInternal($w$))}
		\State return $w$.getElement().getValue()
	\Else
		\State return null
	\EndIf
\end{algo}

\subsubsection*{Algortimo di inserimento - insert}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sulla ricerca vista sopra.
	\item[-] La complessità dell'algoritmo è \(\Theta(h)\), che può diventare \(\Theta(n)\) se l'albero è molto sbilanciato.
\end{itemize}
\begin{algo}{put}{$k$, $v$}{entry $(k,v)$ da inserire e riferimento implicito all'ABR $T$}{vecchio valore della entry $(k,v)$ se presente}
	\State $w \gets$ TreeSearch($k$, $T$.root())
	\If{($T$.isInternal($w$))}
		\State $x \gets w$.getElement().getKey()
		\State sostituisci $v$ con $x$ nella entry $w$.getElement()
		\State return $x$
	\Else
		\State expandExternal($w$,$(k,v)$)
		\State incrementa di 1 il numero di entry di $T$
		\State return null
	\EndIf
\end{algo}

\subsubsection*{Algortimo di rimozione - remove}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si sviluppa in due casi:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[1.] se il nodo da rimuovere ha una foglia come figlio, faccio salire l'altro figlio
		\item[2.] se il nodo da rimuovere non ha foglie come figli, sostituisco il nodo da rimuovere \(w\) con il precedente nella
		visita inorder \(y\) (che avrà una foglia come figlio destro) e sposto il sottoalbero sinistro di \(y\) al posto di \(y\)
	\end{itemize}
	\item[-] La complessità dell'algoritmo è \(\Theta(h)\), ottenuta dalla complessità aggregata di TreeSearch (\(\Theta(h)\)),
	del caso 1 (\(\Theta(1)\)) e del caso 2 (\(\Theta(h)\)), più altre operazioni (\(\Theta(1)\)).
\end{itemize}
\begin{algo}{remove}{$k$}{chiave $k$ della entry da rimuovere e riferimento implicito all'ABR $T$}{valore della entry rimossa}
	\State $w \gets$ TreeSearch($k$, $T$.root())
	\If{($T$.isInternal($w$))}
		\State $avlue \gets w$.getElement().getValue()
		\State decrementa di 1 il numero di entry di $T$
		\State \Comment Caso 1: $w$ interno con almento un figlio foglia
		\If{($T$.isExternal($T$.left($w$)) OR $T$.isExternal($T$.right($w$)))}
			\State $u_L \gets T$.left($w$) $\;\; - \;\;$ $u_R \gets T$.right($w$)
			\If{$T$.isExternal($u_L$)} \Comment Caso 1.1: la foglia è il figlo sinistro
				\State cancella $w$ e $u_L$
				\State fai salire $u_R$ al posto di $w$
			\Else \Comment Caso 1.2: la foglia è il figlo destro
				\State cancella $w$ e $u_R$
				\State fai salire $u_L$ al posto di $w$
			\EndIf
		\Else \Comment Caso 2: $w$ interno con due figli
			\State $y \gets$ nodo con chiave massima del sottoalbero di sinistra
			\State sposta il figlio sinistro di $y$ al posto di $y$
			\State sostituisci il nodo $w$ da rimuovere con il nodo $y$
		\EndIf
		\State return $value$
	\Else
		\State return null
	\EndIf
\end{algo}

\newpage

\subsection{Multi-Way Search Tree - MWST}
\subsubsection*{Definzinione}
Un  Multi-Way Search Tree è un albero ordinato tale che ogni nodo interno ha \(d \geq\) 2 figli (\(v_1, v_2, \dots v_d\)) detto
\(d-node\) che soddisfa le seguenti caratteristiche:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] memorizza \(d-1\) entry \((k_1,x_1), (k_2,x_2), \dots (k_{d-1}, x_{d-1})\) con \(k_1 < k_2 < \dots < k_{d-1}\)
	\item[-] la chiave di ogni entry \(e \in T_{v_i}\) soddisfa \(k_{i-1} < e.\text{getKey}() < k_{i+1}\), supponendo \(k_0 = 0\), \(k_d = +\infty\)
\end{itemize}

\subsubsection*{Proprietà}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] per ogni nodo \(v\) con ad esempio 3 entry \((k_1, x_1), (k_2, x_2), (k_3, x_3)\) e 4 figli \(v_1, v_2, v_3, v_4\),
	le chiavi sono ordinate da sinistra verso destra nel seguente modo:
	\subitem \(0 < \{\text{chiavi} \in T_{v_1}\} < k_1 < \{\text{chiavi} \in T_{v_2}\} < k_2 < \{\text{chiavi} \in T_{v_3}\} < k_3 < \{\text{chiavi} \in T_{v_4}\} < +\infty\)
	\item[-] un MWST con \(n\) entry ha \(n+1\) foglie
	\item[-] se tutti i nodi fossero dei \(d-\)node, allora l'albero viene detto \(d\)-ario, con:
	\subitem \# nodi interni = \(x\), \(\qquad\) \# entry = \((d-1)x\), \(\qquad\) \# foglie = \((d-1)x + 1\)
\end{itemize}

\subsubsection*{Algoritmo di ricerca}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sull'ordinamento reciproco dei nodi all'interno dell'albero ed esegue una discesa dalla radice
	fino al nodo che contiene la entry cercata o alla foglia dove inserirla.
	\item[-] La complessità dell'algoritmo è \(\Theta(h \cdot d_{max})\), con \(d_{max} = \) massimo valore di \(d\) in \(T_v\)  
\end{itemize}
\begin{algo}{MWTreeSearch}{$k$, $v$}{chiave $k$ della entry cercata, nodo $v$ da cui cercare e riferimento implicito al MWST $T$}{nodo in $T_v$ dove è presente la entry con chiave $k$ o dove va inserita}
	\If{($T$.isExternal($v$))}
		\State return $v$
	\EndIf
	\State trova $i$ tale che $k_{i-1} < k \leq k_i$ con $k_0 = 0, k_d = +\infty$
	\If{($k = k_i$)}
		\State return v
	\Else
		\State return MWTreeSearch($k$,$v_i$)
	\EndIf
\end{algo}

\subsubsection*{Algortimo di accesso - get}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sulla ricerca vista sopra.
	\item[-] La complessità dell'algoritmo è \(\Theta(h \cdot d_{max})\).
\end{itemize}
\begin{algo}{get}{$k$}{chiave $k$ della entry cercata e riferimento implicito al MWST $T$}{valore della entry con chiave $k$}
	\State $w \gets$ MWTreeSearch($k$, $T$.root())
	\If{($T$.isInternal($w$))}
		\State trova $e \in w$ tale che $e$.getKey() = $k$
		\State return $e$.getValue()
	\Else
		\State return null
	\EndIf
\end{algo}

\newpage

\subsection{(2,4)-Tree}
\subsubsection*{Definzinione}
Un (2,4)-Tree è un MWS-Tree tale che:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] ogni nodo interno è un \(d\)-node con \(2 \leq d \leq 4\) con \(d\) figli e \(d-1\) entry
	\item[-] tutte le foglie hanno la stessa profondità
\end{itemize}

\subsubsection*{Proprietà}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] l'altezza di un (2,4)-Tree con \(n>0\) entry è \(\Theta(\log n)\)
	\item[-] la complessità dei metodi di ricerca e di accesso è \(\Theta(\log n)\)
\end{itemize}

\subsubsection*{Algoritmo di inserimento - put}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sulla ricerca vista sopra e sul fatto che un nodo più contenete più entry
	\item[-] La complessità dell'algoritmo è \(\Theta(\log n)\) data dalla MWTreeSearch (\(\Theta(\log n)\)) e dallo split (\(\Theta(\log n)\))
\end{itemize}
\begin{algo}{put}{$k$, $x$}{entry $(k,x)$ da inserire e riferimento implicito al (2,4)-Tree $T$}{valore della entry con chiave $k$}
	\State $w \gets$ MWTreeSearch($k$, $T$.root())
	\If{($T$.isInternal($w$))} \Comment Caso 1: se esiste già una entry con chiave $k$
		\State $e \gets $ entry in $w$ con $e$.getKey() = $k$
		\State $y \gets e$.getValue()
		\State sostituisci $x$ con $y$ in $e$.value()
		\State return $y$
	\Else \Comment Caso 2: bisogna inserire una nuova entry
		\State $e \gets (k,x)$
		\If{($T$.isRoot($w$))} \Comment Caso 2.1: l'albero è vuoto $\rightarrow$ creo radice e due figli
			\State expandExternal($w$,$e$)
		\Else \Comment Caso 2.2: lo inserisco nel padre
			\State $u \gets T$.parent($w$)
			\State inserisci $e$ in $u$ aggiungendo una foglia $w'$
			\If{$u$ è un 5-node} \Comment se c'è overflow, invoco split
				\State split($u$)
			\EndIf	
		\EndIf
		\State incrementa di 1 il numero delle entry in $T$
		\State return null
	\EndIf
\end{algo}

\newpage

\subsubsection*{Algoritmo di split}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo propaga l'overflow verso l'alto e se serve aumenta l'altezza dell'albero:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[1.] se ho la radice in overflow, spezzo la radice in due nodi: \((e1, e_2, e_3, e_4) \rightarrow (e_1,e_2), (e_4)\)
		e creo una nuova radice con la entry \say{centrale} \((e_3)\) e con i due nodi \((e_1,e_2), (e_4)\) come figli
		\item[2.] altrimenti spezzo il nodo in due \((e1, e_2, e_3, e_4) \rightarrow (e_1,e_2), (e_4)\) e inserisco la entry
		\say{centrale} \((e_3)\) nel padre; se il padre è in overflow, invoco lo split sul padre
	\end{itemize}
	\item[-] La complessità dell'algoritmo è \(\Theta(\log n)\) in quanto ogni invocazione ha costo \(O(1)\) e \# incocazioni
	è proporzionale all'altezza dell'albero \(\Theta(\log n)\)
\end{itemize}
\begin{algo}{split}{$u$}{nodo in overflow e riferimento implicito al (2,4)-Tree $T$}{albero senza nodi in overflow}
	\State sia $u = (e1, e_2, e_3, e_4)$ con figli $u_1, u_2, u_3, u_4, u_5$
	\State creo due nodi $u' = (e_1, e_2)$ con figli $u_1, u_2, u_3$ e $u'' = (e_4)$ con figli $u_4, u_5$
	\If{($T$.isRoot($u$))} \Comment se ho la radice in overflow
		\State crea una nuova radice contenente $e_3$ e con figli $u'$ e $u''$ e cancella $u$
	\Else \Comment se ho un generico nodo in overflow
		\State $v \gets T$.parent($u$)
		\State inserisci $e_3$ in $v$ con figli $u'$ e $u''$ e cancella $u$
		\If{($v$ è un 5-node)} \Comment se il padre è in overflow
			\State split($u$)
		\EndIf
	\EndIf
\end{algo}

\subsubsection*{Algortimo di rimozione - remove}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo si basa sulla ricerca vista sopra e sul fatto che un nodo più contenete più entry
	\item[-] La complessità dell'algoritmo è \(\Theta(\log n)\) data dalla MWTreeSearch (\(\Theta(\log n)\)) e dal delete (\(\Theta(\log n)\))
\end{itemize}
\begin{algo}{remove}{$k$}{chiave $k$ della entry da rimuovere e riferimento implicito al (2,4)-Tree $T$}{valore della entry rimossa}
	\State $w \gets$ MWTreeSearch($k$, $T$.root())
	\If{($T$.isInternal($w$))} \Comment Caso 1: se esiste la entry con chiave $k$
		\State $e \gets $ entry in $w$ con $e$.getKey() = $k$
		\State $y \gets e$.getValue()
		\If{(height($w$) = 1)} \Comment Caso 1.1: se la entry si trova alla base (non ha nodi figli)
			\State delete($e$, $w$)
		\Else \Comment Caso 1.2: se la entry ha nodi figli
			\State $v \gets$ figlio di $w$ a sx di $e$
			\State $e' \gets$ entry con chiave massima in $T_v$
			\State $z \gets$ nodo contenente $e'$
			\State copia $e'$ al posto di $e$ in $w$ \Comment sostituisco il nodo da eliminare
			\State delete($e'$,$z$) \Comment elimino il sostituito dalla sua posizione originale
		\EndIf
		\State decrementa di 1 il numero delle entry in $T$
		\State return $y$
	\Else \Comment Caso 2: se la entry non esiste
		\State return null
	\EndIf
\end{algo}

\newpage

\subsubsection*{Algoritmo di delete}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo propaga l'underflow verso l'alto e se serve diminuisce l'altezza dell'albero:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[1.] se ho la radice in underflow: imposto come nuova radice il nodo non foglia
		\item[2.] se il nodo in underflow ha un fratello con \(d = 3,4\): vado in prestito di una entry dal fratello ed eseguo
		una rotazione delle entry fratello \(\rightarrow\) padre \(\rightarrow\) nodo in underflow
		\item[3.] se il nodo in underflow ha un fratello con \(d = 2\): vado in prestito dal padre e sposto la entry che lega i
		due fratelli dal padre al fratello,	aggiungo il sottoalbero del nodo in underflow al fratello e richiamo ricorsivamente
		delete sul padre per la entry spostata
	\end{itemize}
	\item[-] La complessità dell'algoritmo è \(\Theta(\log n)\) in quanto ogni invocazione ha costo \(O(1)\) e \# incocazioni
	è proporzionale all'altezza dell'albero \(\Theta(\log n)\)
\end{itemize}
\begin{algo}{delete}{$u$, $e$}{nodo $u$, entry $e \in u$ da rimuovere e riferimento implicito al (2,4)-Tree $T$}{albero senza nodi in underflow}
	\State rimuove $e$ da $u$ e figlio vuoto o foglia discriminata da $e$
	\State applico i casi visti sopra
\end{algo}

\vspace{4em}

\subsection{Red-Black Tree}
\subsubsection*{Definzinione}
Un Red-Black Tree (RB-Tree) è un albero binario di ricerca i cui nodi hanno un colore rosso o nero e in cui valgono le seguenti
proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textit{Root Property}: la radice è nera
	\item[-] \textit{External Property}: le foglie sono nere
	\item[-] \textit{Red Property}: i figli di un nodo rosso sono necessario
	\item[-] \textit{Depth Property}: tutte le foglie hanno la stessa Black Depth
\end{itemize}

\subsubsection*{Correlazione tra (2,4)-Tree e Red-Black Tree}
Si osseerva che è possibile trasformare un (2,4)-Tree in un Red-Black Tree e viceversa:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se ho un solo nodo nero e due foglie \(\rightarrow\) 2-nodo con una entry e due foglie
	\item[-] se ho un nodo nero con un figlio rosso e tre foglie totali \(\rightarrow\) 3-nodo con due entry e 3 foglie
	\item[-] se ho un nodo nero con due figli rossi e quattro foglie totali \(\rightarrow\) 4-nodo con tre entry e 4 foglie
\end{itemize}

\newpage

\section{Multimappe}
\subsection{Introduzione}
\subsubsection*{Definizione}
Una multimappa è una mappa che elimina il vincolo di unicità delle chiavi

\subsubsection*{Metodi e interfaccia in Java}
\begin{lstlisting}[language=Java]
public interface Map<K,V>{
	int size();
	boolean isEmpty();
	/** Returns a collection of all the values of the entrys (k,*) with k = key */
	Iterable<V> get (K key);
	/** Inserts a new entry (key, value) */
	void put (K key, V value);
	/** Removes the entry (k,v) with k = key and returns true if an entry is removed, otherwise false */
	boolean remove (K key, V value);
	Iterable<K> keySet();
	Iterable<V> values();
	Iterable<Entry<K,V>> entrySet();
}
\end{lstlisting}

\subsubsection*{Indice primario e secondario}
Nei database, l'accesso ai dati è reso efficiente attraverso l'accesso ad un indice primario realizzato tramite una mappa e
in parallelo attraverso un indice secondario realizzato tramite una multimappa.

\subsection{Implementazione e relativa complessità}
\subsubsection*{Implementazione}
È possibile implementare una multimappa tramite una mappa in cui le entry sono coppie \((k, L_k)\) con \(k\) chiave e \(L_k\) una
lista che memorizza i valori associati alla chiave. Nel caso in cui si vuole inserire più entry uguali (stessa chiave e valore),
si può decidere se bloccare l'inserimento della seconda o memorizzarle entrambe. In quest'ultimo caso, al momento della rimozione
si può decidee se rimuoverne solo una o tutte le entry uguali.

\subsubsection*{Complessità}
Per migliore praticità si definisce \(n =\) \# chiavi distinte e \(s = \max_k |L_k|\) massima lunghezza delle liste associate
alle chiavi. Di seguito i metodi e le relative complessità:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] get(k) ha complessità \(\Theta(\log n)\) (come quella della mappa, utlizzando la nuova definizione di \(n\))
	\item[-] put(k,v) ha complessità \(\Theta(\log n)\) (come quella della mappa, utlizzando la nuova definizione di \(n\))
	\item[-] remove(k,v) ha complessità \(\Theta(\log n + s)\)
\end{itemize}

\newpage

\section{Grafi}
\subsection{Introduzione}
\subsubsection*{Definizione}
Un grafo \(G\) è definito come una coppia di insiemi (o collezioni se esistono elementi duplicati) \((V, E)\) tali che \(V\)
è l'insieme di vertici e \(E\) è una collezione di archi (o coppie di vertici). 

\subsubsection*{Altre definizioni - concetti fondamentali}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{arco orientato}: un arco \((u,v) \in E\) si dice orientato se è stato definito un orientamento \((u \to v)\)
	tra i vertici su cui insiste
	\item[-] \textbf{arco multiplo}: un arco si dice multiplo se compare più volte nell'insieme \(E\)
	\item[-] \textbf{self loop}: un arco si dice self loop se insisiste su un unico vertice, ad esempio \((u,u) \in E\)
	\item[-] \textbf{vertici adiacenti}: due vertici \(u\), \(v\) si dicono adiacenti se esiste un arco \((u,v) \in E\) che li collega
	\item[-] \textbf{vicini di un vertice}: i vicini di un vertice \(v\) sono tutti i vertici \(u\) tali che \((u,v) \in E\)
	\item[-] \textbf{grado di un vertice}: il grado di un vertice \(v \in V\), degree(\(v\)), è il numero di archi incidenti su \(v\)
	\item[-] \textbf{grafo diretto}: un grafo si dice diretto se ogni arco in \(E\) è orientato
	\item[-] \textbf{grafo non diretto}: un grafo si dice non diretto se nessun arco in \(E\) è orientato
	\item[-] \textbf{grafo semplice}: un grafo si dice semplice se non ha archi multipli o self loop
	\vspace{1em}
	\item[-] \textbf{cammino}: il cammino è una sequenza ordinata di vertici adiacenti \(u_1, u_2, \dots u_k\) con \((u_i, u_{i+1}) \in E\)
	\item[-] \textbf{lunghezza di un cammino}: la lunghezza di un cammino è il numero di archi \((u_i, u_{i+1})\) o la somma dei
	pesi degli archi nel caso di archi pesati
	\item[-] \textbf{cammino semplice}: un cammino si dice semplice se non ha vertici ripetuti
	\item[-] \textbf{cammino minimo}: un cammino tra \(x\) e \(y\) si dice minimo se la sua lunghezza è la minima possibile
	\item[-] \textbf{distanza tra vertici}: dati due vertici \(x\), \(y\) la loro distanta distanza(\(x\),\(y\)) è pari alla lunghezza
	del cammino minimo che li congiunge, se non esiste cammino minimo, distanza(\(x\),\(y\)) = \(+\infty\)
	\item[-] \textbf{ciclo}: un ciclo è un cammino in cui \(u_1 = u_k\), la lunghezza del ciclo è analoga a prima
	\item[-] \textbf{ciclo semplice}: un ciclo si dice semplice se non ha vertici ripetuti ad eccezione degli estremi
	\vspace{1em}
	\item[-] \textbf{sottografo}: un sottografo di \(G\) è definito come \(G' = (V',E')\) tali che \(V' \subseteq V\),  \(E' \subseteq E\)
	\item[-] \textbf{sottografo di copertura}: un sottografo si dice di copertura (o spanning subgraph) se \(V' = V\)
	\item[-] \textbf{grafo connesso}: un grafo si dice connesso se \(\forall u,v \in V \; \exists\)cammino da \(u\) a \(v\)
	\item[-] \textbf{grafo disconnesso}: un grafo si dice disconnesso se \(\exists u,v \in V\) tali che distanza(\(u\),\(v\)) \(= +\infty\)
	\item[-] \textbf{componenti connesse}: le componenti connesse di un grafo \(G = (V,E)\) sono i sottografi connessi massimali
	di \(G\), ovvero i sottografi \(G_i = (V_i, E_i)\) con \(1 \leq i \leq k\) tali che:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[-] \(G_i = (V_i,E_i)\) è sottografo connesso
		\item[-] \(V = V_1 \cup V_2 \cup \dots \cup V_k\) con \(V_i \cap V_j = \varnothing\)
		\item[-] \(E = E_1 \cup E_2 \cup \dots \cup E_k\) con \(E_i \cap E_j = \varnothing\)
		\item[-] \(\forall i \neq j\) non esisto archi \(\in E\) tra \(V_i\) e \(V_j\)
	\end{itemize}
	\item[] Si osserva che se \(G\) è connesso, \(k = 1\), inoltre la partizione in componenti connesse è univoca
	\item[-] \textbf{albero (libero)}: un albero (libero) è un grafo \(G = (V,E)\) connesso e senza cicli
	\item[-] \textbf{albero radicato}: un albero radicato è un grafo \(G = (V,E)\) connesso e senza cicli tale che:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[-] esiste un vertice \(r \in V\) detto radice
		\item[-] per ogni \(u \in V \backslash \{r\}\) esiste un unico padre \(p(u) \in V\) e vale \(E = \{(u,p(u)) : u \in V \backslash \{r\}\}\)
		\item[-] per ogni \(u \in V\), risalendo di padre in padre si arriva a \(r\)
	\end{itemize}
	\item[] Si osserva che un albero radicato è un albero libero e un albero libero si può trasformare in radicato scegliendo
	un opportuno vertice come radice
	\item[-] \textbf{foresta}: una foresta è un grafo \(G\) non connesso, ovvero un insieme di alberi liberi disgiunti; si
	osserva che un albero è una foresta, ma una foresta non è un albero
	\item[-] \textbf{spanning tree}: uno spanning tree di un grafo \(G\) è uno spanning subgraph connesso e senza cicli
	\item[-] \textbf{spanning forest}: una spanning forest di un grafo \(G\) è uno spanning subgraph senza cicli
\end{itemize}

\subsubsection*{Primitive}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{traversal}: esplorazione del grafo (es. crawling)
	\item[-] \textbf{connettività}: verifica se un grafo è connesso (es. reti wireless)
	\item[-] \textbf{identificazione componenti connesse}: (es. reti wireless)
	\item[-] \textbf{ricerca cammini minimi}: (es. navigatore)
	\item[-] \textbf{ricerca minimum spanning tree}: (es. broadcast efficiente)
	\item[-] \textbf{stima sulla distanza media/massima}: (es. social network)
\end{itemize}

\subsubsection*{Proprietà}
Sia \(G = (V,E)\) grafo semplice non diretto con \(|V| = n\) e \(|E| = n\), allora valgono le seguenti proprietà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \(\displaystyle \sum_{v \in V} \text{degree}(v) = 2m\)
	\item[-] \(m \leq \binom{n}{2}\), ovvero \(m \in O(n^2)\)
	\item[-] se \(G\) è albero, allora \(m = n - 1\)
	\item[-] se \(G\) è connesso, allora \(m \geq n-1\)
	\item[-] se \(G\) è senza cicli (foresta), allora \(m \leq n-1\)
\end{itemize}

\subsubsection*{Implementazione / rappresentazioni}
Un grafo può essere rappresentato/implementato in tre modi:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{struttre base}: \\
	utilizzo due liste \(L_V\) e \(L_E\) per rappresentare rispettivamente vertici e archi; questo metodo risulta scomodo negli
	algoritmi in cui è necessario esplorare i vertici adiacenti, per cui non viene molto utilizzata
	\item[-] \textbf{liste di adiacenza}: \\
	per ogni vertice \(v \in V\) ho una lista \(L(v)\) con i puntatori agli archi incidenti su \(v\) o ai vertici raggiungibili
	da \(v\); questa rappresentazione occupa spazio lineare nella taglia del grafo \(\Theta(n+m)\) e consente l'accesso sequenziale
	ai vertici adiacenti in tempo lineare al grado di \(v\)
	\item[-] \textbf{matrice di adiacenza}: \\
	matrice \(n \times n\) in cui le righe e le colonne sono in corrispondenza 1:1 con i vertici e ogni cella è popolata secondo
	\(A[i_1,i_2] := \begin{cases} null & \text{se} \; (i_1, i_2) \notin E \\ \text{puntatore a } e = (i_1, i_2) \in E & \text{se}\; \; (i_1, i_2) \in E \end{cases}\).
	Si osserva che in un grafo non diretto la matrice è simmetrica. Inoltre permette l'accesso ad un arco in tempo costante, ma
	richiede che i vertici siano rappresentati come interi. Inoltre occupa uno spazio \(\Theta(n^2)\), superlineare nella taglia
	del grafo che risulta vantaggiosa solo nel caso di grafi densi o per grafi con pochi vertici.
	\item[-] \textbf{mappe di adiacenza}: \\
	una via di mezzo tra le liste e la matrice di adiacenza, per ogni vertice gli archi incidenti su tale vertice sono memorizzati
	in una mappa (con tabella hash) per cui si ha un tempo di accesso medio lineare \(O(1)\) e occupa uno spazio lineare nella
	taglia del grafo.
\end{itemize}

\newpage

\subsection{Breadth First Search - BFS}
\subsubsection*{Algoritmo}
L'algoritmo di Breath First Search o BFS prevede che:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] prima si visitano tutti i vertici adiacenti e infine si passa ai vicini dei vicini
	\item[-] ogni vertice ha un campo \(I\!D\) che vale 1 se è stato visitato, 0 altrimenti
	\item[-] ogni arco ha un campo \(e.label\) per memorizzare una opportuna etichetta o null altrimenti
	\item[-] incidentEdges(\(v\)) resituisce un iteratore agli archi incidenti su \(v\)
	\item[-] opposite(\(v\), \(e\)), con \(e = (v,w)\) arco insistente su \(v\), resituisce \(w\)
\end{itemize}
Durante l'esecuzione della BFS,
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] vengono visitati tutti i vertici in \(C_s\) componente connessa che contiene \(s\)
	\item[-] ogni arco in \(C_s\) viene etichettato come \verb|DISCOVERY EDGE| o \verb|CROSS EDGE|:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[] \verb|DISCOVERY EDGE| se l'arco è utilizzato per scoprire un nuovo vertice
		\item[] \verb|CROSS EDGE| se l'arco porta ad un vertice già visitato
	\end{itemize}
	\item[-] partiziona \(C_s\) in livelli \(L_i\) in base alla distanza \(i\) dei vertici rispetto a \(s\)
\end{itemize}

\begin{algo}{BFS}{$G$, $s$}{grafo $G = (V,E)$, $s \in C_s$, vertici con \(v.I\!D = 0\), archi con \(e.label = null\)}{etichetta opportunamente ogni vertice e arco in \(C_s\)}
	\State visita \(s\) e \(s.I\!D \gets 1\)
	\State \(L_0 \gets s\) (lista che contiene \(s\))
	\State \(i \gets 0\)
	\While{(! \(L_i\).isEmpty())}
		\State crea una lista \(L_{i+1}\) vuota
		\ForAll{\(v\)}{\(L_i\)}
			\ForAll{\(e\)}{\(G\).incidentEdges(\(v\))}
				\If{(\(e.label =\) null)} \Comment se trovo un arco non ancora visitato
					\State \(w \gets G\).opposite(\(v\), \(e\)) \Comment leggo il vertice opposto
					\If{(\(w.I\!D = 0\))} \Comment se vertice ancora da visitare
						\State \(e.label \gets\) \verb|DISCOVERY EDGE| \Comment \(e = (v,w)\) diventa DISCOVERY
						\State visita vertice \(w\) e \(w.I\!D = 1\) \Comment visito \(w\)
						\State inserisci \(w\) in \(L_{i+1}\) \Comment inserico \(w\) nella rispettiva lista
					\Else \Comment se vertice già visitato
						\State \(e.label \gets\) \verb|DISCOVERY EDGE|
					\EndIf
				\EndIf
			\EndFor
		\EndFor
		\State \(i \gets i+1\) \Comment passo al livello successivo
	\EndWhile
\end{algo}

\subsubsection*{Correttezza e risultati}
Alla fine dell'esecuzione della BFS su una componente connessa \(C_s\) si ottiene:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] quanto previsto di base dall'algoritmo (vedi sopra)
	\item[-] i \verb|DISCOVERY EDGE| formano uno spanning tree di \(C_s\) radicato in \(s\) chiamato BFS tree
	\item[-] se un arco marcato \verb|CROSS EDGE| separa vertici i cui indici di livello differiscono al più di 1
	\item[-] se un vertice \(v\) appartiene alla lista \(L_i\), allora \(d(s,v) = i\)
\end{itemize}

\subsubsection*{Complessità}
La complessità di BFS è proporzionale al numero di archi della componente connessa \(C_s\):
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se il grafo non è connesso, allora la complessità è \(\Theta(m_s)\), con \(m_s = \) \# archi di \(C_s\)
	\item[-] se il grafo è connesso, allora la complessità è \(\Theta(|E|)\)
\end{itemize}

\subsubsection*{Generalizzazione su tutto il grafo}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo generalizza la BFS per visitare totalmente un grafo non connesso
	\item[-] La complessità dell'algoritmo è \(\Theta(|E| + |V|)\), supponendo di poter accedere agli elementi di \(V\) e \(E\)
	in tempo lineare.
\end{itemize}
\begin{algo}{BFSComplete}{$G$}{grafo $G = (V,E)$}{etichetta opportunamente ogni vertice e arco in \(G\)}
	\ForAll{$v$}{$V$}
		\State \(v.I\!D \gets 0\)
	\EndFor
	\ForAll{$e$}{$E$}
		\State \(e.label \gets null\)
	\EndFor
	\ForAll{$v$}{$V$}
		\If{\(v.I\!D = 0\)}
			\State BFS(\(G\), \(v\))
		\EndIf
	\EndFor
\end{algo}

\subsubsection*{Connettività}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo utilizza BFS modificato per contare le componenti connesse di un grafo \(G\)
	\item[-] La complessità dell'algoritmo è \(\Theta(|E| + |V|)\), come per BFSComplete.
	\item[-] Si usa la BFS modificata, che accetta un terzo parametro \(ID\) utilizzato per marcare i vertici visitati
\end{itemize}
\begin{algo}{Connettività}{$G$}{grafo $G = (V,E)$}{restituisce \# componenti connesse ed etichetta i vertici con \(I\!D\) distinti}
	\ForAll{$v$}{$V$}
		\State \(v.I\!D \gets 0\)
	\EndFor
	\ForAll{$e$}{$E$}
		\State \(e.label \gets null\)
	\EndFor
	\State \(i \gets 0\)
	\ForAll{$v$}{$V$}
		\If{\(v.I\!D = 0\)}
			\State \(i \gets i+1\)
			\State BFS(\(G\), \(v\), \(i\))
		\EndIf
	\EndFor
	\State return \(i\)
\end{algo}

\subsubsection*{Spanning Tree}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo utilizza la BFS per ottenere lo spanning tree di un grafo connesso
	\item[-] La complessità dell'algoritmo è \(\Theta(|E|)\), come per BFS normale.
	\item[-] Si usa la BFS modificata, che accetta (e restituisce) una lista \(L\) dove salvare i \(\verb|DISCOVERY EDGES|\)
\end{itemize}
\begin{algo}{SpanningTree}{$G$, $s$}{grafo $G = (V,E)$ connesso, $s \in V$}{lista $L$ con gli archi marcati come $\texttt{DISCOVERY EDGES}$}
	\State \(L \gets\) lista vuota, \(s \gets\) vertice qualsiasi di \(V\) 
	\State return BFS(\(G\),\(s\),\(L\))
\end{algo}

\subsubsection*{Cammini minimi}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo utilizza la BFS per ottenere un cammino minimo tra due vertici
	\item[-] La complessità dell'algoritmo è \(\Theta(|E| + |V|)\), come per BFSComplete
	\item[-] Si usa la BFS modificata che per ogni vertice usa \(v.parent\) per il padre e \(v.edge\) per l'arco al padre
\end{itemize}
\begin{algo}{}{$G$, $s$, $t$}{grafo $G = (V,E)$ connesso, $s,t \in V$}{lista $L$ con gli archi del cammino minimo da \(t\) a \(s\)}
	\ForAll{$v$}{$V$}  \(v.I\!D \gets 0, \quad v.parent \gets \text{null}, \quad v.edge \gets \text{null}\)
	\EndFor
	\ForAll{$e$}{$E$} \(e.label \gets \text{null}\)
	\EndFor
	\State BFS(\(G\),\(s\))
	\If{(\(t.I\!D = 0\))} return null
	\Else
		\State \(L \gets\) lista vuota, e \(w \gets t\)
		\While{(\(w \neq s\)))}
			\State aggiungi \(w.edge\) a \(L\)
			\State \(w \gets w.parent\)
		\EndWhile
		\State return \(L\)
	\EndIf
\end{algo}

\subsubsection*{Ciclicità}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo utilizza la BFS per ottenere un ciclo del grafo, se esiste
	\item[-] La complessità dell'algoritmo è \(\Theta(|E| + |V|)\), come per BFSComplete
	\item[-] Si usa la BFS modificata che:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[-] per ogni vertice usa \(v.parent\) per il padre e \(v.edge\) per l'arco al padre
		\item[-] negli \(I\!D\) di ciascun vertice salvo il livello/distanza dal vertice di partenza
		\item[-] restituisce un cross edge appena lo trova, altrimenti restituisce null
	\end{itemize}
\end{itemize}
\begin{algo}{}{$G$}{grafo $G = (V,E)$ connesso}{lista $L$ con gli archi di un ciclo in \(G\)}
	\ForAll{$v$}{$V$} \(v.I\!D \gets 0, \quad v.parent \gets \text{null}, \quad v.edge \gets \text{null}\)
	\EndFor
	\ForAll{$e$}{$E$} \(e.label \gets \text{null}\)
	\EndFor
	\State \(crossEdge \gets\) BFS(\(G\),\(s\)), con \(s\) vertice generico in \(V\)
	\If{(\(crossEdge =\) null)} return null
	\Else
		\State \(L \gets\) lista con \(crossEdge\)
		\State \(u,v \gets\) vertici di \(crossEdge\)
		\If{(\(u.I\!D > v.I\!D\))} \Comment riporto i vertici allo stesso livello
			\State aggiungi \(u.edge\) a \(L, \quad u \gets u.parent\)
		\ElsIf{(\(u.I\!D < v.I\!D\))}
			\State aggiungi \(v.edge\) a \(L, \quad v \gets v.parent\)
		\EndIf
		\While{(\(u \neq v\)))} \Comment algoritmo modificato per LowestCommonAncestor
			\State aggiungi \(u.edge\) a \(L, \quad u \gets u.parent\)
			\State aggiungi \(v.edge\) a \(L, \quad v \gets v.parent\)
		\EndWhile
		\State return \(L\)
	\EndIf
\end{algo}

\newpage

\subsection{Depth First Search - DFS}
\subsubsection*{Algoritmo}
L'algoritmo di Depth First Search o DFS prevede che:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] prima viene visitato un vertice e ricorsivamente i suoi discendenti e infine si passa ai vertici adiacenti
	\item[-] ogni vertice ha un campo \(I\!D\) che vale 1 se È stato visitato, 0 altrimenti
	\item[-] ogni arco ha un campo \(e.label\) per memorizzare una opportuna etichetta o null altrimenti
	\item[-] incidentEdges(\(v\)) resituisce un iteratore agli archi incidenti su \(v\)
	\item[-] opposite(\(v\), \(e\)), con \(e = (v,w)\) arco insistente su \(v\), resituisce \(w\)
\end{itemize}
Durante l'esecuzione della DFS,
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] vengono visitati tutti i vertici in \(C_s\) componente connessa che contiene \(s\)
	\item[-] ogni arco in \(C_s\) viene etichettato come \verb|DISCOVERY EDGE| o \verb|BACK EDGE|:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[] \verb|DISCOVERY EDGE| se l'arco è utilizzato per scoprire un nuovo vertice
		\item[] \verb|BACK EDGE| se l'arco porta ad un vertice già visitato
	\end{itemize}
\end{itemize}

\begin{algo}{DFS}{$G$,$s$}{grafo $G = (V,E)$, $s \in C_s$, vertici con \(v.I\!D = 0\), archi con \(e.label = null\)}{etichetta opportunamente ogni vertice e arco in \(C_s\)}
	\State visita \(s\) e imposta \(s.I\!D = 1\)
	\ForAll{$e$}{$G$.incidentEdges($v$)}
		\If{(\(e.label =\) null)}
			\State \(w \gets G\).opposite(\(v\),\(e\))
			\If{(\(w.I\!D = 0\))}
				\State \(e.label \gets\) \verb|DISCOVERY EDGE|
				\State DFS(\(G\),\(w\))
			\Else
				\State \(e.label \gets\) \verb|BACK EDGE|
			\EndIf
		\EndIf
	\EndFor
\end{algo}

\subsubsection*{Correttezza e risultati}
Alla fine dell'esecuzione della DFS su una componente connessa \(C_s\) si ottiene:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] quanto previsto di base dall'algoritmo (vedi sopra)
	\item[-] i \verb|DISCOVERY EDGE| formano uno spanning tree di \(C_s\) radicato in \(s\), chiamato DFS Tree, con uno sviluppo
	prevalentemente in altezza (e non in larghezza)
	\item[-] se un arco è marcato \verb|BACK EDGE|, allora collega il vertice ad un suo antenato (nel DFS tree)
\end{itemize}

\subsubsection*{Complessità}
La complessità di DFS è proporzionale al numero di archi della componente connessa \(C_s\):
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] se il grafo non è connesso, allora la complessità è \(\Theta(m_s)\), con \(m_s = \) \# archi di \(C_s\)
	\item[-] se il grafo è connesso, allora la complessità è \(\Theta(|E|)\)
\end{itemize}

\newpage

\subsubsection*{Generalizzazione su tutto il grafo}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] L'algoritmo generalizza la DFS per visitare totalmente un grafo non connesso
	\item[-] La complessità dell'algoritmo è \(\Theta(|E| + |V|)\), supponendo di poter accedere agli elementi di \(V\) e \(E\)
	in tempo lineare.
\end{itemize}
\begin{algo}{DFSComplete}{$G$}{grafo $G = (V,E)$}{etichetta opportunamente ogni vertice e arco in \(G\)}
	\ForAll{$v$}{$V$}
		\State \(v.I\!D \gets 0\)
	\EndFor
	\ForAll{$e$}{$E$}
		\State \(e.label \gets null\)
	\EndFor
	\ForAll{$v$}{$V$}
		\If{\(v.I\!D = 0\)}
			\State DFS(\(G\), \(v\))
		\EndIf
	\EndFor
\end{algo}

\subsection{Confronto tra BFS e DFS}
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] \textbf{in comune}:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[-] su grafi non connessi, hanno la stessa complessità
		\item[-] possono essere usati indipendentemente per trovare connettività, spanning tree e cicli
	\end{itemize}
	\item[-] \textbf{vantaggi del BFS}:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[-] permette di trovare i cammini minimi
	\end{itemize}
	\item[-] \textbf{vantaggi del DFS}:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[-] è più space efficient in quanto richiede spazio proporzionale all'altezza dell'albero della ricorsione (ogni istanza richiede spazio costante)
		\item[-] è più efficiente per trovare un cammino tra due vertici lontani tra loro
		\item[-] viene usato per trovare cicli nei grafi diretti
	\end{itemize}
\end{itemize}

\newpage

\subsection{Cammini minimi su grafi pesati - Dijkstra}
\subsubsection*{Definizione di grafi pesati}
\begin{itemize}
	\item[-] Un grafo pesato è un normale grafo \(G = (V,E,w)\) con l'aggiunta di una funzione \(w: E \to \mathbb{R}_0^+\) tale
	che associa a ciascun arco di \(E\) un peso reale non negativo.
	\item[-] La lunghezza di un cammino \(u_1, u_2, \dots u_k\) è data dalla somma del peso degli archi \(\displaystyle \sum_{i=1}^{k-1} w(u_i,u_{i+1})\)
\end{itemize}

\subsubsection*{Algoritmo}
L'algoritmo di Single-Source Shortest Path o (SSSP):
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] determina le distanze tra \(s\) e gli altri vertici di \(V\) e i relativi cammini minimi
	\item[-] suppone che non esistano cicli e in generale archi con peso negativo
	\item[-] generalizza la BFS per grafi pesati
	\item[-] ogni vertice ha un campo \(v.D\) per la distanza da \(s\) e un campo \(v.parent\) per il vertice padre
	\item[-] utilizza una priority queue \(Q\) con \(Q\).decreaseKey(\(v.D\),\(v\)) che aggiorna la chiave \(v.D\)
\end{itemize}
Durante l'esecuzione dell'algoritmo SSSP:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] vengono visitati tutti i vertici in \(C_s\) componente connessa che contiene \(s\)
	\item[-] ad ogni vertice vengono impostati:
	\begin{itemize}[topsep=0pt, itemsep=0pt]
		\item[-] il campo \(v.D\) con la distanza da \(s\) (\(v.D = +\infty\) se non esiste cammino tra \(s\) e \(v\))
		\item[-] il campo \(v.parent\) con il riferimento al vertice padre nel \say{SSSP Tree}
	\end{itemize}
\end{itemize}

\begin{algo}{ShortestPaths}{$G$,$s$}{grafo $G = (V,E)$, $s \in C_s$}{etichetta opportunamente ogni vertice in \(C_s\)}
	\State \(s.D \gets 0\) \Comment inizializza vertice di partenza
	\State \(s.parent \gets\) null
	\ForAll{$v$}{$V \backslash \{s\}$} \Comment inizializza altri vertici
		\State \(v.D \gets +\infty\)
		\State \(v.parent \gets\) null
	\EndFor
	\State \(Q \gets\) priority queue con entry \((v.D, v)\) per ogni \(v \in V\)
	\While{(!$Q$.isEmpty())} \Comment finché ci sono nodi da visitare
		\State \((u.D, u) \gets Q\).removeMin()
		\ForAll{$(u,v)$}{$G$.incidentEdges($u$)} \Comment aggiorna la distanza dei vicini
			\If{($u.D + w(u,v) < v.D$)} \Comment edge relaxation
				\State \(v.D \gets u.D + w(u,v)\)
				\State \(v.parent \gets u\)
				\State \(Q\).decreaseKey(\(v.D\), \(v\))
			\EndIf
		\EndFor
	\EndWhile
\end{algo}

\subsubsection*{Correttezza e risultati}
Alla fine dell'esecuzione dell'algoritmo di Dijkstra su una componente connessa \(C_s\) si ottiene:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] quanto previsto di base dall'algoritmo (vedi sopra)
	\item[-] se \(v.D = +\infty\), allora \(v.parent = \) null \(\rightarrow\) se \(v \notin C_s\) la sua distanza è \(+\infty\)
	e non ha vertici \say{scopritori}
	\item[-] se \(v.D < +\infty\), il cammino ottenuto risalendo di padre in padre da \(v\) è il cammino minimo da \(s\) a \(v\),
	con \(d(s,v) = v.D\)
	\item[-] (nota invariante del for:) al momento dell'estrazione di una entry \((u.D, u)\), il campo \(u.D = d(s,u)\)
\end{itemize}

\subsubsection*{Complessità}
La complessità dell'algoritmo di Dijkstra è dominata dai metodi della priority queue. La loro complessità dipende dall'implementazione
della priority queue attraverso heap o linked-list:
\begin{center}
	\begin{tabular}{l | c | c | c}
		\textbf{metodo} & \textbf{heap} & \textbf{linked-list} \\
		\toprule
		1. costruzione iniziale & \(\Theta(n)\) & \(\Theta(n)\) \\
		\midrule
		2. removeMin & \(\Theta(\log n)\) & \(\Theta(n)\) \\
		\midrule
		3. decreaseKey & \(\Theta(\log n)\) & \(\Theta(1)\) \\
		\bottomrule
	\end{tabular}
\end{center}
Complessivamente si hanno \(n\) removeMin() e \(\leq m\) edge relaxations, ovvero al più \(m\) decreaseKey():
\begin{center}
	\begin{tabular}{l | c | c | c}
		\textbf{metodo} & \textbf{heap} & \textbf{linked-list} \\
		\toprule
		costruzione iniziale & \(\Theta(n)\) & \(\Theta(n)\) \\
		\midrule
		\(n\) removeMin & \(\Theta(n \log n)\) & \(\Theta(n^2)\) \\
		\midrule
		\(m\) decreaseKey & \(\Theta(m \log n)\) & \(\Theta(m)\) \\
		\bottomrule
	\end{tabular}
\end{center}
Per cui in torale si avrà:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] con l'heap la complessità è \(\Theta(n+m) \log n\)
	\item[-] con la lista la complessità è \(\Theta(n^2)\)
\end{itemize}
In funzione del valore di \(m\) si potrà scegliere l'una o l'altra implementazione:
\begin{itemize}[topsep=3pt, itemsep=0pt]
	\item[-] con \(m \leq n^2 / \log n \;\; \rightarrow \;\;\) heap
	\item[-] con \(m > n^2 / \log n \;\; \rightarrow \;\;\) linked list
\end{itemize}
Nota: usando un Fibonacci Heap è possibile implementare una priority queue e avere come complessità complessiva di ShortestPaths
\(\Theta(m + n \log n)\)

\end{document}
